\documentclass[11pt]{report}

\usepackage{
	amsmath,
    amssymb,
    cancel,
    centernot,
    enumerate,
    enumitem,
    geometry,
    hyperref,
    mathtools,
    svg,
    tcolorbox,
    tikz,
    titlesec
}
\usepackage[italian]{babel}

\newtcolorbox{warningbox}{
  colback=red!10!white,
  colframe=red!75!black,
  fonttitle=\bfseries,
  title=Attenzione,
  left=10pt,
  right=10pt,
  boxsep=5pt,
  arc=0pt,
  boxrule=0pt,
  breakable
}

\newenvironment{warning}{
  \begin{warningbox}
}{
  \end{warningbox}
}

\setlist[enumerate]{wide=0pt, leftmargin=21pt, labelwidth=0pt, align=left}
\setenumerate[1]{label={(\arabic*)}}
\allowdisplaybreaks
\newgeometry{
	vmargin={15mm},
	hmargin={10mm}
}
\usetikzlibrary{arrows.meta}

\tcbuselibrary{
    breakable,
    minted,
    skins
}
\DeclareTCBListing{mintedbox}{O{}m!O{}}{%
    breakable=true,
    listing engine=minted,
    listing only,
    minted language=#2,
    minted style=default,
    minted options={%
        linenos,
        gobble=0,
        breaklines=true,
        breakafter=,
        numbersep=8pt,
    #1},
    boxsep=0pt,
    left skip=0pt,
    right skip=0pt,
    left=25pt,
    right=0pt,
    top=3pt,
    bottom=3pt,
    leftrule=0pt,
    rightrule=1pt,
    bottomrule=2pt,
    toprule=2pt,
    colback=gray!0.99,
    colframe=orange!15,
    enhanced,
    overlay={%
        \begin{tcbclipinterior}
            \fill[orange!15!white] (frame.south west) rectangle([xshift=20pt]frame.north west);
        \end{tcbclipinterior}
    },
    #3
}

\titleformat{name=\section}[block]
  {\sffamily\Large\bf}
  {}
  {0pt}
  {\coloredsection}
\titlespacing*{\section}{0pt}{\baselineskip}{\baselineskip}
\newcommand{\coloredsection}[1]{%
  \colorbox{blue!15}{\parbox{\dimexpr\textwidth-2\fboxsep}{\vspace{8pt}\ #1\hfill\thesection\ \vspace{8pt}}}}
\renewcommand{\theequation}{\arabic{equation}}

\font\titlefont=cmr11 at 40pt
\title{{\titlefont Statistica e Analisi dei Dati}}
\date{}
\author{}

\begin{document}

\maketitle

\tableofcontents

\part{Statistica Descrittiva}

\chapter{Definizioni Generali}

\section{Carattere}

\subsection{Qualitativo}
Si parla di dati qualitativi quando la misurazione è fatta scegliendo un'etichetta a partire da un insieme di etichette disponibili. Le tipologie di dato qualitativo sono:\\
\textbf{Nominale}
\begin{itemize}
    \item Sono dati che non posso confrontare e che non possiedono un ordine o categorie intrinseche.
    \item Colore dei capelli, categoria di prodotti e simili.
\end{itemize}
\textbf{Binario o Booleano}
\begin{itemize}
    \item Sono un caso particolare dei dati a carattere nominale.
    \item Genere, vero o falso, assenza o presenza.
\end{itemize}
\textbf{Ordinale}
\begin{itemize}
    \item Rappresentano caratteri che possono essere messi in relazione con operatori di $<$, $=$, $>$.
    \item Livelli di soddisfazione (basso, medio, alto), classifiche (primo, secondo, terzo) o risultati (triste, neutro, felice).
\end{itemize}
\subsection{Quantitativo}
Si parla di dati quantitativi se l'esito della misurazione è una quantità numerica. Inoltre:
\begin{itemize}
    \item Sono scalari e sono caratteri quantitativi con un ordine definito e spesso hanno unità di misura.
    \item Altezza, peso, temperatura in gradi Celsius.
\end{itemize}
Infine, un carattere quantitativo può essere diviso in:\\
\textbf{Discreto}
\begin{itemize}
    \item Tutti quei caratteri che ha senso rappresentare singolarmente.
    \item Data di nascita, oggetti venduti da un negozio e così via.
\end{itemize}
\textbf{Continuo}
\begin{itemize}
    \item Tutti quei caratteri che ha senso rappresentare tramite un intervallo.
    \item Altezza, temperatura corporea, forza di un supereroe.
\end{itemize}

\section{Popolazione}
Insieme completo di oggetti. Esso è spesso troppo grande perché sia possibile un esame esaustivo. Per esempio i residenti di una regione oppure i televisori prodotti da un'azienda.

\section{Campione}
Un sottogruppo della popolazione, utile per apprendere informazioni dalla popolazione. Esso deve essere rappresentativo della popolazione.
\begin{equation}
    C=\{x_1, \dots, x_n\}
\end{equation}
\subsection{Campione bivariato}
Talvolta non abbiamo a che fare con sequenze di dati singoli, ma con sequenze di coppie di numeri, tra i quali esiste qualche relazione. In questi casi ogni coppia è da considerarsi un'osservazione. Possiamo denotare con $(x_i, y_i)$ la coppia di valori ed essi prendono il nome di campione bivariato. Un esempio di campione bivariato potrebbe essere le ore di studio di uno studente e il suo punteggio all'esame.
\subsection{Bias}
Quando sei convinto di campionare bene la popolazione ma in realtà non lo stai facendo. Il bias è una tendenza a deviare dal valore atteso, introducendo distorsioni nei risultati di un esperimento.

\section{Frequenza}
\subsection{Frequenza assoluta}
Frequenza $f$ con cui uno dei valori compare nel campione.
\subsection{Frequenza relativa}
Rapporto $\frac{f}{n}$ con cui la frequenza compare nel campione rispetto a l'insieme dei dati. Possiamo poi definire la media pesata come
\begin{equation}
    \begin{split}
        \overline{x} & = \frac{1}{n}\sum_{j=1}^{m}v_jf_j\\
        & = v_1\frac{f_1}{n} + v_2\frac{f_2}{n} + ... + v_m\frac{f_m}{n}
    \end{split}
\end{equation}

\section{Classificatori}
Considero un classificatore binario
\begin{center}
    \begin{tikzpicture}
        \draw (0,0) -- (2,0) -- (2,2) -- (0,2) -- (0,0);
        \draw (1,-0.3) -- (1,2.3);
        \draw (1,1) circle (0.75);
        \node at (0,2.2) {positivi};
        \node at (2,2.2) {negativi};
        \node at (0.3,1.8) {FN};
        \node at (1.7,1.8) {VN};
        \node at (1.35,1) {FP};
        \node at (0.65,1) {VP};
    \end{tikzpicture}
\end{center}
Dentro al cerchio mettiamo le predizioni positive mentre il quadrato è il "mondo intero". In sostanza: se sto a sinistra o a destra del quadrato, sono sicuro. Se sono dentro al cerchio, posso essere VP o se sono nel quadrato a destra sono VN, quindi il predittore ha avuto ragione. FN è quando mi dice che è negativo ma è positivo nella realtà, FP è quando mi dice positivo ma è negativo. Qui sotto la \textit{Matrice di Confusione}, dove con E è indicata l'etichetta e con P la predizione:
\begin{center}
    \begin{tabular}{ c| c c c }
    & + & E & - \\
    \hline
    + & VP & & FP \\
    P \\
    - & FN & & VN \\
    \hline
    & TP & & TN
    \end{tabular}
\end{center}
L'\textbf{accuratezza}, compresa tra $0$ e $1$ è data da
$$\frac{\text{VP} + \text{VN}}{\text{TP} + \text{TN}}$$
Per la collettività, a volte, i FN hanno un peso maggiore sull'accuratezza (per esempio, se c'è una malattia, i FN sono liberi di uscire di casa come i VN ma i FN dovrebbero essere come i VP e FP). Dunque si usano la \textbf{sensibilità} e la \textbf{specificità}:
$$\text{SENS} = \frac{\text{VP}}{\text{TP}} = \frac{\text{VP}}{\text{VP} + \text{FN}}$$
$$\text{SPEC} = \frac{\text{VN}}{\text{TN}} = \frac{\text{VN}}{\text{VN} + \text{FP}}$$
Per essere più chiari usando l'esempio di tamponi per la rivelazione di un patogeno, il termine \textit{sensibilità}, quando elevata, si riferisce alla capacità di trovare il patogeno anche se presente in piccole quantità sul tampone. Invece un tampone con elevata \textit{specificità} è in grado di rilevare solo e soltanto quel patogeno, evitando di dare un falso positivo in caso un patogeno con RNA simile sia presente sul tampone.
In particolare si può costruire un grafico a partire da
$$(1 - \text{SPEC}, \text{SENS}) = \left(1 - \frac{\text{VN}}{\text{TN}}, \frac{\text{VP}}{\text{TP}}\right) = \left(\frac{\text{FP}}{\text{TN}}, \frac{\text{VP}}{\text{TP}}\right) = \left(\frac{\text{FP}}{\text{FP} + \text{VN}}, \frac{\text{VP}}{\text{VP} + \text{FN}}\right)$$
\begin{center}
    \begin{tikzpicture}
        \draw[-Stealth] (0,0) -- (0,2.5);
        \draw[-Stealth] (0,0) -- (2.5,0);
        \draw[dashed] (2,0) -- (2,2) -- (0,2);
        \draw[dashed] (0,0) -- (2,2);
        \draw[dashed] (0,2) -- (2,0);
        \node at (-0.2,-0.2) {0};
        \node at (-0.2,2) {1};
        \node at (2,-0.2) {1};
        \node at (2.3,2.3) {CP};
        \node at (0.3,2.3) {CI};
        \node at (2.3,0.3) {CS};
        \node at (0.3,0.3) {CN};
        \node at (1.5,1.5) {CC};
        \node at (1,1) {CC};
        \node at (2.6,-0.6) {1-SPEC};
        \node at (-0.6,2.6) {SENS};
    \end{tikzpicture}
\end{center}
\begin{itemize}
	\item \textbf{Classificatore Costante Positivo:} $\text{CP}$ becca correttamente i casi positivi ma non azzecca alcun caso negativo
	$$\text{SENS} = \frac{\text{VP}}{\text{TP}} = 1,\ \text{SPEC} = \frac{0}{\text{TN}} = 0$$
	\begin{center}
	\begin{tabular}{ c| c c c }
		& + & E & - \\
		\hline
		+ & TP & & TN \\
		P \\
		- & 0 & & 0
	\end{tabular}
	\end{center}
	\item \textbf{Classificatore Costante Negativo:} $\text{CN}$ è l'opposto di $\text{CP}$
	$$\text{SENS} = \frac{0}{\text{TP}} = 0,\ \text{SPEC} = \frac{\text{VN}}{\text{TN}} = 1$$
	\begin{center}
	\begin{tabular}{ c| c c c }
		& + & E & - \\
		\hline
		+ & 0 & & 0 \\
		P \\
		- & TP & & TN
	\end{tabular}
	\end{center}
	\item \textbf{Classificatore Ideale:} $\text{CI}$ non sbaglia mai
	$$\text{SENS} = \frac{\text{VP}}{\text{TP}} = 1,\ \text{SPEC} = \frac{\text{VN}}{\text{TN}} = 1$$
	\begin{center}
	\begin{tabular}{ c| c c c }
		& + & E & - \\
		\hline
		+ & TP & & 0 \\
		P \\
		- & 0 & & TN
	\end{tabular}
	\end{center}
	\item \textbf{Classificatore Sbagliato:} $\text{CS}$ sbaglia sempre
	$$\text{SENS} = \frac{0}{\text{TP}} = 0,\ \text{SPEC} = \frac{0}{\text{TN}} = 0$$
	\begin{center}
	\begin{tabular}{ c| c c c }
		& + & E & - \\
		\hline
		+ & 0 & & TN \\
		P \\
		- & TP & & 0
	\end{tabular}
	\end{center}
	\item \textbf{Classificatore Casuale:} $\text{CC}_{1/2}$ lancio della moneta e $\text{CC}_{4/5}$ dado a 5 facce
    \begin{center}
        \begin{minipage}{0.5\textwidth}
            \begin{center}
                \text{SENS} = $\frac{1}{2}$, \text{SPEC} = $\frac{1}{2}$
                \hfill
                \begin{tabular}{ c| c c c }
                & + & E & - \\
                \hline
                + & $\frac{1}{2}$TP & & $\frac{1}{2}$TN \\
                P \\
                - & $\frac{1}{2}$TP & & $\frac{1}{2}$TN
                \end{tabular}
            \end{center}
        \end{minipage}
    \end{center}
    \begin{center}
        \begin{minipage}{0.5\textwidth}
            \begin{center}
                \text{SENS} = $\frac{4}{5}$, \text{SPEC} = $\frac{1}{5}$
                \hfill
                \begin{tabular}{ c| c c c }
                & + & E & - \\
                \hline
                + & $\frac{4}{5}$TP & & $\frac{4}{5}$TN \\
                P \\
                - & $\frac{1}{5}$TP & & $\frac{1}{5}$TN
                \end{tabular}
            \end{center}
        \end{minipage}
    \end{center}
\end{itemize}
\subsection{Curva ROC}
Più ci si avvicina a CI e meglio è, dunque si osserva la curva ROC (\textit{Receiver Operating Characteristic}). Tuttavia, guardare la curva ROC da sola è un giudizio qualitativo, dunque osservo l'area sottesa a ROC ossia la AUC.
\subsection{AUC}
L'\textit{Area Under the roc Curve} è l'area sottesa alla curva ROC. Essa è $0 \leq \text{AUC} \leq 1$ e raggiunge $1$, area del quadrato ($1^2 = 1$) nel caso in cui la ROC vada subito a CI e non si muova da quella posizione.
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{rocauc}
\end{center}

\chapter{Indici}

\section{Indici di centralità}
\subsection{Media Campionaria}
\begin{equation}
    \overline{x} = \frac{1}{n}\sum_{i=1}^n{x_i}
\end{equation}
La media campionaria è un indice di centralità ed è definita come il "baricentro dei dati". Dalla media campionaria si può arrivare al concetto di \textbf{scarto} dimostrando che
\begin{equation}
    \begin{split}
        \sum_{i=1}^n{(x_i - \overline{x})} & = \sum_{i=1}^n{x_i} - \sum_{i=1}^n{\overline{x}}\\
        & = n\overline{x} - n\overline{x}\\
        & = 0
    \end{split}
\end{equation}
La media campionaria rimane poco robusta rispetto agli outliers.
\subsection{Mediana Campionaria}
\begin{itemize}
    \item Ordino il campione in ordine crescente
    \item Seleziono il valore $m$ tale che:
\end{itemize}
\begin{equation}
	m =
	\begin{cases}
		x_{(\frac{n+1}{2})}														& \text{se }\mid C \mid \text{ è dispari} \\
		\frac{\left[x_{ \left( \frac{n}{2} \right)} + x_{ \left( \frac{n}{2} + 1 \right) }\right]}{2}		& \text{se }\mid C \mid \text{ è pari}
    \end{cases}
\end{equation}
Quindi nel caso dispari seleziono il valore che si trova al centro seguendo l'ordine crescente, mentre nel caso pari faccio la media aritmetica tra i due valori centrali. Se mi capita di avere elementi non scalari e dunque non divisibili, posso riportarli entrambi o sceglierne uno dei due.
\subsection{Moda Campionaria}
Definito come il dato che si presenta più spesso nel campione (caso unimodale). Se mi capita di avere due o più valori con la stessa frequenza (caso multimodale), posso riportare entrambi i valori modali o sceglierne uno dei due, come nella mediana campionaria.

\section{Indici di dispersione}
\subsection{Quantile campionario}
Possiamo definire la mediana campionaria come il valore del campione che è $\geq$ almeno il $50\%$ dei dati e $\leq$ almeno il $50\%$ dei dati. Generalizzando questo concetto si può introdurre il quantile campionario di livello $q \in [0,1]$, ossia il valore del campione $\geq$ almeno il $100 \cdot q\%$ dei dati e $\leq$ almeno il $100 \cdot (1-q)\%$ dei dati.\\
Per esempio, immagino di avere 12 osservazioni e voglio trovare il quantile $q=0.9$
\begin{equation}
    x_1\ x_2\ x_3\ x_4\ x_5\ x_6\ x_7\ x_8\ x_9\ x_{10}\ x_{11}\ x_{12}
\end{equation}
Evidenziamo il valore $\geq 90\%\text{ di }12 = 10.8 = 11$
\begin{equation}
    x_1\ x_2\ x_3\ x_4\ x_5\ x_6\ x_7\ x_8\ x_9\ x_{10}\ \underline{x_{11}\ x_{12}}
\end{equation}
Evidenziamo il valore $\leq 10\%\text{ di }12 = 1.2 = 2$
\begin{equation}
    \underline{x_1\ x_2\ x_3\ x_4\ x_5\ x_6\ x_7\ x_8\ x_9\ x_{10}\ x_{11}}\ x_{12}
\end{equation}
Sovrapponiamoli
\begin{equation}
    x_1\ x_2\ x_3\ x_4\ x_5\ x_6\ x_7\ x_8\ x_9\ x_{10}\ \underline{x_{11}}\ x_{12}
\end{equation}
$x_{11}$ è il quantile che cercavo. Se invece io mi trovo ad avere dei quantili con valori interi, come per esempio nel caso di $n=20$ e $q=0.95$, dove $x_{19}$ e $x_{20}$ sono entrambi risultati, allora eseguo la media aritmetica tra i due
\begin{equation}
    \frac{x_{19} + x_{20}}{2}
\end{equation}
Se cambio la scala, il procedimento resta lo stesso e i quantili assumono nomi specifici. Per scale $[0-100]$ prendono il nome di \textit{percentili}, per scale $[0-10]$ invece \textit{decili} e infine i \textit{quartili} per scale $[0-4]$. A proposito di quartili, definiamo con \textit{range interquartile (IQR)} la differenza tra il terzo e il primo quartile.\\
Riassumendo, nel caso specifico dei quartili, la mediana risulta essere il secondo quartile (Q2), mentre il valore minimo e il valore massimo sono rispettivamente $\text{Q1} - (1.5 \cdot \text{IQR})$ e $\text{Q3} + (1.5 \cdot \text{IQR})$.
\subsection{Varianza campionaria}
Vogliamo osservare quanto i dati siano dispersi o concentrati, per farlo potremmo giudicare dalla loro distanza in modulo $\mid x_i - \overline{x} \mid$. Tuttavia non vogliamo usare il valore assoluto per togliere i valori negativi perché è scomodo lavorare con il valore assoluto, dunque eleviamo al quadrato $(x_i - \overline{x})^2$. Facciamo questo procedimento per ogni elemento del campione e dividiamo così da ottenere
\begin{equation}
    s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \overline{x})^2
\end{equation}
In particolare possiamo osservare che
\begin{equation}
    \begin{split}
        \sum_i(x_i - \overline{x})^2 & = \sum_i(x_i^2 - 2\overline{x}x_i + \overline{x}^2)\\
        & = \sum_ix_i^2 - \sum_i2\overline{x}x_i + \sum_i\overline{x}^2\\
        & = \sum_ix_i^2 - 2n\overline{x}^2 + n\overline{x}^2\\
        & = \sum_ix_i^2 - n\overline{x}^2
    \end{split}
\end{equation}
E dunque possiamo riscrivere la varianza campionaria come
\begin{equation}
    \frac{1}{n-1}\left(\sum_ix_i^2 - n\overline{x}^2\right)
\end{equation}
Alcune proprietà della varianza rispetto alle trasformazioni:
	\begin{itemize}
		\item \textbf{Traslazione:} $\overline{y} =\overline{x}+b$
            \begin{equation}
                \begin{split}
                    s_y^2 & =\frac{1}{n-1}\sum_{i=1}^n{(y_i-\overline{y})^2}\\
                    & = \frac{1}{n-1}\sum_{i=1}^n{(x_i+\bcancel{b}-\overline{x}-\bcancel{b})^2}\\
                    & =s^2
                \end{split}
            \end{equation}
    	\item \textbf{Scalatura:} $y_i = \alpha x_i$
            \begin{equation}
                \begin{split}
                    s_y^2 & =\frac{1}{n-1}\sum_{i=1}^n{(y_i-\overline{y})^2}\\
                    & = \frac{1}{n-1}\sum_{i=1}^n{(\alpha x_i-\alpha \overline{x})^2}\\
                    & = \alpha^2s^2
                \end{split}
            \end{equation}
  	\end{itemize}
\subsection{Deviazione standard campionaria}
Quando usiamo la varianza campionaria abbiamo problemi con le unità di misura che sono tutte elevate al quadrato rispetto al normale, dunque definiamo la deviazione standard campionaria come
\begin{equation}
    s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \overline{x})^2}
\end{equation}
N.B. rispetto alle operazioni di scalatura vale che $ s_y=\mid \alpha \mid s_x$
\subsection{Coefficiente di variazione}
Siccome la varianza assume importanza inversamente proporzionale rispetto all'ordine di grandezza può aver senso "standardizzarla" rispetto alla media campionaria:
\begin{equation}
    s^* = \frac{s}{\mid \overline{x} \mid}
\end{equation}
Nel caso in cui ci siano sia valori positivi che negativi, basta effettuare una opportuna traslazione del campione per far si che tutte le osservazioni abbiano segno concorde. Il coefficiente di variazione risulta particolarmente utile in situazioni in cui vogliamo confrontare la variabilità di distribuzioni che presentano scale di grandezza distinte.
\subsection{Covarianza campionaria}
Preso un campione bivariato, definiamo
\begin{equation}
    Cov(x,y) = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y})
\end{equation}
Se il risultato è $\geq 0$ allora esiste una correlazione diretta tra i dati, indiretta se $<0$. Quindi se ho due campioni di cardinalità $n$, voglio determinare se sono correlati; un esempio di correlazione diretta può essere
$$x_i\ piccolo \iff y_i\ piccolo\ \land\ x_i\ grande \iff y_i\ grande$$

\section{Indici di correlazione}
\subsection{Coefficiente di correlazione campionaria}
Dato un campione bivariato si dice coefficiente di correlazione campionaria e si denota con $r$ la quantità
\begin{equation}
    r = \frac{1}{n-1}\frac{\sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y})}{s_x s_y}
\end{equation}
Questo coefficiente gode delle seguenti proprietà:
\begin{enumerate}
	\item $-1 \leq r \leq 1$, con relazione diretta per $r=1$, indiretta per $r=-1$ e indefinita per $r=0$. Questo è dimostrabile tramite i seguenti passaggi: considerando $y_i = a + bx_i$ e dunque $\overline{y} = a + b\overline{x}$, $s_y = \mid b \mid s_x$ e $y_i - \overline{y} = a + bx_i - a - b\overline{x} = bx_i - b\overline{x}$. Possiamo quindi dedurre che
    \begin{equation}
        \begin{split}
            r & = \frac{1}{n-1}\frac{b\sum_{i=1}^{n}(x_i - \overline{x})^2}{s_x \mid b \mid s_x}\\
            & = \frac{b}{\mid b \mid}\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \overline{x})^2\frac{1}{s^2_x}\\
            & = \frac{b}{\mid b \mid}\\
            & = sign(b)
        \end{split}
    \end{equation}
	\item Se $r$ è il coefficiente di correlazione del campione $(x_i, y_i)$, allora lo è anche per il campione $(a + bx_i, c + dy_i)$ purché $b$ e $d$ abbiano lo stesso segno. Ossia il coefficiente di correlazione del campione non cambia se sommiamo costanti o moltiplichiamo per costanti: ciò significa che $r$ non dipende dalle unità di misura.
\end{enumerate}
\subsection{Coefficiente di correlazione di Pearson}
Considerando che
\begin{equation}
    \begin{split}
        \sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y}) & = \sum_{i=1}^{n}x_iy_i - \sum_{i=1}^{n}\overline{x}y_i - \sum_{i=1}^{n}x_i\overline{y} + \sum_{i=1}^{n}\overline{x}\overline{y}\\
        & = \sum_{i=1}^{n}x_iy_i - n\overline{x}\overline{y} - n\overline{x}\overline{y} + n\overline{x}\overline{y}\\
        & = \sum_{i=1}^{n}x_iy_i - n\overline{x}\overline{y}
    \end{split}
\end{equation}
allora possiamo riscrivere $r$ come
\begin{equation}
    \begin{split}
        r & = \frac{1}{n-1}\frac{\sum_{i=1}^{n}x_iy_i - n\overline{x}\overline{y}}{\frac{1}{n-1}\sqrt{\sum_{i=1}^{n}x_i^2 - n\overline{x}^2}\sqrt{\sum_{i=1}^{n}y_i^2 - n\overline{y}^2}}\\
        & = \frac{\sum_{i=1}^{n}x_iy_i - n\overline{x}\overline{y}}{\sqrt{\sum_{i=1}^{n}x_i^2 - n\overline{x}^2}\sqrt{\sum_{i=1}^{n}y_i^2 - n\overline{y}^2}}
    \end{split}
\end{equation}

\section{Indici di eterogeneità}
\subsection{Eterogeneità}
Un campione omogeneo è tale quando ogni elemento è uguale, mentre un campione eterogeneo è chiamato così quando vi sono le stesse frequenze per ogni valore del campione. Per esempio il seguente campione è omogeneo
$$\blacksquare \blacksquare \blacksquare \blacksquare \blacksquare$$
il seguente è eterogeneo
$$\blacksquare \blacktriangle \blacktriangle \blacksquare$$
mentre quest'ultimo non è né omogeneo né eterogeneo
$$\blacksquare \blacksquare \blacksquare \blacktriangle$$
\subsection{Indice di Gini}
Nel caso in cui stessimo studiando la distribuzione di un attributo di una popolazione (ad esempio la ricchezza), potrei dover discernere tra una distribuzione equa ed una non equa. Posso usare l'indice di Gini che è un indice per l'eterogeneità ed è indicato come
\begin{equation}
    I = 1 - \sum_{j=1}^{k}f_j^2
\end{equation}
definito $\forall j\ 0 \leq f_j \leq 1$, ma che valori può assumere $I$?\\
Analizziamo prima per $\forall j\ f_j \geq 0$
\begin{enumerate}
    \item $\sum_{j=1}^{k}f_j=1$
    \item $\exists j^{\prime}\ f_{j^{\prime}} : f_{j^{\prime}}^2 > 0 \implies \sum_{j=1}^{k}f_j^2 > 0$
    \item $I = 1 - \sum_{j=1}^{k}f_j^2 \implies I < 1$
\end{enumerate}
Ora analizziamo per $\forall j\ 0 \leq f_j \leq 1$
\begin{enumerate}
    \item $f_j^2 \leq f_j$
    \item $\sum_{j=1}^{k}f_j^2 \leq \sum_{j=1}^{k}f_j = 1 \implies 1 - \sum_{j=1}^{k}f_j^2 \geq 0 \implies I \geq 0$
\end{enumerate}
Dunque possiamo asserire che
$$0 \leq I < 1$$
Facciamo degli esempi:
\begin{description}
	\item \textbf{Eterogeneità Minima:} Supponendo di avere $\blacksquare \blacksquare \blacksquare \blacksquare \blacksquare$ ci ritroviamo $\blacksquare$ con frequenza relativa $f = 1$, dunque
    \begin{equation}
        \begin{split}
            I & = 1 - \sum_{j=1}^{k}f_j^2\\
            & = 1 - 1\\
            & = 0
        \end{split}
    \end{equation}
	\item \textbf{Eterogeneità Massima:} In questo caso supponiamo $\blacksquare \blacksquare \blacklozenge \blacklozenge \blacktriangle \blacktriangle$, abbiamo $k=3$ simboli diversi dunque la frequenza relativa sarà $\frac{\alpha}{k \cdot \alpha} = \frac{1}{k}$ e quindi
    \begin{equation}
        \begin{split}
            I & = 1 - \sum_{j=1}^{k}\frac{1}{k}^2\\
            & = 1 - k\frac{1}{k}^2\\
            & = 1 - \frac{1}{k}\\
            & = \frac{k - 1}{k}
        \end{split}
    \end{equation}
\end{description}
\subsection{Indice di Gini normalizzato}
Torna utile normalizzare l'indice di Gini quindi, dato $0 \leq I < 1$, possiamo considerare
\begin{equation}
    I^{\prime} = \frac{k}{k - 1}I
\end{equation}
\subsection{Entropia}
Denotiamo l'entropia con
\begin{equation}
    H = \sum_{j=1}^{k}f_j \cdot \log\frac{1}{f_j} \geq 0
\end{equation}
In caso di eterogeneità minima, ossia con $f_j = 1$ abbiamo $\frac{1}{f_j} = 1$ e dunque $H = 1 \cdot \log 1 = 0$. In caso invece di eterogeneità massima si ha $\forall j\ f_j = \frac{1}{k} \implies H = \sum_{j=1}^{k}\frac{1}{k} \cdot \log k = \log k$ e dunque $0 \leq H \leq \log k$. Da qui si può normalizzare in
\begin{equation}
    H^{\prime} = \frac{H}{\log k}
\end{equation}
così da avere $0 \leq H^{\prime} \leq 1$, utile con il logaritmo in base 2 per misurare in bit.

\section{Indici di concentrazione}
Immaginiamo di avere $n$ osservazioni $a_1, \cdots, a_n$ ordinate dalla più piccola alla più grande, ognuna delle quali rappresenta la quantità (o ricchezza) di un elemento del campione. Definiamo quindi la media campionaria e la somma totale come segue:
\begin{equation}
    \overline{a} = \frac{1}{n} \sum_{i=1}^{n} a_i
\end{equation}
\begin{equation}
    \text{TOT} = n \overline{a} = \sum_{i=1}^{n} a_i
\end{equation}
Possiamo considerare due situazioni estreme:
\begin{enumerate}
    \item \textbf{Caso di concentrazione minima:} tutti gli elementi del campione assumono lo stesso valore $a_1 = a_2 = \cdots = a_n$
    \item \textbf{Caso di concentrazione massima:} tutti gli elementi del campione assumono valore 0, tranne uno che vale $n \overline{a}$
\end{enumerate}
Definiamo ora due frequenze relative cumulate:
\begin{itemize}
    \item \textbf{Frequenza relativa cumulata degli individui fino alla $i$-esima osservazione:}
    \begin{equation}
        F_i = \frac{i}{n}\ \ \ \ \forall i=1, \cdots, n
    \end{equation}

    \item \textbf{Quantità relativa cumulata fino alla $i$-esima osservazione:}
    \begin{equation}
        Q_i = \frac{\sum_{k=1}^{i} a_k}{\text{TOT}}
    \end{equation}
\end{itemize}
Definiamo anche alcune proprietà:
\begin{itemize}
    \item $0 \leq F_i \leq 1$ e $0 \leq Q_i \leq 1$
    \item $Q_i \leq F_i$ poiché le osservazioni sono ordinate in maniera crescente
    \item $Q_i = F_i$ nel caso di concentrazione minima
    \item $Q_n = F_n$
\end{itemize}
Pertanto, per $i = 1, \cdots, n$, le coppie $(F_i, Q_i)$ indicano che il $100F_i\%$ della popolazione detiene il $100Q_i\%$ della quantità considerata.
\subsection{Indice di Gini per la concentrazione}
L'indice di Gini è definito come:
\begin{equation}
    G = \frac{\sum_{i=1}^{n-1} (F_i - Q_i)}{\sum_{i=1}^{n-1} F_i}
\end{equation}
Questo indice fornisce una misura della disuguaglianza nella distribuzione della quantità considerata. Maggiore è il valore di \(G\), maggiore è la disuguaglianza.
Con l'indice di Gini è possibile generare una curva di punti nota come \textit{curva di Lorenz}. L'area tra la curva di Lorenz e la bisettrice di equidistribuzione rappresenta il valore dell'indice di Gini. Un indice di Gini pari a zero indica una distribuzione perfettamente equa, mentre un indice di Gini pari a uno indica una massima disuguaglianza.

\section{Trasformazioni lineari}
Nel caso serva effettuare una trasformazione lineare per mappare dei valori $\{v_1\dots v_n\}$ da un intervallo $[a,b]$ a un altro $[c,d]$, partendo da
\begin{equation}
    \frac{x'-c}{d-c} = \frac{x-a}{b-a}
\end{equation}
posso definire una funzione $f:X\mapsto X'$ tale che
\begin{equation}
    v' = f(v) = c + \frac{d-c}{b-a}\cdot (v-a)
\end{equation}
\subsection{Traslazione}
Dato $k > 0$
\begin{equation}
    \begin{split}
        v & \mapsto v^{\prime} = v + k\\
        v & \mapsto v^{\prime} = v - k
    \end{split}
\end{equation}
La traslazione non ha effetto sulle frequenze. Media, mediana e quantili vengono traslati. I range tra dati e i range interquartili restano invariati e non ha nessun effetto sulla varianza (e quindi sulla deviazione standard).
\subsection{Contrazione e dilatazione}
Dato $h \in \mathbb{R}$
\begin{equation}
    \begin{split}
        h & > 1\ \text{contrazione}\\
        h & < 1\ \text{dilatazione}
    \end{split}
\end{equation}
Media, mediana e quantili vengono scalati. Range e distanza interquartile vengono scalati. La varianza viene invece scalata di $\frac{1}{h^2}$.
\subsection{Standardizzazione}
Viene impiegata per permettere di utilizzare una scala comune tra due variabili aleatorie.
\begin{equation}
    g(v) = \frac{v - \overline{x}}{s_x}
\end{equation}
\subsection{Trasformazione logaritmica}
La trasformazione logaritmica è un utile strumento quando si hanno dati che presentano variazioni significative. Immagina di avere due case: una costa 100.000€ e l'altra 1.000.000€. La differenza percentuale tra questi due prezzi è del 900\%, ma può risultare più intuitiva se applichiamo la trasformazione logaritmica. Dopo la trasformazione, la differenza tra i due prezzi è circa 2.3.
\begin{equation}
    v \mapsto v^{\prime} = \log v
\end{equation}

\section{ANOVA}
Utilizziamo le tecniche dell'\textit{ANalysis Of VAriance} per confrontare due o più gruppi tramite la loro variabilità interna. Con $x_i^g$ definiremo l'i-esima osservazione nel g-esimo gruppo.
\begin{equation}
    \begin{split}
        & \overline{x}^g = \frac{1}{n^g}\sum_{i=1}^{n^g}x_i^g\\
        & \implies n^g\overline{x}^g  = \sum_{i=1}^{n^g}x_i^g\\
        & \implies \sum_{g=1}^{G}n^g\overline{x}^g = \sum_{g=1}^{G}\sum_{i=1}^{n^g}x_i^g\\
        & \implies \frac{1}{n}\sum_{g=1}^{G}n^g\overline{x}^g = \frac{1}{n} \cdot \text{somma delle osservazioni}\\
        & \implies \frac{1}{n}\sum_{g=1}^{G}n^g\overline{x}^g = \overline{x}
    \end{split}
\end{equation}
Da qui possiamo quindi dichiarare $\text{SS}_\text{T}$ (\textit{Sum of Squares Total}), $\text{SS}_\text{W}$ (\textit{Sum of Squares Within})e $\text{SS}_\text{B}$ (\textit{Sum of Squares Between}).
\begin{equation}
    \begin{split}
        & \text{SS}_\text{T} = \sum_{g=1}^{G}\sum_{i=1}^{n^g}(x_i^g - \overline{x})^2\\
        & \text{SS}_\text{W} = \sum_{g=1}^{G}\sum_{i=1}^{n^g}(x_i^g - \overline{x}^g)^2\\
        & \text{SS}_\text{B} = \sum_{g=1}^{G}(\overline{x}^g - \overline{x})^2
    \end{split}
\end{equation}
Possiamo quindi ottenere:
\begin{description}
	\item \textbf{Varianza:} $s^2 = \frac{\text{SS}_\text{T}}{n-1}$
	\item \textbf{Varianza Entro i Gruppi:} $s^2 = \frac{\text{SS}_\text{W}}{n-G}$
	\item \textbf{Varianza Tra i Gruppi:} $s^2 = \frac{\text{SS}_\text{B}}{G-1}$
\end{description}
Inoltre $\text{SS}_\text{T} = \text{SS}_\text{W} + \text{SS}_\text{B}$ e $\frac{\text{SS}_\text{T}}{n-1} = \frac{n-G}{n-1}\frac{\text{SS}_\text{W}}{n-G} + \frac{G-1}{n-1}\frac{\text{SS}_\text{B}}{G-1}$.
L'ANOVA viene spesso utilizzata per testare l'ipotesi che non ci siano differenze significative tra i gruppi. Questo viene fatto confrontando il rapporto delle varianze tra i gruppi ($\text{SS}_\text{B}$) e all'interno dei gruppi ($\text{SS}_\text{W}$). Se il rapporto è sufficientemente grande da suggerire che la variabilità tra i gruppi è significativa rispetto alla variabilità all'interno dei gruppi, possiamo respingere l'ipotesi e concludere che almeno uno dei gruppi è significativamente diverso dagli altri.

\part{Calcolo Combinatorio e Probabilità}

\chapter{Calcolo Combinatorio}

\section{Principio di enumerazione}
Avendo un esperimento con $m$ esiti e un secondo esperimento con $n$ esiti, abbiamo $m \cdot n$ coppie ordinate possibili di esiti. Più in generale:
\begin{equation}
    S = \prod_{i=1}^{t}s_i
\end{equation}
Per esempio se produco penne di 4 modelli e 3 colori diversi ciascuno, avrò $3 \cdot 4 = 12$ possibili combinazioni.

\section{Disposizioni}
Disposizioni di $n$ oggetti in $k$ posti:
\begin{itemize}
	\item L'ordine conta
	\item Con ripetizione e senza ripetizione
\end{itemize}
\subsection{Disposizioni con ripetizione}
Grazie al principio di enumerazione, sappiamo che il conteggio totale è $n^k$, dunque
\begin{equation}
    D_{n,k} = n^k
\end{equation}
Ipotizzando di avere $\{a,b,c,d\}$ oggetti e 2 posti, otterremo:
\begin{center}
\begin{tabular}{ c c c c }
aa & ba & ca & da \\
ab & bb & cb & db \\
ac & bc & cc & dc \\
ad & bd & cd & dd \\
\end{tabular}
\end{center}
ossia $n^k = 4^2 = 16$.\\
Le disposizioni con ripetizione possono risolvere il problema: \textit{Quanti sono i numeri composti da 6 cifre tutte dispari?}\\
Sappiamo che ci sono 5 numeri dispari, ossia $\{1,3,5,7,9\}$, i nostri oggetti, e dobbiamo inserirli in un numero da 6 cifre, le nostre posizioni, dunque avremo $5^6 = 15625$ numeri da 6 cifre con tutte cifre dispari.
\subsection{Disposizioni senza ripetizione}
In questo caso, se scelgo un oggetto, non posso sceglierlo nuovamente.
\begin{equation}
    d_{n,k} = \frac{n!}{(n-k)!}
\end{equation}
\textit{In una gara di 10 cavalli indovinare i primi 3 tenendo conto della posizione}. Ovviamente un cavallo non può arrivare secondo e contemporaneamente quinto, quindi usiamo disposizioni senza ripetizione impostando 10 cavalli e 3 posizioni. Il risultato sarà $\frac{10!}{(10-3)!} = \frac{10 \cdot 9 \cdot 8 \cdot 7!}{7!} = 720$.
\subsection{Permutazioni}
Le permutazioni di $n$ oggetti, identificate con $P_{n}$, non sono altro che disposizioni senza ripetizione in cui il numero delle posizioni equivale al numero degli oggetti. Vogliamo quindi riposizionare gli stessi oggetti, solamente tramite configurazioni diverse. Dunque
\begin{equation}
    \begin{split}
        P_n & = d_{n,n}\\
        & = \frac{n!}{(n-n)!}\\
        & = n!
    \end{split}
\end{equation}
Un esempio di applicazione sono gli anagrammi, per esempio quanti anagrammi ha la parola "giornale"? Esso è composto da 8 lettere da posizionare in 8 posizioni, quindi $8! = 40320$. Quanti anagrammi di "giornale" invece contengono la sillaba "le"? Si considera $\{g,i,o,r,n,a,le\}$, quindi $7! = 5040$.\\
In realtà, se vogliamo costruire anagrammi "più corretti", se usiamo le permutazioni ci aspettiamo che ogni lettera sia diversa dall'altra, ma se abbiamo lettere ripetute come nel caso di "tappo" e "mamma" non dobbiamo riconsiderare lo stesso anagramma, infatti per "tappo" avremo 5 lettere ma "p" si ripete 2 volte quindi $\frac{5!}{2!} = 60$, mentre per "mamma" abbiamo 5 lettere con "a" ripetuto 2 volte e "m" ripetuto 3 volte, quindi $\frac{5!}{2!3!}$.\\
Infine possiamo vedere come da $n! = n \cdot (n-1)!$ si riesca a provare $1 = 1! = 1 \cdot 0! = 0!$.

\section{Combinazioni}
\textit{In una gara di 10 cavalli indovinare i primi 3, senza considerare l'ordine}, dunque non ci interessa se Fulmine è primo, Saetta secondo e Fiamma terzo, ci interessa invece se Fulmine, Saetta e Fiamma salgono sul podio. Quello che bisogna fare è pensare ad una disposizione senza ripetizione in cui le coppie che contengono gli stessi elementi, semplicemente mescolati in maniera differente, identificano in realtà la stessa coppia. Per esempio prendendo $d_{3,2}$ su $\{a,b,c\}$ avremo:
\begin{center}
\begin{tabular}{ c c c }
ab & ac &    \\
ba &    & bc \\
   & ca & cb \\
\hline
\{a,b\} & \{a,c\} & \{b,c\}
\end{tabular}
\end{center}
Dunque $\{a,b\}, \{a,c\}, \{b,c\}$ sono le combinazioni che ci interessano. Per ottenerle utilizzeremo la formula
\begin{equation}
    \begin{split}
        C_{n,k} & = \frac{d_{n,k}}{P_k}\\
        & = \frac{n!}{k!(n-k)!}
    \end{split}
\end{equation}
essa può essere riscritta sfruttando il coefficiente binomiale
\begin{equation}
    C_{n,k} = \binom{n}{k}
\end{equation}
Riprendendo l'esempio di prima vediamo infatti come $C_{3,2} = \frac{6}{2} = 3$ e tornando all'esercizio dei cavalli basta svolgere $C_{10,3} = \frac{10!}{3!7!} = \frac{10 \cdot 9 \cdot 8 \cdot 7!}{3!7!} = \frac{10 \cdot 9 \cdot 8}{3!} = \frac{720}{6} = 120$.\\
Nel caso in cui ci trovassimo davanti ad una combinazione in cui però avviene una ripetizione, la formula sarà la seguente
\begin{equation}
    C_{n,k} = \binom{n+k-1}{k}
\end{equation}
Un esempio di applicazione di combinazione con ripetizione potrebbe essere il seguente:\\
\textit{Un'urna contiene 20 palline numerate. In quanti modi si possono estrarre 3 palline, supponendo che dopo ogni singola estrazione la pallina venga reimmessa nell'urna e senza tenere conto dell'ordine con cui le palline sono state estratte?}
\begin{equation}
    C_{n,k} = \binom{20+3-1}{3} = \binom{22}{3} = 1540
\end{equation}

\section{Esercizi}
\begin{itemize}
	\item \textbf{In quanti modi diversi possono sedersi 6 persone nei 6 posti di uno scompartimento ferroviario?}\\
	$6!$
	\item \textbf{Quanti anagrammi per la parola "matematica"?}\\
	$\frac{10!}{2!3!2!}$
	\item \textbf{Quante password da 5 cifre esistono?}\\
	$D_{10,5} = 10^5 = 100000$
	\item \textbf{Di queste password, se le prime 3 cifre sono uguali, quante ce ne sono?}\\
	Le prime tre cifre uguali possono essere $000, 111, \dots , 999$, dunque 10 possibili combinazioni. Le restanti due sono $D_{10,2}$, dunque: $10 \cdot 10^2 = 1000$
	\item \textbf{Qual è il numero di password distinte da 5 cifre che hanno 3 cifre uguali in qualsiasi posizione consecutiva?}\\
	$10 \cdot 1 \cdot 1 \cdot 10 \cdot 10 + 10 \cdot 10 \cdot 1 \cdot 1 \cdot 10 + 10 \cdot 10 \cdot 10 \cdot 1 \cdot 1 = 10^3 + 10^3 + 10^3 = 3 \cdot 10^3$
	\item \textbf{Tra tutti i numeri di 9 cifre diverse tra loro e diverse da zero, quanti sono quelli le cui prime due sono, nell'ordine, 5 e 2?}\\
	$7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2 \cdot 1 \cdot 1 \cdot 1$
	\item \textbf{Un sistemista del Totocalcio vuole giocare tutte le possibili colonne in cui figurano sei segni 1, cinque segni X e due segni 2.}\\
	$\frac{13!}{6!5!2!}$
	\item \textbf{Scrivi tutti i numeri di tre cifre, le cui cifre sono tutte dispari e diverse tra loro}\\
	$\frac{5!}{(5-3)!}$
	\item \textbf{Tre coppie di amici vanno a cercare un ristorante. Viene riservato a loro un tavolo con 10 posti. in quanti modi si possono disporre se si vuole che le donne siedano al lato del tavolo più vicino al muro e gli uomini al lato opposto?}\\
	(donne $\cdot$ uomini) $\rightarrow \left( \frac{5!}{2!} \right) \cdot \left( \frac{5!}{2!} \right)$
	\item \textbf{Quante diverse squadre di pallacanestro (che hanno 5 giocatori) è possibile selezionare da 16 ragazzi?}\\
	$\frac{16!}{5!(16-5)!}$
	\item \textbf{Si abbia una popolazione di 10 oggetti. Si estraggano senza ripetizione tutti i possibili campioni non ordinati
di dimensione 4. Quanti campioni fanno parte dello spazio campionario?}\\
	$\frac{10!}{4!(10-4)!}$
	\item \textbf{Ho 3 libri, 2 di matematica (M) e 1 di italiano (I), allora:}
	\begin{itemize}
		\item $3!$ ordina gli oggetti tra loro (es. \{MIM, MMI, IMM, ...\})
		\item $2! \cdot 1!$ ordina gli oggetti tra loro ma col primo gruppo posto davanti (ordinamento intra-gruppo e con precedenza di posizione, in questo contesto risulta \{MMI, MMI\})
		\item $(2! \cdot 1!) \cdot 2!$ ordina in tutte le combinazioni possibili di raggruppamenti (in questo contesto risulta \{MMI, MMI, IMM, IMM\})
	\end{itemize}
	\item \textbf{Tra tutti i numeri di 10 cifre diverse tra loro quanti sono multipli di 100?}\\
	Zero, un multiplo di 100 termina per forza con 00, due cifre uguali.
	\item \textbf{Tra tutti i numeri di 10 cifre tutte diverse tra loro, quanti sono quelli le cui 5 prime cifre sono dispari?}\\
	Abbiamo 5 numeri dispari tutti diversi tra loro alla fine ($5 \cdot 4 \cdot 3 \cdot 2 \cdot 1$) e quindi prima mettiamo tutti i numeri rimanenti, diversi tra loro, ossia quelli pari che sono anch'essi 5 ($5 \cdot 4 \cdot 3 \cdot 2 \cdot 1$), dunque:
	$$(5 \cdot 4 \cdot 3 \cdot 2 \cdot 1) * (5 \cdot 4 \cdot 3 \cdot 2 \cdot 1) = 14400$$
	\item \textbf{Tra tutti i numeri che possiamo formare con le cifre del numero 4550444, quanti sono i multipli di 10? E quanti sono i numeri pari?}\\
	I multipli di 10 saranno tutti quei numeri che termineranno per 0 (quindi imponiamo un $\cdot 1$ alla fine). Le cifre a nostra disposizione sono 4 numeri 4 e 2 numeri 5, ossia abbiamo $6! \cdot 1 = 6!$ possibili scelte. Tuttavia scrivere 4550444 e 4550444, invertendo gli ultimi due numeri, è la stessa cosa, quindi ci si comporta come con gli anagrammi:
	$$\frac{6!}{4!2!} = 15$$
	Quanti sono invece i numeri pari componibili? Esattamente come prima il ragionamento, però questa volta consideriamo che l'ultimo numero può essere uno 0 oppure uno dei 4 possibili 4, dunque 5 possibilità tenendo conto che 4 di esse sono ripetute, quindi:
	$$15 \cdot 5!/4! = 75$$
	\item \textbf{Tra tutti i  numeri di 3 cifre, tutte dispari e diverse tra loro, quanti sono i multipli di 5?}\\
	I multipli di 5 sono tutti quei numeri che terminano con 0 o con 5, tuttavia se termina con 0 allora il numero di 3 cifre non è composto da tutte cifre dispari. Quindi consideriamo solo il caso in cui termini per 5 ($\cdot 1$) mentre le altre cifre possono essere qualsiasi numero dispari, escluso il 5 poiché già usato (dato che viene richiesto che le cifre siano tutte diverse tra loro). Infine avremo:
	$$4 \cdot 3 \cdot 1 = 12$$
	\item \textbf{Quanti sono i possibili numeri di 5 cifre?}\\
	La cifra più a sinistra di un numero di 5 cifre deve per forza essere un numero compreso da 1 e 9 (9 valor), gli altri invece sono numeri compresi tra 0 e 9 (10 valori), dunque:
	$$9 \cdot 10 \cdot 10 \cdot 10 \cdot 10 = 90000$$
	\item \textbf{Quanti sono i numeri di 7 cifre, contenenti solo cifre pari escluso lo zero?}\\
	Le cifre pari escluso lo zero sono 4 ($\{2,4,6,8\}$). Dunque:
	$$4 \cdot 4 \cdot 4 \cdot 4 \cdot 4 \cdot 4 \cdot 4 = 16384$$
	\item \textbf{Dodici amici dopo aver partecipato a una cena si salutano e ognuno stringe la mano a tutti gli altri, quante sono le strette di mano?}\\
	$\binom{12}{2} = 66$
	\item \textbf{In una classe di 20 studenti si devono formare una squadra di calcio e una di pallacanestro. In quanti modi diversi si possono formare le due squadre se nessuno studente può appartenere a entrambe?}\\
	Una squadra di calcio ha 11 giocatori mentre una squadra di pallacanestro ne ha 5. Calcolo per esempio prima quanti potrebbero giocare a calcio:
	$$\binom{20}{11}$$
	Ora ho scelto 11 persone, ne ho altre 9 da far giocare a basket:
	$$\binom{9}{5}$$
	Quindi in totale:
	$$\binom{20}{11} \cdot \binom{9}{5} = 167960 \cdot 126 = 21162960$$
	\item \textbf{Si mescolano 12 carte, 3 vengono date a A, 3 a B , 3 a C e 3 a D, in quanti modi diversi può avvenire la distribuzione?}\\
	$\binom{12}{3} \cdot \binom{9}{3} \cdot \binom{6}{3} \cdot \binom{3}{3} = 369600$
	\item \textbf{In quanti modi possiamo distribuire 7 cioccolatini identici indistinguibili a 4 bambini diversi?}\\
	Possiamo vedere i bambini come delle urne e ciascuna urna riceve almeno un cioccolatino. Siccome i cioccolatini sono 7 e i bambini 4, significa che qualche bambino riceve più di un cioccolatino, dunque l'urna può ricevere una re-immissione. Dunque:
	$$\binom{12 - 3 + 1}{3} = \binom{10}{3} = 120$$
\end{itemize}

\chapter{Teoria della probabilità}

\section{Teoria degli insiemi}
La frase \textit{"ho il 60\% di probabilità di trovare il petrolio"} può essere interpretata in tre chiavi differenti: quella soggettiva che ci dice che \textit{"il 60\% delle volte ho trovato petrolio qui"}, quella frequentistica che ci suggerisce che \textit{"al 60\% trovo il petrolio"} e infine la chiave assiomatica che è quella della teoria degli insiemi.
\subsection{Insieme universo}
L'insieme universo $\Omega$ non è altro che l'insieme degli esiti possibili, lo spazio degli eventi. Per determinare il genere del nascituro $\Omega = \{M,F\}$, per una corsa di 5 cavalli $\Omega = \{\text{permutazioni di 7 elementi}\}$, ossia la cardinalità di $\Omega$ risulta $\mid \Omega \mid = 7!$, oppure per determinare il dosaggio minimo di un farmaco $\Omega = \mathbb{R}^+$.
\subsection{Evento elementare}
Definiamo un esito $w$ come una delle $x \in \Omega$ tale che $\forall x \in \Omega$. Nel caso della nascita $w = \{F\}$, nel caso di una corsa di cavalli $w = \{(1,2,3,4,5)\}$, mentre nel caso delle medicine $\{42\}$.
\subsection{Evento}
Qualcosa di incerto, tale che $E \subseteq \Omega$, per esempio $\{F\}$ e  $\{M\}$ nel caso del sesso, oppure $(3,7]$ per il dosaggio di un farmaco, ma non $\{M,F\}$ in quanto evento certo e non $\emptyset$ in quanto evento impossibile.
\subsection{Algebra degli eventi}
Definiamo $\mathcal{A} = \{\Omega,\emptyset\}$ e diciamo che $\bigcup_{i=1}^{n}E_i \in \mathcal{A}$ e che $2^{\Omega}$, anche noto come $P(\Omega)$ è l'insieme delle parti di $\Omega$.
\subsection{Funzione di probabilità}
Data $P:\mathcal{A} \rightarrow \mathbb{R}$, definiamo tre assiomi principali (detti \textit{Assiomi di Kolmogorov}):
\begin{enumerate}
	\item $\forall E \in \mathcal{A}\ \ \ \ 0 \leq P(E) \leq 1$
	\item $P(\Omega) = 1$
	\item \textit{Assioma di Kolmogorov}\\
	$\forall E_1,E_2,...,E_n\ \ \ \ \forall_iE_i \in \mathcal{A}\\
	\forall_{i \neq j}E_i \cap E_j = \emptyset \rightarrow P\left(\bigcup_{i=1}^{n} E_i\right) = \sum_{i=1}^{n}P(E_i)$
\end{enumerate}
\subsection{Operazioni}
\begin{description}
	\item \textbf{Intersezione:} $E \cap F$
	\item \textbf{Unione:} $E \cup F$
	\item \textbf{Esclusione:} $E \backslash F$
	\item \textbf{Complementare:} $\overline{E} = \Omega \backslash E$ (oppure $E^c$)
	\item \textbf{Sottoinsieme:} $E \subseteq F : \forall x \in \Omega\ x\in E \rightarrow x\in F$
	\item \textbf{AND:} $E \subseteq F \wedge F \subseteq E \rightarrow E = F$
\end{description}
\subsection{Proprietà di intersezione e unione}
\begin{description}
	\item \textbf{Commutativa:} $E \cap F = F \cap E$ e $E \cup F = F \cup E$
	\item \textbf{Associativa:} $(E \cap F) \cap G = E \cap (F \cap G)$ e $(E \cup F) \cup G = E \cup (F \cup G)$
	\item \textbf{Distributiva:} $E \cup (F \cap G) = (E \cup F) \cap (E \cup G)$
	\item \textbf{Leggi di De Morgan:} $\overline{E \cap F} = \overline{E} \cup \overline{F}$ e $\overline{E \cup F} = \overline{E} \cap \overline{F}$
\end{description}
\subsection{Spazio delle probabilità}
Definito come $(\Omega, \mathcal{A}, P)$.
\subsection{Spazi equiprobabili}
Tutti gli eventi elementari hanno probabilità costante $p = \frac{1}{N}$.\\
Con $\Omega$ infinito non esistono spazi equiprobabili, con $\Omega$ con soli eventi elementari con probabilità nulla non ha senso dato che la loro somma deve fare 1. Invece per $\Omega$ finito abbiamo $\Omega = \{e_1, e_2,..., e_n\}$, dunque $P(\{e_1\}) = P(\{e_2\}) = ... = P(\{e_n\}) = P$. In questo ultimo caso $\Omega = \bigcup_{i=1}^{N}\{e_i\}$ e dunque
\begin{equation}
    \begin{split}
        1 & = P(\Omega)\\
        & = P \left( \bigcup_{i=1}^{N}\{e_i\} \right )\\
        & = \sum_{i=1}^{N}P \left( \{e_i\} \right)\\
        & = \sum_{i=1}^{N}p\\
        & = Np
    \end{split}
\end{equation}
Dunque, in un contesto equiprobabile
\begin{equation}
    \begin{split}
        P(E) & = \sum_{j=1}^{k}P \left( \{e_j\} \right)\\
        & = \sum_{j=1}^{k} \frac{1}{N}\\
        & = \frac{k}{N}\\
        & = \frac{\mid E \mid}{N}\\
        & = \frac{\text{\# casi favorevoli}}{\text{\# casi possibili}}
    \end{split}
\end{equation}
\subsubsection{Esempio 1}
Consideriamo un'urna con 6 palle bianche e 5 palle nere. L'urna è stata scossa e dunque si trova in un contesto equiprobabile e io devo estrarre 2 palle senza reimmissione: qual è la probabilità che io estragga due colori diversi?.\\
Per trovare il numero dei casi possibili consideriamo che ho una sequenza (quindi l'ordine conta) senza reimmissione, dunque si tratta di una disposizione senza ripetizione data da $d_{11,2} = 11 \cdot 10 = 110$.\\
Il numero dei casi favorevoli è dato dalla possibilità che io estragga due colori differenti: sapendo che ho 6 palle bianche e 5 nere, la probabilità è data da $6 \cdot 5 = 30$. Tuttavia l'ordine conta e non ho vincoli sull'estrarre prima una pallina bianca, quindi considero anche i casi in cui prima estraggo una pallina nera e poi una bianca, portandoci ad un totale di 60.\\
Dunque $P(\text{estraggo due colori diversi}) = \frac{\mid E \mid}{N} = \frac{60}{110} = \frac{6}{11}$.
\subsubsection{Esempio 2}
In una commissione di 5 persone ci sono 6 uomini e 9 donne. Se vengono tutti scelti con la stessa probabilità, qual è la probabilità di avere 3 uomini e 2 donne?
\begin{equation}
    \begin{split}
        P(3U + 2D) & = \frac{\mid E \mid}{N}\\
        & = \frac{\binom{6}{3} \cdot \binom{9}{2}}{\binom{15}{5}}\\
        & = \frac{240}{1001}
    \end{split}
\end{equation}

\section{Teoremi fondamentali}
\subsection{Probabilità del complementare di un evento}
\textbf{Teorema}\\
$\forall E \in \mathcal{A}\ \ \ \ P(\overline{E}) = 1 - P(E)$\\
\textbf{Dimostrazione}
\begin{enumerate}
	\item Sappiamo che $P(\Omega) = 1$ e quindi $1 = P(\Omega)$
	\item Sappiamo che $E \cap \overline{E} = \emptyset$ e che $E \cup \overline{E} = \Omega$
	\item $1 = P(\Omega)$ diventa $1 = P(E \cup \overline{E})$
	\item Per l'assioma di Kolmogorov ottengo $1 = P(E \cup \overline{E}) = P(E) + P(\overline{E})$
	\item Dunque riscriviamo $1 = P(E) + P(\overline{E})$ come $1 - P(E) = P(\overline{E})$
\end{enumerate}
\subsection{Probabilità dell'unione di due eventi}
\textbf{Teorema}\\
$\forall E,F \in \mathcal{A}\ \ \ \ P(E \cup F) = P(E) + P(F) - P(E \cap F)$\\
\textbf{Dimostrazione}
\begin{enumerate}
	\item Consideriamo il seguente\\
	\begin{tikzpicture}[fill=orange!75]
		\begin{scope}
    		\clip (0,0) circle(1);
    		\clip (1,0) circle(1);
    		\fill (0,0) circle(1);
		\end{scope}

		\draw (0,0) circle (1) (0,1)  node [text=black,above] {$E$}
		      (1,0) circle (1) (1,1)  node [text=black,above] {$F$}
		      (-2,-2) rectangle (3,2) node [text=black,above] {$\Omega$};
		\node at (-.5,0) {\tiny $E \cap \overline{F}$};
		\node at (.5,0) {\tiny $E \cap F$};
		\node at (1.5,0) {\tiny $\overline{E} \cap F$};
	\end{tikzpicture}
	\item $(E \cap \overline{F}) \cap (E \cap F) = (E \cap E) \cap (F \cap \overline{F})$
	\item $(E \cap E) \cap (F \cap \overline{F}) = E \cap \emptyset$
	\item $E \cap \emptyset = \emptyset \implies (E \cap \overline{F}) \cap (E \cap F) = \emptyset$
	\item $E \cup F = (E \cap \overline{F}) \cup (E \cap F) \cup (\overline{E} \cap F)$
	\item $P(E \cup F) = P((E \cap \overline{F}) \cup (E \cap F) \cup (\overline{E} \cap F))$
	\item Per l'assioma di Kolmogorov ottengo $P(E \cup F) = P(E \cap \overline{F}) + P(E \cap F) + P(\overline{E} \cap F)$
	\item $P(E \cup F) = P(E) + P(\overline{E} \cap F)$
	\item $P(E \cup F) = P(E) + P(\overline{E} \cap F) + P(E \cap F) - P(E \cap F)$
	\item $P(E \cup F) = P(E) + P(F) - P(E \cap F)$
\end{enumerate}
\subsubsection{Esempio}
Sappiamo che il 28\% degli americani fuma le sigarette ($E$) e che il 7\% fuma i sigari ($F$). Il 5\% degli americani invece, fuma sia sigarette che sigari ($E \cap F$). Qual è la percenutale di non fumatori?
\begin{equation}
    \begin{split}
        P(\text{non fuma}) & = 1 - P(\overline{\text{non fuma}})\\
        & = 1 - P(\text{fuma})\\
        & = 1 - P(E \cup F)\\
        & = 1 - (P(E) + P(F) - P(E \cap F))\\
        & = 1 - (0.28 + 0.07 - 0.05)\\
        & = 0.7 \implies 70\% \text{ non fuma}
    \end{split}
\end{equation}
\subsection{Probabilità di un evento impossibile}
\textbf{Teorema}\\
$P(\emptyset) = \emptyset$\\
\textbf{Dimostrazione}
\begin{enumerate}
	\item $P(\Omega) = 1$
	\item $P(\overline{E}) = 1 - P(E)$
	\item $P(\emptyset) = P(\overline{\Omega})$
	\item $P(\emptyset) = 1 - P(\Omega)$ e dato che $P(\Omega) = 1$ allora $P(\emptyset) = \emptyset$
\end{enumerate}

\section{Probabilità condizionata}
Lanciando due dadi bilanciati, la probabilità che lanciando due dadi io ottenga 8, è data dal numero di casi favorevoli, ossia $\{(6,2), (5,3), (4,4), (3,5), (2,6)\}$, quindi 5, diviso il numero di casi possibili, ossia $6 \cdot 6 = 36$. Arrivando così ad avere $P(l_1 + l_2 = 8) = \frac{5}{36}$.\\
Prendendo sempre come esperimento il lancio di due dadi bilanciati, so che il primo dado vale 3 ma qual è la probabilità $P(\text{somma = 8})$? I casi possibili sono $\{(3,1), (3,2), (3,3), (3,4), (3,5), (3,6)\}$ ma solo $(3,5)$ è favorevole, quindi posso dire che
\begin{equation}
    P(E \mid F) = \frac{P(E \cap F)}{P(F)}
\end{equation}
Il che significa che, la probabilità tirato 3 al primo dado ($F$), allora ($\mid$) la somma dei dadi sia 8 ($E$), ossia equivale a dire che tutti gli eventi al di fuori di $F$ non possono verificarsi. Dunque supponendo che $P(F) \neq 0$ e che infatti $P(F) = \frac{6}{36} = \frac{1}{6}$, e sapendo che $P(E \cap F) = P(\{3,5\}) = \frac{1}{36}$, possiamo dire che $P(E \mid F) = \frac{P(E \cap F)}{P(F)} = \frac{\frac{1}{36}}{\frac{1}{6}} = \frac{1}{6}$.
\subsubsection{Esempio}
Avendo una confezione di 40 pennarelli di cui 5 guasti, 10 difettosi e 25 accettabili, dopo averli mescolati, mi chiedo qual è la probabilità che ne abbia pescato uno accettabile?\\
Definisco con $E$ quelli accettabili e con $F$ quelli non guasti (difettosi o accettabili)
\begin{equation}
    \begin{split}
        P(E \mid F) & = \frac{P(\text{accettabile)}}{1 - P(\text{guasto})}\\
        & = \frac{\frac{25}{40}}{1-\frac{5}{40}}\\
        & = \frac{\frac{25}{40}}{\frac{35}{40}}\\
        & = \frac{5}{7}
    \end{split}
\end{equation}
\subsection{Regola di fattorizzazione}
Essa asserisce che
\begin{equation}
    P(A \cap B) = P(A \mid B) \cdot P(B)
\end{equation}
\subsubsection{Esempio}
Sappiamo che la probabilità che apra un nuovo ufficio è data da $P(U) = 0.3$, inoltre sappiamo che la probabilità che io diventi direttore dell'ipotetico nuovo ufficio è $P(M \mid U) = 0.6$, dunque la probabilità che apra un nuovo ufficio e che io diventi direttore è data da $P(U \cap M) = P(M \mid U) \cdot P(U) = 0.3 \cdot 0.6 = 0.18$
\subsection{Teorema delle probabilità totali}
Ipotizzo di avere due eventi $E,F \in \mathcal{A}$, allora
\begin{equation}
    \begin{split}
        P(E) & = P((E \cap \overline{F}) \cup (E \cap F))\\
        & = P(E \cap \overline{F}) + P(E \cap F)\\
        & = P(E \mid \overline{F}) \cdot P(\overline{F}) + P(E \mid F) \cdot P(F)\\
        & = P(E \mid F) \cdot P(F) + P(E \mid \overline{F}) \cdot P(\overline{F})\\
        & = P(E \mid F) \cdot P(F) + P(E \mid \overline{F}) \cdot (1 - P(F))
    \end{split}
\end{equation}
\subsubsection{Esempio}
Un cliente può essere incline ad un incidente nel prossimo anno con una probabilità $P(\text{incline}) = 0.4$ oppure non incline $P(\text{incline}) = 0.2$. I primi compongono il 30\% dei clienti mentre i secondi il 70\% dei clienti. Calcolare la probabilità che il prossimo anno avverrà un incidente.\\
Innanzitutto definiamo con $A$ l'incidente nel prossimo anno e con $H$ il cliente incline agli incidenti. Dati $P(H) = 0.3$, $P(\overline{H}) = 0.7$, $P(A \mid H) = 0.4$ e $P(A \mid \overline{H})$, possiamo dire che $P(A) = P(A \mid H) \cdot P(H) + P(A \mid \overline{H}) \cdot P(\overline{H}) = 0.4 \cdot 0.3 + 0.2 \cdot 0.7 = 0.26$.
\subsection{Formula generalizzata del teorema delle probabilità totali}
Ipòotizziamo di avere una partizione di $\Omega$ con $F_1, F_2,..., F_n \subseteq \Omega$ le sue parti costituenti tali che $\bigcup_{i=1}^{n}F_i = \Omega\ :\ \forall i=j\ F_i \cap F_j = \emptyset$.
\begin{center}
\begin{tikzpicture}[fill=orange!75]
	\begin{scope}
    		\fill (0,0) ellipse (2 and 1);
	\end{scope}

	\draw (0,0) ellipse (2 and 1) (0,1) node [text=black,above] {$E$}
	      (-2.5,-2) rectangle (2.5,2) node [text=black,above] {$\Omega$}
	      (-1.5,-2) -- (-1.5,2) (-2,-2) node [text=black,above] {$F_1$}
	      (-.5,-2) -- (-.5,2) (-1,-2) node [text=black,above] {$F_2$}
	      (0,-2) node [text=black,above] {$F_3$}
	      (.5,-2) -- (.5,2) (1,-2) node [text=black,above] {$F_4$}
	      (1.5,-2) -- (1.5,2) (2,-2) node [text=black,above] {$F_5$};
\end{tikzpicture}
\end{center}
\begin{equation}
    \begin{split}
        P(E) & = P\left(\bigcup_{i=1}^{n}(E \cap F_i)\right)\\
        & = \sum_{i=1}^{n}P(E \cap F_i)\\
        & = \sum_{i=1}^{m}P(E \mid F_i) \cdot P(F_i)
    \end{split}
\end{equation}
\subsubsection{Esempio}
\begin{center}
    \begin{tabular}{ c | c c }
        & pezzi difettosi & \% pezzi prodotti sul totale \\
        \hline
        A & 2 & 60 \\
        B & 3 & 30 \\
        C & 4 & 10 \\
    \end{tabular}
\end{center}
Calcolo la probabilità di un pezzo difettoso, assegnando a $D$ il pezzo difettoso e ad $A$ il pezzo prodotto dal macchinario $A$, $B$ dal macchinario $B$ e $C$ dal macchinario $C$.\\
$P(D) = P(D \mid A) \cdot P(A) + P(D \mid B) \cdot P(B) + P(D \mid C) \cdot P(C)$ ossia $P(D) = 0.02 \cdot 0.6 + 0.03 \cdot 0.3 + 0.04 \cdot 0.1 = 0.025$.
\subsection{Teorema di Bayes}
Ipotizzo un test diagnostico tramite analisi del sangue che funziona il 99\% dei casi mentre l'1\% dei casi ritorna falsi positivi. La malattia che si vuole analizzare affligge lo $0.5\%$ della popolazione. L'esito positivo $E$, e il caso di malattia $M$, sono legati da $P(E \mid M) = 0.99$ (vero-positivo), $P(E \mid \overline{M}) = 0.01$ (falso-positivo) e $P(\overline{E} \mid \overline{M})$ (vero-negativo). Qual è la probabilità che io sia malato dato che l'esito è positivo? No, la risposta non è 99\%, quella è la probabilità che l'esito sia positivo dato che sono malato. Se lo $0.5\% = \frac{1}{200}$ della popolazione soffre di questo male, in media su 200 persone vi sarà un solo malato. Se egli si sottopone alle analisi, verrà trovato positivo quasi certamente (con probabilità $0.99$). D'altro canto le $199$ persone sane hanno una probabilità di $0.01$ di risultare positive e dunque $199 \cdot 0.01 = 1.99$ falsi positivi su $200$. Dunque si ricava che la frazione di malati reali tra i soggetti positivi alle analisi è di $\frac{0.99}{0.99 + 1.99} \approx 0.3322$ e, per essere estensivi
\begin{equation}
    \begin{split}
        P(M \mid E) & = \frac{P(M \cap E)}{P(E)}\\
        & = \frac{P(E \cap M)}{P(E)}\\
        & = \frac{P(E \mid M)P(M)}{P(E)}\\
        & = \frac{P(E \mid M)P(M)}{P(E \mid M)P(M) + P(E \mid \overline{M})P(\overline{M})}\\
        & = \frac{0.99 \cdot 0.005}{0.99 \cdot 0.005 + 0.01 \cdot 0.995} \approx 0.3322
    \end{split}
\end{equation}
Trovandoci quindi nelle stesse ipotesi del teorema delle probabilità totali, possiamo definire il teorema di Bayes come
\begin{equation}
    \begin{split}
        P(F_j \mid E) & = \frac{P(F_j \cap E)}{P(E)}\\
        & = \frac{P(E \mid F_j)P(F_j)}{\sum_{i=1}^{n}P(E \mid F_i)P(F_i)}
    \end{split}
\end{equation}
Dunque non c'è necessariamente una relazione causa-effetto tra evento condizionato ed evento condizionante e con dovuto manipolazioni si possono invertire.
\subsubsection{Esempio 1}
Ho un sospetto di colpevolezza $P(C) = 0.6$ e so che il sospettato è mancino, questa cosa influenza? So che il colpevole è macino (al 100\%) allora $P(M \mid C) = 1$, mentre la probabilità di essere mancini è $P(M) = 0.2$. Dunque $P(C \mid M) = \frac{P(M \mid C)P(C)}{P(M \mid \overline{C})P(\overline{C}) + P(M \mid C)P(C)}$, ma possiamo vedere $P(M \mid \overline{C})$ come semplicemente $P(M)$ e quindi $P(C \mid M) \approx 0.87$.
\subsubsection{Esempio 2}
Un aereo è scomparso e si suppone che possa essere caduto in una qualsiasi di tre regioni, con uguale probabilità. Per $i = 1,2,3$, siano $\alpha_i$ le costanti che rappresentano la probabilità di non rinvenire il velivolo nella regione i-esima e $1 - \alpha_i$ la probabilità di rintracciare un velivolo che cada nella regione i-esima. Qual è la probabilità che l'aereo si trovi in ciascuna delle tre regioni se una ricerca della regione 1 ha dato esito negativo?\\
Denotiamo con $R_i$ l'evento "il velivolo si trova nella regione i-esima" e sia $E$ l'evento "la ricerca nella regione 1 non ha successo", dunque
\begin{equation}
    \begin{split}
        P(R_1 \mid E) & = \frac{P(E \mid R_1)P(R_1)}{\sum_{i=1}^{3}P(E \mid R_i)P(R_i)} = \frac{\frac{\alpha_1}{3}}{\frac{\alpha_1}{3} + \frac{1}{3} + \frac{1}{3}} = \frac{\alpha_1}{\alpha_1 + 2}\\
        P(R_2 \mid E) & = \frac{P(E \mid R_2)P(R_2)}{\sum_{i=1}^{3}P(E \mid R_i)P(R_i)} = \frac{\frac{1}{3}}{\frac{\alpha_1}{3} + \frac{1}{3} + \frac{1}{3}} = \frac{1}{\alpha_1 + 2}\\
        P(R_3 \mid E) & = \frac{P(E \mid R_3)P(R_3)}{\sum_{i=1}^{3}P(E \mid R_i)P(R_i)} = \frac{\frac{1}{3}}{\frac{\alpha_1}{3} + \frac{1}{3} + \frac{1}{3}} = \frac{1}{\alpha_1 + 2}
    \end{split}
\end{equation}
Quindi se ad esempio fosse $\alpha_1 = 0.4$, la probabilità che il velivolo sia nella prima regione nonostante cercandolo lì non sia stato trovato, sarebbe di $\frac{1}{6}$.
\subsection{Teorema Naive-Bayes}
Ci piacciono i supereroi Marvel $M$ e gli occhi neri $N$, vogliamo capire se $P(M \mid N) = \frac{P(N \mid M)P(M)}{P(N)}$. Guardo la frequenza con cui un supereroe Marvel ha gli occhi neri, se si avvicina a 1 allora tendenzialmente, se il supereroe ha gli occhi neri, allora è della Marvel, altrimenti no. Se però vogliamo prendere in considerazione il colore degli occhi $O$ (neri o no) e il colore dei capelli $C$ (neri o no), allora diventa
\begin{equation}
    P(M \mid C=c \cap O=o) = \frac{P(C=c \cap O=o \mid M)P(M)}{P(C=c \cap O=o)}
\end{equation}
questo può essere approssimato (e per questo naive), prendendo spunto dal terzo assioma di Kolmogorov, come
\begin{equation}
    P(M \mid C=c \cap O=o) \approx \frac{P(C=c \mid M) \cdot P(O=o \mid M) \cdot P(M)}{P(C=c \cap O=o)}
\end{equation}
Questo non accade sempre, però se ho $m$ colori degli occhi e $n$ colori dei capelli, allora ho $m \cdot n$ stime e mi conviene approssimare, perché almeno mi ritrovo con solo $m + n$ stime.\\
Dunque, formalizzando, date $m$ possibili classi $Y = y_k$ (si verifica quando il supereroe appartiene alla classe k-esima) e $n$ attributi (features) $X_i = x_i$ (si verifica quando l'i-esimo degli attributi assume un determinato valore), ottengo
\begin{equation}
    \begin{split}
        P(Y = y_k \mid X_1 = x_1, X_2 = x_2,..., X_n = x_n) & = \frac{P(X_1 = x_1, X_2 = x_2,..., X_n = x_n \mid Y = y_k)P(Y = y_k)}{P(X_1 = x_1, X_2 = x_2,..., X_n = x_n)}\\
        & \approx \frac{\prod_{i=1}^{n}P(X_i = x_i \mid Y = y_k)P(Y = y_k)}{P(X_1 = x_1, X_2 = x_2,..., X_n = x_n)}\\
        & \rightarrow \text{ se } P(X_1 = x_1, X_2 = x_2,..., X_n = x_n) \text{ non dipende da } k\\
        & = \text{argmax}_k \prod_{i=1}^{n}P(X_i = x_i \mid Y = y_k)P(Y = y_k)
    \end{split}
\end{equation}
\subsection{Indipendenza}
Nel caso avessimo $P(E \mid F) = P(E)$ significa che, sapere che si è verificato $F$, non ci dice nulla di più su $E$. Definiamo questo concetto come \textit{indipendenza} tra $E$ e $F$, scritto $E \bot F$. Dato che $\frac{P(E \cap F)}{P(F)} = P(E)$ allora $P(E \cap F) = P(E) \cdot P(F)$. Di seguito alcune considerazioni:
\begin{enumerate}
	\item \textbf{Teorema}\\
	$E,F\ \text{indipendenti} \implies E,\overline{F}\ \text{indipendenti}$\\
	\textbf{Dimostrazione}
	\begin{enumerate}
		\item $E = (E \cap F) \cup (E \cap \overline{F})$
		\item $P(E) =  P(E \cap F) + P(E \cap \overline{F})$
		\item $P(E \cap \overline{F}) = P(E) - P(E \cap F) = P(E) - P(E) \cdot P(F) = P(E) \cdot (1 - P(F)) = P(E) \cdot P(\overline{F})$
	\end{enumerate}
	\item Ipotizzo un esperimento in cui lancio due dadi. Sappiamo che la somma vale 7 ($E$) e che il primo vale 4 ($F$) mentre il secondo vale 3 ($G$), dunque $P(E) = P(F) = P(G) = \frac{1}{6}$ e $P(E \cap F) = P(E \cap G) = \frac{1}{36}$. Però cosa posso dire di $P(E \mid F \cap G)$? Beh, $P(E \mid F \cap G) = 1$, dato che $4 + 3 = 7$, ma come estendo questo concetto?\\
	Dati $E,F,G$ indipendenti, allora $P(E \cap F \cap G) = P(E) \cdot P(F) \cdot P(G)$.
	\item Ipotizzo di avere $n$ eventi $E_1,...,E_n$ che sono indipendenti se e solo se $\forall r = 2,...,n\ \forall\ 1 \leq \alpha_1 < ... < \alpha_r \leq n\ \ \ \ P \left( \bigcap_{i=1}^{r}E_{\alpha_1} \right) = \prod_{i=1}^{r}P \left( E_{\alpha_1} \right)$. Questo concetto lo posso applicare per esempio ai sistemi in serie e in parallelo.\\
    In un sistema in serie $\forall i = 1,...,n\ p_i = P(\text{l'i-esimo componente funziona})$, dunque $P(\text{il sistema funziona}) = P(\text{tutti i componenti funzionano})$ e quindi $P\left(\bigcap_{i=1}^{n}\text{componente i-esimo funziona}\right) = \prod_{i=1}^{n}p_i$.\\
    Nel caso del sistema in parallelo invece $P(\text{il sistema funziona}) = 1 - P(\text{il sistema non funziona})$ (ossia il complementare). Siccome è in parallelo, anche se un componente non funziona non importa, tutti i componenti non devono andare per far sì che il sistema non funzioni, dunque
    \begin{equation}
        \begin{split}
            P(\text{il sistema funziona}) & = 1 - P(\text{il sistema non funziona})\\
            & = 1 - P(\text{tutti i componenti non funzionano})\\
            & = 1 - P\left( \bigcap_{i=1}^{n}\text{componente i-esimo non funziona} \right)\\
            & = 1 - \prod_{i=1}^{n}P(\text{componente i-esimo non funziona})\\
            & = 1 - \prod_{i=1}^{n}(1 - p_i)
        \end{split}
    \end{equation}
\end{enumerate}
\subsubsection{Esempio 1}
Pesco una carta da un mazzo di 52 carte mischiato in maniera equiprobabile. La probabilità che io peschi un asso è data da $P(A) = \frac{4}{52}$ mentre la probabilità che io peschi cuori è data da $P(C) = \frac{13}{52}$. Siccome $A \bot C$, la probabilità che io peschi un asso di cuori è $P(A \cap C) = P(A) \cdot P(C) = \frac{4}{52} \cdot \frac{13}{52} = \frac{1}{52}$.
\subsubsection{Esempio 2}
Considero $E$ e $F \cup G$ dunque
\begin{equation}
    \begin{split}
        P(E \cap (F \cup G)) & = P((E \cap F) \cup (E \cap G))\\
        & = P(E \cap F) + P(E \cap G) - P((E \cap F) \cap (E \cap G))\\
        & = P(E)P(F) + P(E)P(G) - P(E \cap F \cap G)\\
        & = P(E)P(F) + P(E)P(G) - P(E)P(F)P(G)\\
        & = P(E)(P(F) + P(G) - P(F)P(G))\\
        & = P(E)(P(F) + P(G) - P(F \cap G))\\
        & = P(E)P(F \cup G)
    \end{split}
\end{equation}
\subsubsection{Esempio 3}
Considero una raccolta di figurine numerate da 1 a $n$. Per $j = 1, 2,..., n$ sia $p_j$ la probabilità di ottenere la figurina $j$, con $\sum_{j=1}^{n}p_j = 1$. Determinare la probabilità che una collezione di $k$ figurine ottenute indipendentemente contenga almeno un esemplare di quella numero $j$, sapendo che ne contiene almeno uno di quella numero $i$.\\
Iniziamo col denotare $A_r$ l'evento che la collezione contenga almeno una figurina numero $r$, allora per definizione $P(A_j \mid A_i) = \frac{P(A_j \cap A_i)}{P(A_i)}$. Da questo possiamo calcolare $P(A_i)$ come
\begin{equation}
    \begin{split}
        P(A_i) & = 1 - P(\overline{A_i})\\
        & = 1 - P(\text{nessuna figurina ha numero i})\\
        & = 1 - P \left( \bigcap_{r=1}^{k}\text{all'r-esimo acquisto nessuna figurina ha numero i} \right )\\
        & = 1 - \prod_{i=1}^{n}P(\text{all'r-esimo acquisto nessuna figurina ha numero i})\\
        & = 1 - \prod_{i=1}^{n}(1 - p_i)\\
        & = 1 - (1 - p_i)^k
    \end{split}
\end{equation}
e possiamo calcolare $P(A_j \cap A_i)$ come
\begin{equation}
    \begin{split}
        P(A_j \cap A_i) & = 1 - P(\overline{A_j \cap A_i})\\
        & = 1 - P(\overline{A_j} \cup \overline{A_i})\\
        & = 1 - (P(\overline{A_j}) + P(\overline{A_i}) - P(\overline{A_j} \cap \overline{A_i}))\\
        & = 1 - ((1 - p_j)^k + (1 - p_i)^k - P(\overline{A_j} \cap \overline{A_i}))\\
        & = 1 - ((1 - p_j)^k + (1 - p_i)^k - P(\text{mai né i né j}))\\
        & = 1 - \left((1 - p_j)^k + (1 - p_i)^k - P\left(\bigcap_{r=1}^{k}\text{all'r-esimo acquisto né i né j}\right)\right)\\
        & = 1 - \left( (1 - p_j)^k + (1 - p_i)^k - \prod_{r=1}^{k}(1 - p_i - p_j) \right )\\
        & = 1 - ((1 - p_j)^k + (1 - p_i)^k - (1 - p_i - p_j)^k)\\
        & = 1 - (1 - p_j)^k - (1 - p_i)^k + (1 - p_i - p_j)^k
    \end{split}
\end{equation}
e per concludere possiamo quindi dire che
\begin{equation}
    P(A_j \mid A_i) = \frac{1 - (1 - p_j)^k - (1 - p_i)^k + (1 - p_i - p_j)^k}{1 - (1 - p_i)^k}
\end{equation}
\section{Variabile aleatoria}
Una variabile aleatoria è definita in uno spazio di probabilità $(\Omega, \mathcal{A}, P)$ con $X : \Omega \rightarrow \mathbb{R}$, ossia le immagini di $X$ sono una codifica numerica dei possibili esiti di un esperimento aleatorio. L'evento $X = \alpha$ si verificherà quando avrò estratto un evento di $\Omega$ tale che $X(\text{evento}) = \alpha$, per l'esattezza
\begin{equation}
    \{X = \alpha\} \equiv \{\omega \in \Omega : X(\omega) = \alpha\}
\end{equation}
o meglio
\begin{equation}
    P(X = \alpha)
\end{equation}
In particolare definiamo con \textbf{specificazione} il valore dell'evento permettendoci di dire che essa determina l'occorrenza di un evento $X$.
\subsection{Esempio}
Considero l'esperimento del lancio di due dadi: $\Omega$ è il lancio dei due dadi, $X$ è la somma dei due esiti (tutti i numeri da 2 a 12), ma bisogna chiedersi se $X$ assumerà i suoi valori con la stessa probabilità. La risposta è no. Prendo per esempio $X=3$
\begin{equation}
    \{X=3\} \equiv \{\omega \in \Omega : X(\omega) = 3\} = \{(1,2), (2,1)\} \implies P(X=3) = \frac{2}{36}
\end{equation}
Ora osservo $X=9$
\begin{equation}
    \{X=9\} \equiv \{\omega \in \Omega : X(\omega) = 9\} = \{(3,6), (4,5), (5,4), (6,3)\} \implies P(X=9) = \frac{4}{36}
\end{equation}
Si può notare come il picco sia a $P(X=7) = \frac{6}{36}$ e andando verso il 2 o verso il 12 i numeri diminuiscano fino a raggiungere $\frac{1}{36}$ da entrambi i lati. Inoltre evidenziamo come
\begin{equation}
    \begin{split}
        \sum_{i=2}^{12}P(X=i) & = P\left(\bigcup_{i=2}^{12}\{X=i\}\right)\\
        & = P(\Omega)\\
        & = 1
    \end{split}
\end{equation}

\section{Funzione indicatrice}
Definiamo un'altra variabile aleatoria a partire dallo stesso esperimento del lancio dei due dadi e chiamiamola $I$, ossia l'esito del primo lancio. E notiamo che $P(\mathbb{I}=i) = \frac{1}{6}\ \ \ \forall i=1,...,6$. Dunque chiamiamo la funzione indicatrice (o \textit{caratteristica}), la funzione tale che, dato un evento $A \subseteq \mathbb{R}$ e $I_A : \mathbb{R} \rightarrow \{0,1\}$ allora
\begin{equation}
    I_A(x) =
    \begin{cases}
    	1 \text{ se } x \in A\\
    	0 \text{ se } x \notin A
    \end{cases}
\end{equation}
e quindi $P(\mathbb{I} = i) = \frac{1}{6} \mathbb{I}_{\{1,...,6\}}(i)$.
\subsection{Esempio 1}
Acquisto 2 componenti elettronici: la probabilità che uno sia difettoso è $P(d) = 0.3$ e la probabilità che uno sia funzionante è $P(f) = 0.7$. Nell'ipotesi in cui ci sia indipendenza tra gli eventi, allora
\begin{equation}
	\Omega = \{(d,d), (d,f), (f,d), (f,f)\}
\end{equation}
e dunque, identificando con $X$ il numero di componenti funzionanti ($X$ può assumere valori da 0 a 2 ovviamente)
\begin{equation}
    \begin{split}
        P(X=0) & = \{(d,d)\} = 0.3 \cdot 0.3 = 0.09\\
        P(X=1) & = \{(d,f), (f,d)\} = (0.3 \cdot 0.7) + (0.7 \cdot 0.3) = 0.42\\
        P(X=2) & = \{(f,f)\} = 0.7 \cdot 0.7 = 0.49\\
    \end{split}
\end{equation}
o più precisamente possiamo costruire la funzione indicatrice di un evento che ci dice se almeno un componente funziona
\begin{equation}
    \mathbb{I} =
    \begin{cases}
    	1 \text{ se almeno un componente funziona } & \rightarrow P(\mathbb{I}=1) = 0.42 + 0.49 = 0.91\\
    	0 \text{ altrimenti } & \rightarrow P(\mathbb{I}=0) = 0.09
    \end{cases}
\end{equation}
\subsection{Esempio 2}
Immaginiamo di avere tre eventi $A$, $B$ e $C$, e voglio definire una funzione indicatrice che restituisca 1 se e solo se almeno due degli eventi si verificano contemporaneamente. Posso scrivere questa funzione indicatrice come $I_{(A \cap B) \cup (A \cap C) \cup (B \cap C)}(x) = 1$ se $x$ appartiene a uno degli insiemi dati.

\section{Funzione di ripartizione di distribuzione cumulativa}
Anche nota come \textit{f.d.c.} (o \texttt{cdf} su Python) essa è definita per $F_x : \mathbb{R} \rightarrow [0,1]$, ci dice che $\forall x \in \mathbb{R}\ \ \ \ F_X(x) = P(X \leq x)$. Una particolare variabile aleatoria è distribuita seconda una particolare funzione di ripartizione, così da avere $X \sim F_X$, nel dettaglio
\begin{equation}
    \{X \leq b\} = \{X \leq a\} \cup \{a < X \leq b\}
\end{equation}
e applicando il terzo assioma di Kolmogorov
\begin{equation}
    \begin{split}
        & P(X \leq b) = P(X \leq a) + P(a < X \leq b)\\
        & F_X(b) = F_X(a) + P(a < X \leq b)\\
        & P(a < X \leq b) = F_X(b) - F_X(a)
    \end{split}
\end{equation}

\section{Variabili aleatorie discrete}
Variabili aleatorie con un numero finito o numerabile di valori possibili sono dette discrete.
\subsection{Funzione di massa di probabilità}
$$p_X : \mathbb{R} \rightarrow [0,1]\ \ \ \forall x \in \mathbb{R}\ \ \ p_X(x) = P(X=x)$$
Proprietà:
\begin{enumerate}
	\item $p_X(x) \geq 0\ \ \ \forall x \in \mathbb{R}$
	\item $\sum_{i=1}^{n}p_X(x_i) = 1$
\end{enumerate}
\subsubsection{Esempio}
Ipotizzo di avere una variabile aleatoria $X$ discreta tale che $D_X = \{1,2,3\}$, $P(X=1) = \frac{1}{2}$ e $P(X=2) = \frac{1}{3}$. Mi domando quindi, quanto vale $P(X=3)$?
\begin{equation}
    \begin{split}
        & p_X(1) + p_X(2) + p_X(3) = 1\\
        & p_X(3) = 1 - \frac{1}{2} - \frac{1}{3} = \frac{1}{6}
    \end{split}
\end{equation}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{fdmp}
\end{center}
\subsection{Funzione di ripartizione}
Sappiamo che
\begin{equation}
    \begin{split}
        F_X(x) & = P(X \leq x)\\
        & = P \left( \bigcup_{a \leq x} \{X = a\} \right)\\
        & = \sum_{a \leq x} P(X = a)\\
        & = \sum_{a \leq x} p_X(a)
    \end{split}
\end{equation}
Inoltre
\begin{equation}
    \begin{split}
            & \forall x \in \mathbb{R}\ \ \ F_X(x) \geq 0\\
            & \lim_{x \to \infty} F_X(x) = 1
    \end{split}
\end{equation}
Quindi $F_X$ continua da destra, ma non continua da sinistra.
\subsubsection{Esempio}
Prendo l'esempio di prima in cui ipotizzo di avere una variabile aleatoria $X$ discreta tale che $D_X = \{1,2,3\}$, $P(X=1) = \frac{1}{2}$ e $P(X=2) = \frac{1}{3}$.
\begin{equation}
	F_X(x) =
	\begin{cases}
        0           & \text{se } x \leq 1 \\
		\frac{1}{2} & \text{se } 1 \leq x \leq 2\\
        \frac{5}{6} & \text{se } 2 \leq x \leq 3\\
		1           & \text{se } x \geq 3
    \end{cases}
\end{equation}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{fr}
\end{center}
\begin{warning}
Nel capitolo dei modelli discreti, per semplicità pratica, uso degli step-plot. Tuttavia è questo qui sopra il grafico preciso da utilizzare quando si ha a che fare con funzioni di ripartizione discrete.
\end{warning}

\section{Valore atteso}
Il valore atteso di una variabile aleatoria $X$ è definito come
\begin{equation}
    \mathbb{E}(X) = \sum_{i=1}^{n}x_iP(X=x_i)
\end{equation}
\subsubsection{Esempio 1}
Dato $X$ il punteggio di un dado bilanciato e $D_X = \{1,...,6\}$, $p_X(x) = \frac{1}{6}I_{\{1,...,6\}}(x)$, allora
\begin{equation}
	\begin{split}
		\mathbb{E}(X) & = \sum_{x=1}^{6}x \cdot p_X(x)\\
		& = \sum_{x=1}^{6}x \frac{1}{6}\\
		& = \frac{1}{6}\sum_{x=1}^{6} x
	\end{split}
\end{equation}
e sapendo che $\sum_{x=1}^{n}x = \frac{n(n+1)}{2}$, allora $\mathbb{E}(X) = \frac{1}{6}\sum_{x=1}^{6}x = \frac{1}{6} \cdot \frac{6 \cdot 7}{2} = \frac{7}{2}$.
\subsubsection{Esempio 2}
Considerando $X$ la mia vincita ad un gioco d'azzardo e sapendo che gioco $n$ volte, la mia vincita media non è altro che $\frac{n_1x_1 + ... + n_kx_k}{n}$ ossia
\begin{equation}
    \sum_{i=1}^{k}\frac{n_i}{n}x_i \approx \sum_{i=1}^{k}P(X=x_i) \cdot x_i
\end{equation}
\subsubsection{Esempio 3}
Avendo un evento $A$ e sapendo che $D_{I_A} = \{0,1\}$, con $I_A$ che vale 0 se $A$ non si verifica o 1 se $A$ si verifica, allora $\mathbb{E}(I_A) = P(A)$, dato che la probabilità che la variabile aleatoria sia uguale a 1 avviene quando $A$ si verifica.
\subsubsection{Esempio 4}
Consideriamo $g(x) = y$ ossia $g(x) = x^2$ e dunque $x = \sqrt{y}$
\begin{center}
\begin{tabular}{ c | c | c }
x & $P(X=x)$ & y \\
\hline
0 & 0.2 & 0 \\
1 & 0.5 & 1 \\
2 & 0.3 & 4 \\
\end{tabular}
\end{center}
Possiamo quindi dire che
\begin{equation}
    \begin{split}
        & \mathbb{E}(y) = 0 \cdot 0.2 + 1 \cdot 0.5 + 4 \cdot 0.3 = 1.7\\
        & \mathbb{E}(x^2) = 0^2 \cdot 0.2 + 1^2 \cdot 0.5 + 2^2 \cdot 0.3 = 1.7
    \end{split}
\end{equation}
e più in generale
\begin{equation}
    \begin{split}
        \mathbb{E}(g(x)) & = \sum_{i}g(x_i) \cdot P(X=x_i)\\
        & = \sum_{i}g(x_i)p_X(x_i)
    \end{split}
\end{equation}
\subsubsection{Esempio 5}
Un'urna contiene $3$ palline contrassegnate dai numeri $\{1, 2, 3\}$. Si estrae con riposizione un campione di ampiezza $2$. Sia $Y$ la variabile casuale che esprime la media aritmetica dei numeri riportati sulle palline estratte. Calcolare il valore atteso di $Y$.\\
Le possibili combinazioni sono
$$\{(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)\}$$
Ora calcoliamo la media aritmetica $Y$ per ciascuna combinazione
\begin{equation}
	\begin{split}
		Y_1 & = \frac{1 + 1}{2} = 1\\
		Y_2 & = \frac{1 + 2}{2} = 1.5\\
		Y_3 & = \frac{1 + 3}{2} = 2\\
		Y_4 & = \frac{2 + 1}{2} = 1.5\\
		Y_5 & = \frac{2 + 2}{2} = 2\\
		Y_6 & = \frac{2 + 3}{2} = 2.5\\
		Y_7 & = \frac{3 + 1}{2} = 2\\
		Y_8 & = \frac{3 + 2}{2} = 2.5\\
		Y_9 & = \frac{3 + 3}{2} = 3
	\end{split}
\end{equation}
Da qui possiamo per esempio vedere che il valore $1$ compare $1$ volta, il valore $1.5$ compare $2$ volte e così via, dunque, data la formula del valore atteso, abbiamo
\begin{equation}
	\begin{split}
		\mathbb{E}(X) & = \sum_{i=1}^{n}x_iP(X=x_i)\\
		& = 1 \cdot \frac{1}{9} + 1.5 \cdot \frac{2}{9} + 2 \cdot \frac{3}{9} + 2.5 \cdot \frac{2}{9} + 3 \cdot \frac{1}{9}\\
		& = \frac{18}{9}\\
		& = 2
	\end{split}
\end{equation}

\subsection{Considerazione}
Dato $g(x) = ax + b$, allora
\begin{equation}
    \begin{split}
        \mathbb{E}(g(x)) & = \mathbb{E}(ax + b)\\
        & = \sum_{i}(ax_i + b) \cdot P(X=x_i)\\
        & = \sum_{i}(ax_iP(X=x_i) + bP(X=x_i))\\
        & = a\sum_{i}x_iP(X=x_i) + b\sum_{i}P(X=x_i)\\
        & = a\mathbb{E}(X) + b
    \end{split}
\end{equation}
E dunque se $a = 0$ allora $\mathbb{E}(b) = b$, mentre se $b = 0$ allora $\mathbb{E}(aX) = a\mathbb{E}(X)$.
\section{Varianza}
Possiamo vedere quanto una $X$ si discosta dal valore atteso, dunque $g(X) = \mid X - \mathbb{E}(X) \mid$. Tuttavia cerchiamo di togliere il valore assoluto, ottenendo quindi la varianza di $X$ come
\begin{equation}
    Var(X) = \mathbb{E}((X - \mathbb{E}(X))^2)
\end{equation}
Da qui in poi useremo nuove nomenclature quali $\mu = \mathbb{E}X$ e $Var(X) = \mathbb{E}((X - \mu)^2)$. Inoltre
\begin{equation}
    \begin{split}
        Var(X) & = \mathbb{E}((X - \mu)^2)\\
        & = \mathbb{E}(X^2 - 2 \mu X + \mu^2)\\
        & = \mathbb{E}(X^2) - 2\mu\mathbb{E}(X) + \mu^2\\
        & = \mathbb{E}(X^2) - 2\mu^2 + \mu^2\\
        & = \mathbb{E}(X^2) - \mu^2\\
        & = \mathbb{E}(X^2) - \mathbb{E}(X)^2
    \end{split}
\end{equation}
Proseguendo posso anche notare che
\begin{equation}
    \begin{split}
        Var(ax + b) & = \mathbb{E}(((ax + b) - (\mathbb{E}(ax + b)))^2)\\
        & = \mathbb{E}((ax + b - a\mathbb{E}(x) - b)^2)\\
        & = \mathbb{E}((ax - a\mathbb{E}(x))^2)\\
        & = \mathbb{E}(a^2(x - \mathbb{E}(x))^2)\\
        & = a^2\mathbb{E}((x - \mathbb{E}(x))^2)\\
        & = a^2Var(X)
    \end{split}
\end{equation}
e che, considerando $\mathbb{I}_A = 1$ se $A$ si verifica, 0 altrimenti, allora
\begin{equation}
    \begin{split}
        Var(\mathbb{I}_A) & = \mathbb{E}(\mathbb{I}_A^2) - \mathbb{E}(\mathbb{I}_A)^2\\
        & = \mathbb{E}(\mathbb{I}_A) - \mathbb{E}(\mathbb{I}_A)^2\\
        & = \mathbb{E}(\mathbb{I}_A)(1 - \mathbb{E}(\mathbb{I}_A))\\
        & = P(A) \cdot P(\overline{A})
    \end{split}
\end{equation}
\subsection{Esempio 1}
Sapendo che $p_X(x) = \frac{1}{6} \mathbb{I}_{D_X}(x)$ e che $\mu = \frac{7}{2}$, allora
\begin{equation}
    \begin{split}
        Var(X) & = \mathbb{E} \left( \left( X - \frac{7}{2} \right)^2 \right)\\
        & = \sum_{x=1}^{6} \left( \left( X - \frac{7}{2} \right)^2 \right)\\
        & = ...
    \end{split}
\end{equation}
ma è troppo lungo da fare, quindi... (ricordiamoci anche di essere in un contesto di lancio di dadi equiprobabile)
\begin{equation}
    \begin{split}
        Var(X) & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
        & = \mathbb{E}(X^2) - \left( \frac{7}{2} \right)^2\\
        & = \sum_{x=1}^{6}x^2\frac{1}{6} - \left( \frac{7}{2} \right)^2\\
        & = \frac{1}{6}\sum_{x=1}^{6}x^2 - \left( \frac{7}{2} \right)^2\\
        & = \frac{1}{6} \cdot \frac{6 \cdot 7 \cdot 13}{6} - \left( \frac{7}{2} \right)^2\\
        & = \frac{91}{6} - \frac{49}{4} = \frac{35}{12} = \frac{36 - 1}{12}
    \end{split}
\end{equation}
\subsection{Deviazione standard}
Non è altro che
\begin{equation}
    \sigma_X = \sqrt{Var(X)}
\end{equation}

\section{Funzione di massa di probabilità congiunta}
Considero $p_{X,Y}(x,y) = P(X = x, Y = y)$ ossia $\bigcup_{j}\{X=x_i, Y=y_j\} = \{X=x_i\}$. In egual modo distribuzione marginale risulta
\begin{equation}
    \sum_{j}p_{X,Y}(x_i,y_j) = p_X(x_i)
\end{equation}
\subsection{Considerazione}
Questa tipologia di funzione viene impiegata per campioni bivariati di variabili aleatorie discrete (una funzione di densita viene impiegata nel caso di variabili aleatorie continue). Risulta necessario sottolineare che un campione bivariato non è un campione bimodale, infatti:
\begin{itemize}
	\item \textbf{Campione bimodale}
	\begin{itemize}
		\item \textbf{Scenario:} Misurare l'altezza di una popolazione composta da uomini e donne.
		\item \textbf{Descrizione:} Se la popolazione ha due sottogruppi distinti, ad esempio uomini e donne, la distribuzione delle altezze potrebbe presentare due picchi distinti, uno per ciascun gruppo.
		\item \textbf{Evento:} Le altezze delle donne potrebbero concentrarsi attorno a un certo valore, mentre quelle degli uomini attorno a un altro, generando un campione con due mode.
	\end{itemize}
	\item \textbf{Campione bivariato}
	\begin{itemize}
		\item \textbf{Scenario:} Misurare il numero di ore di studio e i punteggi ottenuti in un esame per uno studio di studenti.
		\item \textbf{Descrizione:} Considerando due variabili casuali, come il numero di ore di studio ($X$) e i punteggi in un esame ($Y$), il campione bivariato mostrerebbe la relazione tra queste due variabili.
		\item \textbf{Evento:} Ad esempio, potrebbe emergere che uno studente ha una maggiore probabilità di ottenere un punteggio elevato se ha studiato molte ore, creando una distribuzione di probabilità congiunta.
	\end{itemize}
\end{itemize}

\section{Funzione di ripartizione congiunta}
Data una coppia $X,Y$, la funzione $F_{X,Y}(x,y) = P(X \leq x, Y \leq y)$. La distribuzione marginale è invece nota come
\begin{equation}
    \begin{split}
        \lim_{y \to \infty} F_{X,Y}(x,y) & = \lim_{y \to \infty} P(X \leq x, Y \leq y)\\
        & = P(X \leq x)\\
        & = F_X(x)
    \end{split}
\end{equation}
Noto che i profili delle funzioni di ripartizione discrete sono a gradini.
\subsection{Esempio}
Se volessimo calcolare la probabilità che il dado $X$ assuma un valore minore o uguale a 3 e contemporaneamente il dado $Y$ assuma un valore minore o uguale a 4, avremmo $F_{X,Y}(3,4) = P(X \leq 3, Y \leq 4)$. Questa probabilità dipenderà dalla distribuzione congiunta dei due dadi, considerando tutte le combinazioni possibili dei risultati.

\section{Indipendenza}
$X$ e $Y$ sono indipendenti se e solo se $\forall A,B \in \mathbb{R}\ \ \ P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B)$ (il fatto che $4 < 5$ non ha nulla a che fare col fatto che 4 sia pari). Ciò equivale a dimostrare che:
\begin{enumerate}
	\item $\forall a,b \in \mathbb{R}\ \ \ F_{X,Y}(a,b) = F_X(a)F_Y(b)$
	\item $\forall a,b \in \mathbb{R}\ \ \ p_{X,Y}(a,b) = p_X(a)p_Y(b)$
\end{enumerate}
quindi:
\begin{enumerate}
	\item Dimostro che $P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B) \rightarrow p_{X,Y}(a,b) = p_X(a)p_Y(b)$:\\
	$A = \{a\}$ e $B = \{b\}$
	\item Dimostro che $p_{X,Y}(a,b) = p_X(a)p_Y(b) \rightarrow \forall A,B \in \mathbb{R}\ \ \ P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B)$:
    \begin{equation}
        \begin{split}
            P(X \in A, Y \in B) & = \sum_{a \in A}\sum_{b \in B}p_{X,Y}(a,b)\\
            & = \sum_{a \in A}\sum_{b \in B}p_X(a)p_Y(b)\\
            & = \sum_{a \in A}p_X(a)\sum_{b \in B}p_Y(b)\\
            & = P(X \in A) \cdot P(Y \in B)
        \end{split}
    \end{equation}
\end{enumerate}
Inoltre, se ho un vettore di variabili aleatorie $X_1, X_2,...,X_n$ indipendenti, allora ne consegue
\begin{equation}
    \begin{split}
        F_{X_1,X_2,...,X_n}(X_1, X_2,...,X_n) & = P(X_1 \leq x_1, X_2 \leq x_2,..., X_n \leq x_n)\\
        & = \prod_{i=1}^{n}F_{X_i}(X_i)
    \end{split}
\end{equation}
e
\begin{equation}
    \begin{split}
        p_{X_1,X_2,...,X_n}(X_1, X_2,...,X_n) & = P(X_1 = x_1, X_2 = x_2,..., X_n = x_n)\\
        & = P\left(\bigcap_{i=1}^{n}X_i \in A_i \right)\\
        & = \prod_{i=1}^{n}P(X_i \in A_i)
    \end{split}
\end{equation}
\subsection{Considerazione}
Per riassumere, la dipendenza o indipendenza tra variabili aleatorie in una distribuzione congiunta dipende dalla natura specifica della distribuzione. Due variabili aleatorie $X$ e $Y$ si dicono indipendenti nella distribuzione congiunta se la probabilità congiunta è il prodotto delle probabilità marginali.

\section{Variabile aleatoria multivariata}
Cosa succede quando calcolo il valore atteso di una funzione di variabili aleatorie?\\
Per una variabile aleatoria $X$ e funzione $g: \mathbb{R} \rightarrow \mathbb{R}$
\begin{equation}
    \mathbb{E}(X) = \sum_i x_i P(X = x_i) \rightarrow \mathbb{E}(g(x)) = \sum_i g(x_i)P(X=x_i)
\end{equation}
Per variabili aleatorie $X,Y$ e funzione $f: \mathbb{R}^2 \rightarrow \mathbb{R}$
\begin{equation}
    \mathbb{E}(f(X,Y)) = \sum_{i,j} f(x_i,y_j)P(X=x_i,Y=y_j)
\end{equation}
\subsubsection{Esempio 1}
Dato $f(x,y) = x + y$, allora
\begin{equation}
    \begin{split}
        \mathbb{E}(X + Y) & = \sum_{i=1}^n\sum_{j=1}^m(x_i + y_j)P(X=x_i,Y=y_j)\\
        & = \sum_{i=1}^n\sum_{j=1}^m x_i P(X=x_i,Y=y_j) + \sum_{i=1}^n\sum_{j=1}^m y_j P(X=x_i,Y=y_j)\\
        & = \sum_{i=1}^n x_i \sum_{j=1}^m P(X=x_i,Y=y_j) + \sum_{j=1}^m y_j \sum_{i=1}^n P(X=x_i,Y=y_j)\\
        & \rightarrow \sum_{j=1}^m P(Y=y_j) = \Omega\\
        & = \sum_{i=1}^n x_i P(X=x_i) + \sum_{j=1}^m y_j P(Y=y_j)\\
        & = \mathbb{E}(X) + \mathbb{E}(Y)
    \end{split}
\end{equation}
dunque
\begin{equation}
    \mathbb{E}(X + Y) = \mathbb{E}(X) + \mathbb{E}(Y)
\end{equation}
\subsubsection{Esempio 2}
Il valore atteso della somma dei risultati di due dadi non truccati $\mathbb{E}(X) = \sum_{2}^{12} iP(X=i)$ con $X = X_1 + X_2$ non è altro che
\begin{equation}
    \begin{split}
        \mathbb{E}(X) & = \mathbb{E}(X_1 + X_2)\\
        & = \mathbb{E}(X_1) + \mathbb{E}(X_2)\\
        & = \frac{7}{2} + \frac{7}{2}\\
        & = 7
    \end{split}
\end{equation}
\subsubsection{Esempio 3}
Ho $n$ lettere e $n$ buste accoppiate a caso, quanti abbinamenti corretti riusciamo ad avere tale che alla busta $n$ corrisponda l'esatta lettera $n$? Stabiliamo che $X_i$ vale 1 se l'i-esima lettera è soddisfa la nostra richiesta, 0 altrimenti, dunque
\begin{equation}
    \begin{split}
        \mathbb{E}\left(\sum_{i=1}^n X_i \right) & = \sum_{i=1}^n \mathbb{E}(X_i)\\
        & = \sum_{i=1}^n \frac{1}{n}\\
        & = 1
    \end{split}
\end{equation}
\subsubsection{Esempio 4}
Ci sono 20 buoni differenti e io ho comprato 10 confezioni, ognuna con un buono al suo interno: quanti buoni diversi avrò? Stabilisco che $X_i$ vale 1 se ho almeno 1 buono i-esimo, 0 altrimenti, dunque
\begin{equation}
    \begin{split}
        \mathbb{E} \left( \sum_{i=1}^{20} X_i \right) & = \sum_{i=1}^{20} \mathbb{E}(X_i)\\
        & = \sum_{i=1}^{20} P(X_i = 1)\\
        & = \sum_{i=1}^{20} P(\text{almeno un esemplare del buono i-esimo)}\\
        & \rightarrow \textit{"almeno"} \implies \text{ragiono con l'evento complementare}\\
        & = \sum_{i=1}^{20} (1 - P\text{(in nessuna delle 10 confezioni compare il buono i-esimo}))\\
        & = \sum_{i=1}^{20} \left(1 - P\left(\bigcap_{j=1}^{10} \text{nella confezione j non compare il buono i-esimo} \right)\right)\\
        & = \sum_{i=1}^{20} \left(1 - \prod_{j=1}^{10}P(\text{nella confezione j non compare il buono i-esimo})\right)\\
        & = \sum_{i=1}^{20} \left(1 - \prod_{j=1}^{10}(1 - P(\text{nella confezione j compare il buono i-esimo}))\right)\\
        & = \sum_{i=1}^{20} \left(1 - \prod_{j=1}^{10}\left(1 - \frac{1}{20}\right)\right)\\
        & = \sum_{i=1}^{20} \left(1 - \prod_{j=1}^{10}\frac{19}{20}\right)\\
        & = \sum_{i=1}^{20} \left(1 - \left(\frac{19}{20}\right)^{10}\right)\\
        & \approx \sum_{i=1}^{20} \mathbb{E}(X_i)\\
        & \approx 20 \cdot \left(1 - \left(\frac{19}{20}\right)^{10}\right) \approx 8.025
    \end{split}
\end{equation}
\subsection{Considerazione}
Ipotizzo di voler calcolare $\mathbb{E}((X-c)^2)$, non posso fare $\mathbb{E}(X^2 - 2Xc + c^2)$, tuttavia
\begin{equation}
    \begin{split}
        \mathbb{E}((X-c)^2) & = \mathbb{E}((X - \mu + \mu - c)^2)\\
        & = \mathbb{E}((a + b)^2)\\
        & = \mathbb{E}(a^2 - 2ab + b^2)\\
        & = \mathbb{E}((X - \mu)^2 - 2(X - \mu)(\mu - c) + (\mu -c)^2)\\
        & = \mathbb{E}((X - \mu)^2) - 2(\mu - c)\mathbb{E}(X - \mu) + (\mu -c)^2\\
        & \rightarrow \mathbb{E}(X - \mu) = \mathbb{E}(X) - \mu = 0\\
        & = \mathbb{E}((X - \mu)^2) + (\mu - c)^2\\
        & = Var(X) + (\mu - c)^2
    \end{split}
\end{equation}
e dunque
\begin{equation}
    \mathbb{E}((X-c)^2) = Var(X) + (\mu - c)^2 \geq Var(X)
\end{equation}
Definendo $(X-c)^2$ come \textbf{scarto quadratico}, questo implica che io decido che $c$ descriva in modo deterministico la mia quantità aleatoria, togliendo quindi l'aleatorietà. Se scelgo $c=\mu$ e quindi minimizzo lo scarto quadratico medio, allora $\mathbb{E}((X-c)^2) = Var(X)$, altrimenti $c$ mi serve per rappresentare l'incertezza in relazione al valore dello scarto quadratico medio.

\section{Covarianza}
Sappiamo che $\mathbb{E}(X + X) = \mathbb{E}(X) + \mathbb{E}(X) = 2\mathbb{E}(X)$ e quindi $\mathbb{E}(2X) = 2\mathbb{E}(X)$, mentre la varianza $Var(X + X) = Var(2X) = 2^2Var(X) = 4Var(X)$. Ma se ho due variabili aleatorie $X,Y$ definisco la covarianza tra $X$ e $Y$ come
\begin{equation}
    Cov(X,Y) = \mathbb{E}((X - \mu_X)(Y - \mu_Y))
\end{equation}
e dichiaro le seguenti proprietà
\begin{enumerate}
	\item simmetria
	\item
    \begin{equation}
        \begin{split}
            Cov(X,Y) & = \mathbb{E}(XY - \mu_XY - \mu_YX + \mu_X\mu_Y)\\
    		& = \mathbb{E}(XY) - \mu_X\mathbb{E}(Y) - \mu_Y\mathbb{E}(X) + \mu_X\mu_Y\\
    		& = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)
        \end{split}
    \end{equation}
	\item
        \begin{equation}
            \begin{split}
                Cov(aX,Y) & = \mathbb{E}(aXY) - \mathbb{E}(aX)\mathbb{E}(Y)\\
        		& = a\mathbb{E}(XY) - a\mathbb{E}(X)\mathbb{E}(Y)\\
        		& = a(\mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y))\\
        		& = aCov(X,Y)
            \end{split}
        \end{equation}
	\item
        \begin{equation}
            \begin{split}
                Cov(X + Y,Z) & = \mathbb{E}((X+Y)Z) - \mathbb{E}(X+Y)\mathbb{E}(Z)\\
        		& = \mathbb{E}(XZ + YZ) - (\mathbb{E}(X) + \mathbb{E}(Y))\mathbb{E}(Z)\\
        		& = \mathbb{E}(XZ) + \mathbb{E}(YZ) - \mathbb{E}(X)\mathbb{E}(Z) - \mathbb{E}(Y)\mathbb{E}(Z)\\
        		& = Cov(X,Z) + Cov(Y,Z)
            \end{split}
        \end{equation}
	\item
        \begin{equation}
            \begin{split}
                Cov\left(\sum_i X_i, \sum_j Y_j\right) & = \sum_i Cov\left(X_i, \sum_j Y_j\right)\\
        		& = \sum_i Cov\left(\sum_j Y_j, X_i\right)\\
        		& = \sum_i \sum_j Cov(Y_j, X_i)\\
        		& = \sum_i \sum_j Cov(X_i, Y_j)\\
            \end{split}
        \end{equation}
	\item
         \begin{equation}
            \begin{split}
        		Cov(X,X) & = \mathbb{E}((X - \mu_X)(X - \mu_X))\\
                & = \mathbb{E}((X - \mu_X)^2)\\
                & = Var(X)
            \end{split}
        \end{equation}
        o anche
        \begin{equation}
            \begin{split}
        		Cov(X,X) & = \mathbb{E}(X \cdot X) - \mathbb{E}(X)\mathbb{E}(X)\\
                & = \mathbb{E}(2X) - \mathbb{E}(X)^2\\
                & = Var(X)
            \end{split}
        \end{equation}
\end{enumerate}
\subsection{Considerazione 1}
\begin{equation}
	\begin{split}
	Var(X + Y) & = \mathbb{E}((X + Y)^2) - \mathbb{E}(X + Y)^2\\
	& = \mathbb{E}(X^2 + 2XY + Y^2) - (\mathbb{E}(X) + \mathbb{E}(Y))^2\\
	& = \mathbb{E}(X^2) + \mathbb{E}(2XY) + \mathbb{E}(Y^2) - \mathbb{E}(X)^2 - 2\mathbb{E}(X)\mathbb{E}(Y) - \mathbb{E}(Y)^2\\
	& = \mathbb{E}(X^2) - \mathbb{E}(X)^2 + \mathbb{E}(Y^2) - \mathbb{E}(Y)^2 + \mathbb{E}(2XY) - 2\mathbb{E}(X)\mathbb{E}(Y)\\
	& = Var(X) + Var(Y) + 2Cov(X,Y)
	\end{split}
\end{equation}
e dunque
\begin{equation}
    \begin{split}
        Var(X + X) & = Var(X) + Var(X) + 2Cov(X,X)\\
        & = Var(X) + Var(X) + 2Var(X)\\
        & = 4Var(X)
    \end{split}
\end{equation}
e infine
\begin{equation}
    Var \left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i) + 2 \sum_{i \neq j}^n Cov(X_i,X_j)
\end{equation}
\subsection{Considerazione 2}
In caso di differenza, siccome
\begin{equation}
	Var(-Y) = Var(Y)
\end{equation}
allora
\begin{equation}
	Var(X - Y) = Var(X) + Var(Y) - 2Cov(X,Y)
\end{equation}
\subsection{Considerazione 3}
Se $X,Y$ indipendenti, allora $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$, infatti
\begin{equation}
    \begin{split}
        \mathbb{E}(XY) & = \sum_i \sum_j x_i y_j P(X=x_i, Y=y_j)\\
        & = \sum_i \sum_j x_i y_j P(X=x_i) P(Y=y_j)\\
        & = \sum_i x_i P(X=x_i) \sum_j  y_j P(Y=y_j)\\
        & = \mathbb{E}(X)\mathbb{E}(Y)
    \end{split}
\end{equation}
e sempre con variabili indipendenti abbiamo che
\begin{equation}
    \begin{split}
        Cov(X,Y) & = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)\\
        & = \mathbb{E}(X)\mathbb{E}(Y) - \mathbb{E}(X)\mathbb{E}(Y)\\
        & = 0
    \end{split}
\end{equation}
e che
\begin{equation}
    Var \left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i)
\end{equation}
\subsubsection{Esempio 1}
La varianza della somma di 10 lanci indipendenti di un dato bilanciato?
\begin{equation}
    \begin{split}
        Var\left(\sum_{i=1}^{10} X_i\right) & = \sum_{i=1}^{10} Var(X_i)\\
        & = \sum_{i=1}^{10} \frac{35}{12}\\
        & = 10 \cdot \frac{35}{12}\\
        & = \frac{175}{6}
    \end{split}
\end{equation}
\subsubsection{Esempio 2}
La varianza del numero di teste che si verificano lanciando 10 volte una moneta bilanciata?
\begin{equation}
    \begin{split}
        Var(I_A) & = P(A) \cdot P(\overline{A})\\
        & = \frac{1}{4}
    \end{split}
\end{equation}
e quindi
\begin{equation}
    \begin{split}
        Var\left(\sum_{i=1}^{10} I_A\right) & = \sum_{i=1}^{10} Var(I_A)\\
        & = 10 \cdot \frac{1}{4}\\
        & = \frac{5}{2}
    \end{split}
\end{equation}

\section{Coefficiente di correlazione}
Ipotizzo $X = I_A$, $Y = I_B$ e $A,B \in \mathcal{A}$. Allora $\mathbb{E}(X) = P(X = 1)$ e $\mathbb{E}(Y) = P(Y = 1)$ per definizione. $XY$ vale 1 se $A$ e $B$ si verificano, 0 altrimenti, quindi $\mathbb{E}(XY) = P(XY = 1)$, e dunque
\begin{equation}
    \begin{split}
        Cov(X,Y) & = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)\\
        & = P(XY = 1) - P(X = 1)P(Y = 1)\\
        & = P(X=1, Y=1) - P(X = 1)P(Y = 1)
    \end{split}
\end{equation}
ora pongo
\begin{equation}
    \begin{split}
        Cov(X,Y) > 0 & \rightarrow P(X=1, Y=1) > P(X=1)P(Y=1)\\
        & = \frac{P(X=1, Y=1)}{P(Y=1)} > P(X=1)\\
        \\
        & \implies P(X=1 \mid Y=1) > P(X=1)
    \end{split}
\end{equation}
ossia la probabilità che $X$ valga 1 conoscendo $Y$, è maggiore della probabilità che $X$ valga 1. Infine mi annoto che $Cov(2X, 2Y) = 4Cov(X,Y)$.
\subsection{Formula}
\begin{equation}
    \rho_{X,Y} = \frac{Cov(X,Y)}{\sigma_X \cdot \sigma_Y}
\end{equation}
Quindi
\begin{equation}
    \begin{split}
        \rho_{2X,2Y} & = \frac{Cov(2X,2Y)}{\sigma_X \cdot \sigma_Y}\\
        & = 4\frac{Cov(X,Y)}{4 \cdot \sigma_X \cdot \sigma_Y}
    \end{split}
\end{equation}
e da cui si ottiene
\begin{equation}
    Var(2X) = 4Var(X)
\end{equation}
e
\begin{equation}
    \begin{split}
        \sigma_{2X} & = \sqrt{Var(2X)}\\
        & = 2 \sqrt{Var(X)}\\
        & = 2 \sigma_X
    \end{split}
\end{equation}
Tale coefficiente è invariante rispetto alla scalatura, infatti $\rho_{x,y}=\rho_{2x,2y}$.

\section{Funzione di densità di probabilità}
Data $f_X:\mathbb{R} \rightarrow \mathbb{R}$, la funzione di densità di probabilità è
\begin{equation}
    \forall B \subseteq \mathbb{R},\ P(X \in B) = \int_B f_X(x)\ dx
\end{equation}
Se volessi calcolare $P(a \leq X \leq b)$?
\begin{equation}
    \int_a^b f_X(x)\ dx
\end{equation}
I valori $a$ e $b$ sono finiti, come anche $f_X(a)$ e $f_X(b)$, dunque
\begin{equation}
    P(a \leq X \leq b) = P(a < X < b)
\end{equation}
Inoltre
\begin{equation}
    \begin{split}
        P(X = a) & = P(a \leq X \leq a)\\
        & = \int_a^a f_X(x)\ dx\\
        & = 0
    \end{split}
\end{equation}
e
\begin{equation}
    P \left( a - \frac{\varepsilon}{2} \leq X \leq a + \frac{\varepsilon}{2} \right) = \int_{a - \frac{\varepsilon}{2}}^{a + \frac{\varepsilon}{2}} f_X(x)\ dx \approx \varepsilon f(a)
\end{equation}
Infine posso anche dire che
\begin{equation}
    \begin{split}
        \int_{-\infty}^{\infty}f_X(x) dx & = \int_{\mathbb{R}} f_X(x)\ dx\\
        & = P(X \in \mathbb{R})\\
        & = 1
    \end{split}
\end{equation}
la funzione di ripartizione viene vista come
\begin{equation}
    \int_{- \infty}^{a}f_X(x)\ dx = P(X \leq a) = F_X(a)
\end{equation}
quindi ci sta dicendo che la funzione di ripartizione la ottengo integrando la funzione di densità di probabilità: per il teorema fondamentale del calcolo integrale sappiamo che
\begin{equation}
    \frac{d}{dx}F_X(x) = f(x)
\end{equation}
Se una variabile aleatoria è continua, il suo valore atteso sarà
\begin{equation}
    \mathbb{E}(X) = \int_{-\infty}^{\infty} x f_X(x)\ dx
\end{equation}
Se la variabile aleatoria continua $X$ assume solo specificazioni non negative, allora
\begin{equation}
    \mathbb{E}(X) = \int_0^{\infty}1  - F_x(x)\ dx
\end{equation}
Riassumendo, una funzione è funzione di densità quando entrambe le seguenti proprietà sono vere:
\begin{itemize}
	\item $\int_{-\infty}^{\infty}f_X(x) dx = 1$
	\item $f_X(x)$ è sempre non negativa
\end{itemize}
\subsubsection{Esempio}
Considero una v.a. $X$ continua
\begin{equation}
	f_X(x) =
    \begin{cases}
		c \cdot (4x - 2x^2)   		& \text{se } 0 < x < 2\\
		0							& \text{altrimenti}
	\end{cases}
\end{equation}
dunque $f_X(x) = c \cdot (4x - 2x^2) I_{(0,2)}(x)$.
\begin{itemize}
	\item \textbf{Quanto vale $c$?}
    \begin{equation}
        \begin{split}
            & \int_{-\infty}^{\infty}f_X(x)\ dx = 1\\
        	& \int_{0}^{2} c \cdot (4x - 2x^2)\ dx = 1\\
        	& c \cdot \int_{0}^{2}(4x - 2x^2)\ dx = 1\\
        	& c \cdot \left(4 \cdot \frac{x^2}{2} - 2 \cdot \frac{x^3}{3}\right)_0^2 = 1\\
        	& c \cdot 8 - 8 \cdot \frac{2}{3} = 1\\
        	& c = \frac{3}{8}
        \end{split}
    \end{equation}
	\item \textbf{Quanto vale $P(X > 1)$?}
    \begin{equation}
        \begin{split}
            P(X > 1) & = \int_{1}^{\infty}f_X(x)\ dx\\
            & = \int_{1}^{2} \frac{3}{8} \cdot (4x - 2x^2)\ dx\\
            & = \frac{3}{8} \cdot \left(4 \cdot \frac{x^2}{2} - 2 \cdot \frac{x^3}{3}\right)_1^2\\
            & = \frac{1}{2}
        \end{split}
    \end{equation}
\end{itemize}

\section{Sommario delle proprietà essenziali}
\begin{itemize}
	\item $\forall X,Y$ indipendenti $\iff f_{x,y}(x,y) = f_x(x)\cdot f_y(y)$ (regola di fattorizzazione)
    \item $\forall X,Y$ indipendenti $\to Cov(X,Y)=0$
    \item $\forall X,Y$ indipendenti $\to \mathbb{E} \left( \prod_1^n X_i \right)=\prod_1^n\mathbb{E}(X_i)$
    \item $\mathbb{E}(X_1 + \dots + X_n) = \sum_{i=1}^{n} \mathbb{E}(X_i)$
    \item $Var(X+Y) = Var(Y)+Var(Y)+2Cov(X,Y)$
    \item $Var\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i) + \sum_{i \neq J}Cov(X_i,X_j)$
    \item $Var\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i)$, se $X,Y$ indipendenti
\end{itemize}

\section{Disuguaglianza di Markov}
Il teorema vuole che data una variabile aleatoria $X \geq 0$, $\forall a > 0$ allora
\begin{equation}
    P(X \geq a) \leq \frac{\mathbb{E}(X)}{a}
\end{equation}
Lo si dimostra facilmente
\begin{equation}
    \begin{split}
        \mathbb{E}(X) & = \int_{0}^{\infty} x f_X(x)\ dx\\
        & = \int_{0}^{a} x f_X(x)\ dx + \int_{a}^{\infty} x f_X(x)\ dx \geq \int_{a}^{\infty} x f_X(x)\ dx
    \end{split}
\end{equation}
e dunque, siccome $x$ non è sotto $f_X(x)$
\begin{equation}
    \begin{split}
        \int_{a}^{\infty} x f_X(x)\ dx & \geq \int_{a}^{\infty} a \cdot f_X(x)\ dx\\
        & = a \cdot \int_{a}^{\infty} f_X(x)\ dx\\
        & = a \cdot P(X \geq a)
    \end{split}
\end{equation}
per concludere
\begin{equation}
    \mathbb{E}(X) \geq a \cdot P(X \geq a) = \frac{\mathbb{E}(X)}{a} \geq P(X \geq a)
\end{equation}
\subsubsection{Esempio}
Ipotizziamo di avere una variabile aleatoria $X$ che rappresenta il numero di pezzi prodotti in una settimana. Inoltre sappiamo che $\mathbb{E}(X) = 50$: trova $P(X \geq 75)$.\\
Semplicemente
\begin{equation}
    P(X \geq 75) \leq \frac{\mathbb{E}(X)}{75} = \frac{50}{75} = \frac{2}{3}
\end{equation}
quindi $P(X \geq 75) \leq \frac{2}{3}$, tuttavia in questo caso mi serve a poco, dunque dobbiamo dedurre un'altra disuguaglianza.

\section{Disuguaglianza di Chebyshev}
Siano date una variabile aleatoria, il suo valore atteso e la sua varianza, allora $\forall > 0$
\begin{equation}
    P(\mid X - \mu \mid \geq r) \leq \frac{\sigma^2}{r^2}
\end{equation}
Lo si dimostra tramite i seguenti passaggi
\begin{equation}
    \mid X - \mu \mid \geq r \iff (X - \mu)^2 \geq r^2
\end{equation}
quindi
\begin{equation}
    \begin{split}
        P(\mid X - \mu \mid \geq r) & = P((X - \mu)^2 \geq r^2)\\
        & = P(Y \geq r^2)
    \end{split}
\end{equation}
e per Markov
\begin{equation}
    \begin{split}
        & P(Y \geq r^2) \leq \frac{\mathbb{E}(Y)}{r^2}\\
        & P((X - \mu)^2 \geq r^2) \leq \frac{\mathbb{E}((X - \mu)^2)}{r^2}\\
        & P(\mid X - \mu \mid \geq r) \leq \frac{Var(X)}{r^2}\\
        & P(\mid X - \mu \mid \geq r) \leq \frac{\sigma^2}{r^2}
    \end{split}
\end{equation}
\subsubsection{Esempio 1}
Riprendendo l'esempio usato per Markov in cui si ipotizza di avere una variabile aleatoria $X$ che rappresenta il numero di pezzi prodotti in una settimana, con un $\mathbb{E}(X) = 50$, veniamo ora a conoscenza della sua varianza ossia $Var(X) = 25$. A questo punto possiamo calcolare per esempio
\begin{equation}
    \begin{split}
        P(40 < X < 60) & = P(40 - \mu < X < 60 - \mu)\\
        & = P(-10 < X < 10)\\
        & = P(\mid X - \mu \mid < 10)
    \end{split}
\end{equation}
Non posso applicare Chebyshev perché non ho $\geq$, quindi
\begin{equation}
    P(\mid X - \mu \mid < 10) = 1 - P(\mid X - \mu \mid \geq 10)
\end{equation}
Risolvendo solo $P(\mid X - \mu \mid \geq 10)$ si ottiene
\begin{equation}
    P(\mid X - \mu \mid \geq 10) \leq \frac{25}{100} = \frac{1}{4}
\end{equation}
E dunque ritornando a $1 - P(\mid X - \mu \mid \geq 10)$ si ha
\begin{equation}
    1 - P(\mid X - \mu \mid \geq 10) \geq 1 - \frac{1}{4} = \frac{3}{4}
\end{equation}
Quindi
\begin{equation}
    \begin{split}
        & 1 - P(\mid X - \mu \mid \geq 10) \geq \frac{3}{4}\\
        & P(\mid X - \mu \mid < 10) \geq \frac{3}{4}
    \end{split}
\end{equation}
\subsubsection{Esempio 2}
{\footnotesize (Preso da Wikipedia)}\\
Suppongo che la media di parole in ogni articolo sia 1000 con una $\sigma$ di 200, considerando $2\sigma$ possiamo dire che la probabilità che il numero di parole in un articolo valga $600 < X < 1400$ è $\geq \frac{1}{k^2}$, ossia almeno $\frac{1}{4}$, quindi almeno il 75\% dato che consideriamo
\begin{equation}
	\begin{split}
		& P(-r < X < r) = 1 - P(\mid X - \mu \mid \geq r)\\
		& \rightarrow 1 - P(\mid X - \mu \mid \geq r) \geq 1 - \frac{\sigma^2}{r^2}\\
		& \rightarrow P(\mid X - \mu \mid < r) \geq 1 - \frac{\sigma^2}{r^2}
	\end{split}
\end{equation}
In altre parole, la probabilità di stare entro 2$\sigma$ è almeno il 75\%.
\subsubsection{Considerazione}
Se $P(\mid X - \mu \mid \geq k\sigma)$ allora
\begin{equation}
    \begin{split}
        & P(\mid X - \mu \mid \geq k\sigma) \leq \frac{\sigma^2}{\sigma^2 k^2}\\
        & P(\mid X - \mu \mid \geq k\sigma) \leq \frac{1}{k^2}
    \end{split}
\end{equation}
Dunque la deviazione standard è un indice di dispersione, ma da qui capiamo proprio che essa è una sorta di unità di misura tanto che \textit{"qual è la probabilità che un valore si collochi entro $k$ deviazioni standard?"}.

\section{Esercizi}
\begin{itemize}
	\item \textbf{Si lanciano 3 monete, calcolare la probabilità che escano 3 teste}\\
	$\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{8} = 0.125$\\
	In alternativa potevo impostarlo come una distribuzione binomiale $X \sim B(3, 0.5)$ e risolvere $P(X=3) = 0.125$.
	\item \textbf{Si lanciano 4 monete, calcolare la probabilità che escano 2 teste e 2 croci}\\
	Considero i seguenti eventi favorevoli:
	$$\{TTTT, TTCC, CCTT, CTCT, TCCT, TCTC\}$$
	Sono 6, mentre gli eventi possibili sono $2^4 = 16$, dunque:
	$$\frac{6}{16} = \frac{3}{8} = 0.375$$
	In alternativa potevo impostarlo come una distribuzione binomiale $X \sim B(4, 0.5)$ e risolvere $P(X=2) = 0.375$.
	\item \textbf{Si lanciano due dadi, calcolare la probabilità che la somma dei risultati sia 4}\\
	I casi favorevoli sono 3 ($\{(1,3), (2,2), (3,1)\}$) mentre i casi possibili sono 36, dunque:
	$$\frac{3}{36} = \frac{1}{12}$$
	\item \textbf{L'urna 1 contiene 4 biglie rosse e 3 biglie blu, e l'urna 2 contiene 2 biglie rosse e 2 blu. Una biglia viene scelta a caso dall'urna 1 e inserita nell'urna 2. Poi viene estratta una biglia dall'urna 2. Qual è la probabilità che la biglia estratta dall'urna 2 sia rossa? Qual è la probabilità che la biglia estratta dall'urna 1 sia rossa se la biglia estratta dall'urna 2 è blu?}\\
	Si noti che dopo la prima estrazione la composizione dell'urna 1 cambia: se si estrae dall'urna 1 una biglia rossa, l'urna 2 conterrà 5 palline, 3 rosse e 2 blu. Se invece la biglia estratta dall'urna 1 è blu, allora l'urna 2 conterrà 5 palline, 2 rosse e 3 blu. Possiamo quindi calcolare la probabilità richiesta come segue:
	$$P(R_2 \cap (R_1 \cup B_1)) = P(R_2 \cap R_1) + P(R_2 \cap B_1)$$
	Noi non conosciamo l'intersezione, tuttavia:
	$$P(R_2 \cap R_1) + P(R_2 \cap B_1) = P(R_2|R_1)P(R_1) + P(R_2|B_1)P(B_1) = \frac{3}{5}\frac{4}{7} + \frac{2}{5}\frac{3}{7} = 0.5143$$
	Qual è quindi la probabilità che la biglia estratta dall'urna 1 sia rossa se la biglia estratta dall'urna 2 è blu?
	$$P(R_1|B_2) = \frac{P(B_2|R_1)P(R_1)}{P(B_2|R_1)P(R_1) + P(B_2|B_1)P(B_1)} = 0.4706$$
	\item \textbf{Calcola la probabilità che estraendo una carta da un mazzo di 40 questa non sia un asso nero}\\
	La probabilità che sia un asso nero è $\frac{2}{40}$, quindi la probabilità che non sia un asso nero è:
	$$1 - \frac{2}{40} = \frac{38}{40} = \frac{19}{20}$$
	\item \textbf{Calcola la probabilità che estraendo una carta da un mazzo di 40 carte, questa sia un asso sapendo che non è di fiori}\\
	$P(asso|\overline{fiori}) = \frac{P(A \cap \overline{fiori})}{P(\overline{fiori})} = \frac{\frac{3}{40}}{\frac{30}{40}} = \frac{1}{10}$
	\item \textbf{Si estraggono consecutivamente due carte da un mazzo di 40, calcolare la probabilità che la seconda carta sia un asso dato che la prima è un 2}\\
	$P(asso | 2) = \frac{\frac{4}{39} \cdot \frac{4}{40}}{\frac{4}{40}} = \frac{4}{39}$
	\item \textbf{Si estraggono 3 carte di fila da un mazzo di 40, calcolare la probabilità che siano tutte carte di fiori}\\
	$\frac{10}{40} \cdot \frac{9}{39} \cdot \frac{8}{38} = 0.0121$
	\item \textbf{Un'urna contiene 30 palline di cui 12 rosse, 10 bianche e 8 nere. Se ne estraggono 2, calcola la probabilità che siano dello stesso colore nei due seguenti casi:\\
	A) La prima pallina viene reinserita nell'urna prima di procedere alla secondo estrazione\\
	B) La prima pallina non viene reinserita nell'urna prima di procedere alla seconda estrazione}\\
	Partendo dal punto B, la probabilità che una pallina sia bianca è $\frac{10}{30}$, che sia rossa è $\frac{12}{30}$ e che sia nera è $\frac{8}{30}$. La probabilità di inserire due bianche senza reinserimento qual è?
	$$\frac{10}{30} \cdot \frac{9}{29}$$
	Per le rosse?
	$$\frac{12}{30} \cdot \frac{11}{29}$$
	E per le nere?
	$$\frac{8}{30} \cdot \frac{7}{29}$$
	Ne possiamo pescare una qualsiasi, non ci dice quale risultato scegliere, quindi è o bianca o nera o rossa. Il termine "o" si traduce con l'unione, dunque bisogna effettuare una somma, quindi:
	$$\left( \frac{10}{30} \cdot \frac{9}{29} \right) + \left( \frac{12}{30} \cdot \frac{11}{29} \right) + \left( \frac{8}{30} \cdot \frac{7}{29} \right) \approx 0.32$$
	Analogamente per rispondere al punto A si applica la stessa procedura ma con reinserimento, dunque:
	$$\left( \frac{10}{30} \cdot \frac{10}{30} \right) + \left( \frac{12}{30} \cdot \frac{12}{30} \right) + \left( \frac{8}{30} \cdot \frac{8}{30} \right) \approx 0.34$$
	Generalizzando, possiamo dire che la formula generale risulta essere:
	$$P( (B_1 \cap B_2) \cup (R_1 \cap R_2) \cup (N_1 \cap N_2) )$$
	Dove la probabilità singola muta a seconda della presenza o meno di reinserimento.
\end{itemize}

\chapter{Modelli Discreti}

\section{Modello di Bernoulli}
Descrive un esperimento con due esiti (successo=1 e fallimento=0)
\begin{equation}
    X\sim B(p)
\end{equation}
\begin{itemize}
	\item \textbf{Supporto:} $D_X=\{0,1\}$
    \item \textbf{Funzione di massa di probabilità:}
    \begin{equation}
		p_X(x) = P(X = x) =
      	\begin{cases}
			1-p   		& x=0\\
        	p 			& x=1\\
        	0			& \text{altrimenti}
      	\end{cases}
      	\ \ \ =\ p^x(1-p)^{1-x} I_{\{0,1\}}(x)
    \end{equation}
    \item \textbf{Funzione di ripartizione:}
    \begin{equation}
    	F_X(x) =
        \begin{cases}
        	0   		& x<0\\
          	1-p 		& 0\leq x<1\\
          	1   		& x\geq1
        \end{cases}
        \ \ \ =\ (1-p) I_{[0,1)}(x) + I_{[1,\infty)}(x)
	\end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
        \begin{split}
            \mathbb{E}(X) & = \sum_i x_i P(X=x_i)\\
            & = 0 \cdot P(X=0) + 1 \cdot P(X=1)\\
            & = p
        \end{split}
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
    	\begin{split}
   			Var(X) & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
   			& = \mathbb{E}(X) - \mathbb{E}(X)^2\\
   			& = p - p^2\\
   			& = p \cdot (1 - p)
		\end{split}
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{bern-ev}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{bern-m}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{bern-r}
\end{center}

\section{Modello Binomiale}
Se eseguo $n$ ripetizioni indipendenti di un esperimento bernoulliano di parametro $p$, il conteggio dei successi mi porta ad un modello chiamato \textit{binomiale}.
\begin{equation}
    X\sim B(n,p)\ \ \ n \in \mathbb{N}
\end{equation}
\begin{itemize}
    \item \textbf{Supporto:} $D_X=\{0,1,\dots,n\}$
    \item \textbf{Funzione di massa di probabilità:}
    \begin{equation}
        p_X(i) = \binom{n}{i}p^i(1-p)^{n-i} \mathbb{I}_{\{0,\dots,n\}}(i)
    \end{equation}
    Dove $i$ è il numero di successi e $n-i$ il numero di insuccessi.
    \item \textbf{Funzione di ripartizione:}
    \begin{equation}
        \begin{split}
            F_X(x) & = P(X \leq x)\\
            & = \sum_{i=0}^{\lfloor x\rfloor}\binom{n}{i}p^i(1-p)^{n-i} I_{[0,n]}(x) + I_{(n,\infty)}(x)
        \end{split}
    \end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
    	\begin{split}
   			\mathbb{E}(X) & = \mathbb{E}\left(\sum_{i=1}^n X_i \right)\\
   			& = \sum_{i=1}^n \mathbb{E}(X_i)\\
   			& = \sum_{i=1}^n p\\
   			& = np
		\end{split}
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
    	\begin{split}
   			Var(X) & = Var\left(\sum_{i=1}^n X_i \right)\\
            & \rightarrow \text{indipendenza}\\
   			& = \sum_{i=1}^n Var(X_i)\\
   			& = \sum_{i=1}^n p(1-p)\\
   			& = np \cdot (1-p)
		\end{split}
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{binom-m}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{binom-r}
\end{center}

\subsection{Esempio}
La probabilità di avere una penna difettosa è $P(\text{penna difettosa}) = 0.01$ e una confezione contiene 10 penne. Il rimborso avviene se c'è più di una penna difettosa nella confezione: quale percentuale delle scatole verrà restituita?
\begin{equation}
    P(\text{rimborso}) = P(\text{più di una penna difettosa su 10})
\end{equation}
Identifichiamo con $X$ il numero di penne difettose per confezione, dunque $X \sim B(10, 0.01)$ e quindi
\begin{equation}
    P(\text{rimborso}) = P(X > 1) = \sum_{i=2}^{10} \binom{10}{i} 0.01^i (0.99)^{10-i}
\end{equation}
Ma mi è scomodo lavorare così, quindi posso considerare che:
\begin{equation}
    \begin{split}
        P(X > 1) & = 1 - P(X \leq 1)\\
        & = 1 - \binom{10}{0} 0.01^0 (0.99)^{10} - \binom{10}{1} 0.01^1 (0.99)^9\\
        & = 1 - 0.99^{10} - 10 \cdot 0.01 \cdot 0.99^9\\
        & \approx 0.0043
    \end{split}
\end{equation}
Il che significa che lo 0.43\% delle scatole viene rimborsato.\\
Ma se compro 3 scatole, qual è la probabilità di restituire esattamente 1 scatola?\\
Definisco $Y$ come il numero di scatole che restituisco su 3 acquistate e dunque $Y \sim B(3, 0.0043)$, quindi
\begin{equation}
    P(Y = 1) = \binom{3}{1} 0.0043^1 (0.0043)^{2} \approx 0.01278
\end{equation}
Quindi 1.278\%. Se invece mi viene chiesta la probabilità di restituirne almeno 1 su 3 allora semplicemente
\begin{equation}
    P(Y \geq 1) = 1 - P(Y = 0) = 1 - (1 - 0.0043)^3 \approx 0.01284
\end{equation}
Ossia 1.284\%.\\
Da qui si evince che in caso di due variabili aleatorie $X_1 \sim B(n_1,p)$ e $X_2 \sim B(n_2,p)$, si ha
\begin{equation}
    Y = X_1 + X_2 = \sum_{i=1}^n X_{1,i} + \sum_{j=1}^m X_{2,j} = \sum_{i=1}^{n+m} Y_i \rightarrow Y \sim B(n_1 + n_2, p)
\end{equation}

\section{Modello Uniforme Discreto}
Ipotizzo $n \in \mathbb{N}$ esiti (da 1 a n) equiprobabili, dunque
\begin{equation}
    X \sim U(n)\ \ \ n \in \mathbb{N}
\end{equation}
\begin{itemize}
    \item \textbf{Supporto:} $D_X=\{1,\dots,n\}$
    \item \textbf{Funzione di massa di probabilità:}
    \begin{equation}
        p_X(i) = P(X = i) = \frac{1}{n} \cdot \mathbb{I}_{D_X}(i)
    \end{equation}
    \item \textbf{Funzione di ripartizione:}
    \begin{equation}
        \begin{split}
            F_X(x) & = P(X \leq x)\\
            & = \sum_{i=1}^{\lfloor x\rfloor} P(X = i)\\
            & = \sum_{i=1}^{\lfloor x\rfloor} \frac{1}{n}\\
            & = \frac{\lfloor x\rfloor}{n} \cdot \mathbb{I}_{[1,n]}(x)+\mathbb{I}_{[n,\infty)}(x)
        \end{split}
    \end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
    	\begin{split}
   			\mathbb{E}(X) & = \sum_{i=1}^n i \cdot P(X=i)\\
   			& = \frac{1}{n} \sum_{i=1}^n i\\
   			& = \frac{1}{n} \cdot \frac{n(n+1)}{2}\\
   			& = \frac{n+1}{2}
		\end{split}
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
    	\begin{split}
   			Var(X) & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
            & = \mathbb{E}(X^2) - \left(\frac{n+1}{2}\right)^2\\
            & = \sum_{i=1}^n i^2 \cdot P(X=i) - \left(\frac{n+1}{2}\right)^2\\
            & = \frac{1}{n} \sum_{i=1}^n i^2 - \left(\frac{n+1}{2}\right)^2\\
            & = \frac{1}{n} \cdot \frac{n(n+1)(2n+1)}{6} - \left(\frac{n+1}{2}\right)^2\\
            & = \frac{(n+1)(2n+1)}{6} - \left(\frac{n+1}{2}\right)^2\\
            & = (n+1) \cdot \left(\frac{2n+1}{6 - \frac{n+1}{4}}\right)\\
            & = (n+1) \cdot \frac{4n+2-3n-3}{12}\\
            & = (n+1) \cdot \frac{n-1}{12}\\
   			& = \frac{n^2-1}{12}
		\end{split}
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{unifd-ev}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{unifd-m}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{unifd-r}
\end{center}

\section{Modello Geometrico}
Basato sul modello di Bernoulli, ripeto l'esperimento finché non ottengo successo: posso contare il numero totale di esperimenti, oppure quello di insuccessi. Nel nostro caso la nostra $X$ sarà il numero di insuccessi prima del primo successo in una sequenza di esperimenti bernoulliani con lo stesso parametro e tra loro indipendenti.
\begin{equation}
    X\sim G(p)
\end{equation}
Dire $X=i$ significa dire che ho effettuato $i+1$ esperimenti bernoulliani, di cui i primi $i$ hanno avuto insuccesso, e l'$i+1$-esimo è stato un successo.
\begin{itemize}
    \item \textbf{Supporto:} $D_X=\mathbb{N} \cup \{0\}$
    \item \textbf{Funzione di massa di probabilità:}
    \begin{equation}
        \begin{split}
            f_X(i) & = P(X=i)\\
            & = p(1-p)^i \mathbb{I}_{\mathbb{N}\cup\{0\}}(i)
        \end{split}
    \end{equation}
    \item \textbf{Funzione di ripartizione:} Inizio osservando che
    \begin{equation}
    	\begin{split}
   			P(X > n) & = \sum_{i=n+1}^\infty P(X=i)\\
            & = \sum_{i=n+1}^\infty p(1-p)^i\\
            & \rightarrow \text{moltiplico e divido per lo stesso valore}\\
            & = \sum_{i=n+1}^\infty p(1-p)^i \cdot \frac{(1-p)^{n+1}}{(1-p)^{n+1}}\\
            & = p(1-p)^{n+1} \cdot \sum_{i=n+1}^\infty (1-p)^{i - (n+1)}\\
            & \rightarrow j = i - (n+1) \text{ dunque se } i=n+1 \implies j=0\\
            & = p(1-p)^{n+1} \cdot \sum_{j=0}^\infty (1-p)^j\\
            & \rightarrow \text{ per la serie geometrica } \sum_{n=0}^\infty x^n = \frac{1}{1-x}\\
            & = p(1-p)^{n+1} \cdot \frac{1}{1-(1-p)}\\
            & = p(1-p)^{n+1} \cdot \frac{1}{p}\\
            & = (1-p)^{n+1}
		\end{split}
    \end{equation}
    Quindi possiamo dedurre che:
    \begin{equation}
        \begin{split}
            F_X(x) & = P(X \leq x)\\
            & = 1 - P(X > x)\\
            & = 1 - (1-p)^{\lfloor x\rfloor + 1} \mathbb{I}_{\mathbb{N}\cup\{0\}}(x)
        \end{split}
    \end{equation}
    \item \textbf{Valore atteso:} Prima calcolo $\sum_{i=0}^\infty i \alpha^i$:
    \begin{equation}
    	\begin{split}
            \sum_{i=0}^\infty i \alpha^i & = \alpha \sum_{i=0}^\infty i \alpha^{i-1}\\
            & \rightarrow \text{ la derivata della somma è uguale alla somma delle derivate }\\
            & = \alpha \sum_{i=0}^\infty \frac{d}{dx} \alpha^{i}\\
            & = \alpha \frac{d}{dx} \sum_{i=0}^\infty \alpha^{i}\\
            & \rightarrow \text{ per la serie geometrica } \sum_{n=0}^\infty x^n = \frac{1}{1-x}\\
            & = \alpha \frac{d}{dx} \frac{1}{1 - \alpha}\\
            & = \alpha \cdot \frac{1}{(1-\alpha)^2}\\
            & = \frac{\alpha}{(1-\alpha)^2}
		\end{split}
    \end{equation}
    Dunque:
    \begin{equation}
    	\begin{split}
            \mathbb{E}(X) & = \sum_{i=0}^\infty i \cdot P(X=i)\\
            & = \sum_{i=0}^\infty i \cdot p \cdot (1-p)^i\\
            & = p \cdot \sum_{i=0}^\infty i \cdot (1-p)^i\\
            & = p \cdot \frac{1-p}{p^2}\\
            & = \frac{1-p}{p}
		\end{split}
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
    	\begin{split}
   			Var(X) & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
            & = \mathbb{E}(X^2) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( \sum_{i=0}^\infty i^2 \cdot p \cdot (1-p)^i \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( p(1-p) \cdot \sum_{i=0}^\infty i^2 \cdot (1-p)^{i-1} \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( p(1-p) \cdot \sum_{i=0}^\infty i \cdot i \cdot (1-p)^{i-1} \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( p(1-p) \cdot \sum_{i=0}^\infty i \cdot \left( -\frac{d}{dp}(1-p)^i \right) \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( -p(1-p) \cdot \sum_{i=0}^\infty i \cdot \frac{d}{dp}(1-p)^i \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( -p(1-p) \cdot \frac{d}{dp} \sum_{i=0}^\infty i \cdot (1-p)^i \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( -p(1-p) \cdot \frac{d}{dp} \frac{1-p}{p^2} \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( -p(1-p) \cdot \frac{-p^2 - 2p(1-p)}{p^4} \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( -p(1-p) \cdot \frac{p+2(1-p))}{p^2} \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \left( \frac{1-p}{p^2} \cdot (p+2-2p) \right) - \left(\frac{1-p}{p}\right)^2\\
            & = \frac{(1-p)(2-p)}{p^2} - \left(\frac{1-p}{p}\right)^2\\
            & = \frac{1-p}{p^2} \cdot (2-p-1+p)\\
            & = \frac{1-p}{p^2}
		\end{split}
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{geom-m}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{geom-r}
\end{center}
\subsection{Considerazione}
Un'importante proprietà del modello geometrico è l'\textbf{assenza di memoria}, pertanto vale la seguente relazione:
\begin{equation}
    \begin{split}
        P(X > i + j \mid X > i) & = \frac{P(X > i + j \cap X > i)}{P(X > i)}\\
        & = \frac{P(X > i + j)}{P(X > i)}\\
        & = \frac{(1-p)^{i+j}}{(1-p)^i}\\
        & = (1-p)^j\\
        & = P(X > j)\\
        \\
        & \implies P(X \geq i + j \mid X \geq i) = P(X \geq j)
    \end{split}
\end{equation}
Quindi per esempio, se gioco d'azzardo da più tempo di qualcuno, e ho avuto molti insuccessi, NON significa che avrò più successi di qualcuno che ha appena iniziato a giocare.
\subsection{Esempio}
Se $X$ è il numero di volte in cui mi ammalo mensilmente ed esso è $X \sim B(12, 0.01)$, la probabilità che io mi ammali 3 volte entro la fine dell'anno sarà:
\begin{equation}
    P(X = 3) = \binom{12}{3} p^3(1-p)^{12-3} \approx 0.085
\end{equation}
Se mi sono ammalato almeno 3 volte allora devo andare dal dottore:
\begin{equation}
    \begin{split}
        P(X \geq 3) & = 1 - P(X=0) - P(X=1) - P(X=2)\\
        & = 1 - \binom{12}{0} p^0(1-p)^{12-0} - \binom{12}{1} p^1(1-p)^{12-1} - \binom{12}{2} p^2(1-p)^{12-2}\\
        & \approx 0.11
    \end{split}
\end{equation}
Tuttavia a me interessa andare dal medico per la prima volta tra 5 anni: mentre prima contavo iterativamente le occorrenze di ammalarmi e ho usato un modello binomiale, ora invece quando vado per la prima volta dal dottore, smetto di contare, e dunque uso un modello geometrico $Y \sim G(p')$. Sapendo che la probabilità di ammalarmi almeno 3 volte all'anno è 0.11, qual è la probabilità che la prima volta che vada dal dottore sia tra 5 anni? (Considerando che ho 4 insuccessi e il quinto è un successo)
\begin{equation}
    \begin{split}
        P(M) & = P(Y=4)\\
        & = p'(1-p')^4\\
        & = 0.11 \cdot (1-0.11)^4\\
        & \approx 0.069
    \end{split}
\end{equation}
Ora penso: dato che sono andato dal medico, per l'ultima volta, 2 anni fa, qual è la probabilità che io debba andarci l'anno prossimo? (Almeno due insuccessi li ho visti, ne vedrò un terzo?)
\begin{equation}
    \begin{split}
        P(Y \geq 3 \mid Y \geq 2) & = P(Y \geq 1)\\
        & = 1 - p'
    \end{split}
\end{equation}

\section{Modello di Poisson}
\begin{equation}
    X\sim P(\lambda)
\end{equation}
\begin{itemize}
	\item \textbf{Supporto:} $D_X=\mathbb{N} \cup \{0\}\ \ \ (\lambda > 0)$
    \item \textbf{Funzione di massa di probabilità:}
    \begin{equation}
    	\begin{split}
   			p_X(i) & = P(X = i)\\
   			& = e^{-\lambda} \frac{\lambda^i}{i!} \mathbb{I}_{\mathbb{N}\cup\{0\}}(i)\\
            & = \sum_{i=0}^\infty e^{-\lambda} \frac{\lambda^i}{i!}\\
            & = e^{-\lambda} \sum_{i=0}^\infty \frac{\lambda^i}{i!} \rightarrow \text{ sviluppo in serie di McLaurin di } e^\lambda
		\end{split}
    \end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
    	\begin{split}
   			\mathbb{E}(X) & = \sum_{i=0}^\infty i \cdot e^{-\lambda} \cdot \frac{\lambda^i}{i!}\\
            & \rightarrow \text{ se i vale 0 non succede nulla }\\
            & = \sum_{i=1}^\infty i \cdot e^{-\lambda} \cdot \frac{\lambda^i}{i!}\\
            & \rightarrow \text{ porto fuori un } \lambda\\
            & = \sum_{i=1}^\infty i \cdot e^{-\lambda} \cdot \lambda \frac{\lambda^{(i-1)}}{i!}\\
            & \rightarrow \text{ divido per i}\\
            & = \sum_{i=1}^\infty e^{-\lambda} \lambda \frac{\lambda^{(i-1)}}{(i-1)!}\\
            & = e^{-\lambda} \lambda \sum_{i=1}^\infty \frac{\lambda^{(i-1)}}{(i-1)!}\\
            & \rightarrow \text{ considero ora } j = i-1\\
            & = \lambda e^{-\lambda} \sum_{j=0}^\infty \frac{\lambda^j}{j!}\\
            & = \lambda e^{-\lambda} e^\lambda\\
            & = \lambda
		\end{split}
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
    	\begin{split}
   			Var(X) & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
   			& = \mathbb{E}(X^2) - \lambda^2\\
            & = \left( \sum_{i=0}^\infty i^2 \cdot e^{-\lambda} \cdot \frac{\lambda^i}{i!} \right) - \lambda^2\\
            & = \left( \sum_{i=0}^\infty i \cdot e^{-\lambda} \cdot \frac{\lambda^i}{(i-1)!} \right) - \lambda^2\\
            & = \left( \sum_{i=1}^\infty i \cdot e^{-\lambda} \cdot \frac{\lambda^i}{(i-1)!} \right) - \lambda^2\\
            & = \left( \sum_{i=1}^\infty i \cdot e^{-\lambda} \cdot \lambda \frac{\lambda^{(i-1)}}{(i-1)!} \right) - \lambda^2\\
            & = \left( \lambda \sum_{i=1}^\infty i \cdot e^{-\lambda} \cdot \frac{\lambda^{(i-1)}}{(i-1)!} \right) - \lambda^2\\
            & = \left( \lambda \sum_{i=1}^\infty ((i-1)+1) \cdot e^{-\lambda} \cdot \frac{\lambda^{(i-1)}}{(i-1)!} \right) - \lambda^2\\
            & = \left( \lambda \sum_{i=1}^\infty \left( (i-1) \cdot e^{-\lambda} \cdot \frac{\lambda^{(i-1)}}{(i-1)!} + e^{-\lambda} \cdot \frac{\lambda^{(i-1)}}{(i-1)!} \right) \right) - \lambda^2\\
            & = \left( \lambda \sum_{i=1}^\infty (i-1) \cdot e^{-\lambda} \cdot \frac{\lambda^{(i-1)}}{(i-1)!} + \lambda \sum_{i=1}^\infty e^{-\lambda} \cdot \frac{\lambda^{(i-1)}}{(i-1)!} \right) - \lambda^2\\
            & \rightarrow \text{ considero ora } j = i-1\\
            & = \left( \lambda \sum_{j=0}^\infty j \cdot e^{-\lambda} \cdot \frac{\lambda^j}{j!} + \lambda \sum_{j=0}^\infty e^{-\lambda} \cdot \frac{\lambda^j}{j!} \right) - \lambda^2\\
            & = \left( \lambda \cdot \lambda + \lambda \cdot 1 \right) - \lambda^2\\
            & = \left( \lambda^2 + \lambda \right) - \lambda^2\\
            & = \lambda^2 + \lambda - \lambda^2\\
            & = \lambda\\
            &\\
            & \implies \mathbb{E}(X) = Var(X)
		\end{split}
    \end{equation}
\end{itemize}
Il parametro $\lambda$ rappresenta il numero medio di eventi che accadono solitamente, con $X$ si modella la probabilità che accada un certo numero di eventi nel relativo intervallo temporale, ma a cosa serve? Il modello di Poisson è legato al modello binomiale: immagino un modello binomiale in cui mi chiedo se, al tendere di $n$ ad infinito, non si ottenga un comportamento particolare. La risposta è sì, infatti questo comportamento particolare lo si ha per $n$ grande e $p$ piccolo con $X \sim B(n,p)$. Da qui si può trarre che $np = \lambda$ e $p = \frac{\lambda}{n}$. Nel dettaglio:
\begin{equation}
    \begin{split}
        P(X=i) & = \binom{n}{i}p^i(1-p)^{n-i}\\
        & = \binom{n}{i} \left( \frac{\lambda}{n} \right)^i \left( 1 - \frac{\lambda}{n} \right)^{n-i}\\
        & = \frac{n(n-1) \cdot ... \cdot (n-i+1)}{i!} \left( \frac{\lambda}{n} \right)^i \left( 1 - \frac{\lambda}{n} \right)^{n-i}\\
        & = \frac{n(n-1) \cdot ... \cdot (n-i+1)}{i!} \frac{\lambda^i}{n^i} \left( 1 - \frac{\lambda}{n} \right)^{n-i}\\
        & = \frac{n(n-1) \cdot ... \cdot (n-i+1)}{n^i} \frac{\lambda^i}{i!} \left( 1 - \frac{\lambda}{n} \right)^{n-i}\\
        & = \frac{n}{n} \cdot \frac{n-1}{n} \cdot ... \cdot \frac{n-i+1}{n} \frac{\lambda^i}{i!} \frac{\left( 1 - \frac{\lambda}{n} \right)^n}{\left( 1 - \frac{\lambda}{n} \right)^i}\\
        & \rightarrow \lim_{n\to\infty} \frac{n}{n} \cdot \frac{n-1}{n} \cdot ... \cdot \frac{n-i+1}{n} \frac{\lambda^i}{i!} \frac{\left( 1 - \frac{\lambda}{n} \right)^n}{\left( 1 - \frac{\lambda}{n} \right)^i}\\
        & = 1 \cdot 1 \cdot ... \cdot 1 \cdot \frac{\lambda^i}{i!} \frac{\left( 1 - \frac{\lambda}{n} \right)^n}{\left( 1 - \frac{\lambda}{n} \right)^i}\\
        & = \frac{\lambda^i}{i!} \frac{\left( 1 - \frac{\lambda}{n} \right)^n}{\left( 1 - \frac{\lambda}{n} \right)^i}\\
        & \rightarrow \text{ sappiamo che } \lim_{n\to\infty} \left( 1 + \frac{\alpha}{n} \right)^n = e^\alpha\\
        & = \frac{\lambda^i}{i!} \frac{e^{-\lambda}}{(1-0)^i}\\
        & = \frac{\lambda^i}{i!} \frac{e^{-\lambda}}{1^i}\\
        & = \frac{\lambda^i}{i!} \frac{e^{-\lambda}}{1}\\
        & = \frac{\lambda^i}{i!} e^{-\lambda}\\
    \end{split}
\end{equation}
Dunque una poissoniana può essere utilizzata per approssimare una binomiale quando $n$ cresce molto e $p$ decresce. Indicativamente per $n > 100$ e $p < 0.1$ posso dire che $B(n,p)\sim P(np)$.\\
N.B. la $f_X$ di una poissoniana può essere unimodale oppure bimodale.
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{poiss-m}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{poiss-r}
\end{center}
\subsection{Esempio 1}
Una compagnia di assicurazioni riceve in media 5 richieste di rimborso al giorno. In che frazione delle giornate vedrò arrivare meno di 3 richieste?\\
Considero $X \sim P(5)$ come il numero di richieste al giorno, dunque
\begin{equation}
    \begin{split}
        P(X < 3) & = P(X=0) + P(X=1) + P(X=2)\\
        & = e^{-5} \left( \frac{5^0}{0!} + \frac{5^1}{1!} + \frac{5^2}{2!} \right)\\
        & \approx 0.1247
    \end{split}
\end{equation}
Dunque il 12.5\% delle volte ricevo meno di 3 richieste di rimborso al giorno.\\
Con che probabilità in una settimana lavorativa (5 giorni) arrivano 4 richieste in 3 giorni (4 richieste lunedì, martedì e venerdì per esempio)?
\begin{equation}
    \begin{split}
       P(\text{in 3gg su 5 arrivano 4 richieste}) & \rightarrow P(\text{in 1gg arrivano 4 richieste})\\
       & = P(X=4)\\
       & = e^{-5} \frac{5^4}{4!}\\
       & \approx 0.1755\\
       & \rightarrow Y \sim B(5,p)\\
       & \rightarrow P(Y=3)\\
       & = \binom{5}{3} p^3 (1-p)^2\\
       & \approx 0.0367
    \end{split}
\end{equation}
\subsection{Esempio 2}
La probabilità che un pezzo sia difettoso è 0.1, ci sono 10 pezzi, qual è la probabilità che al più uno sia difettoso?\\
Innanzitutto $X \sim B(10,0.1)$, dunque
\begin{equation}
    \begin{split}
        P(X \leq 1) & = P(X=0) + P(X=1)\\
        & = \binom{10}{0} 0.1^0 \cdot 0.9^{10} + \binom{10}{1} 0.1^1 \cdot 0.9^9\\
        & \approx 0.7361
    \end{split}
\end{equation}
Però sapendo che $\lambda = n \cdot p = 10 \cdot 0.1 = 1$, possiamo dire che $X' \sim P(\lambda)$ e quindi si poteva svolgere:
\begin{equation}
    \begin{split}
        P(X' \leq 1) & = P(X'=0) + P(X'=1)\\
        & = e^{-1} \left( \frac{1^0}{0!} + \frac{1^1}{1!} \right)\\
        & \approx 0.7358
    \end{split}
\end{equation}
Da qui deduciamo che iniziamo ad avere un errore a partire dalla terza cifra significativa.

\section{Modello Ipergeometrico}
Dati
\begin{itemize}
    \item $\mathbf{n}$: numero di estrazioni senza reimmissione da eseguire
    \item $\mathbf{N}$: numero totale di oggetti "funzionanti"
    \item $\mathbf{M}$: numero totale di oggetti "difettosi"
\end{itemize}
Definiamo con
\begin{equation}
    X\sim H(n,N,M)
\end{equation}
La distribuzione ipergeometrica, ossia quella che modella un'estrazione di oggetti binari da un'urna senza effettuare reimmissione.\\
Siccome l'insieme da cui si estrae è formato da variabili aleatorie bernoulliane (1 se l'i-esimo oggetto estratto funziona, 0 altrimenti), posso calcolare
\begin{equation}
    \begin{split}
        \mathbb{E}(X_i) & = P(X_i = 1)\\
        & = \frac{\text{funzionante}}{\text{guasto} + \text{funzionante}}\\
        & = p
    \end{split}
\end{equation}
Ossia la frequenza relativa delle vittorie. Questo ci torna utile nella definizione del modello.\\
Come ulteriore considerazione, al crescere del fattore $N+M$ si sta considerando un'urna di dimensioni elevate e dunque l'ipergeometrica è approssimabile con una binomiale.
\begin{equation}
    \lim_{(N+M)\to\infty} H(n,N,M)\sim B \left( n, \frac{N}{N+M} \right)
\end{equation}
Di seguito le caratteristiche del modello ipergeometrico (N.B. $N$ e $M$ sono intercambiabili, basta che ci sia logica nell'usarli e che le formule siano corrette):
\begin{itemize}
	\item \textbf{Supporto:} $D_X=\mathbb{N} \cup \{0\}\ \ \ ([max \{ 0, n-M \}, min \{ n, N \}])$
    \item \textbf{Funzione di massa di probabilità:}
    \begin{equation}
        \begin{split}
            f_X(i) & = P(X = i)\\
            & = \frac{\binom{N}{i}\binom{M}{n-i}}{\binom{N+M}{n}} \mathbb{I}_{\{0, \dots ,n\}}(i)
        \end{split}
    \end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
    	\begin{split}
   			\mathbb{E}(X) & = \mathbb{E} \left( \sum_{i=1}^n X_i \right)\\
            & = \sum_{i=1}^n \mathbb{E}(X_i)\\
            & = \sum_{i=1}^n \frac{\text{funzionante}}{\text{guasto} + \text{funzionante}}\\
            & = np\\\\
            & \implies \text{ come nel modello binomiale }
		\end{split}
    \end{equation}
    \item \textbf{Varianza:} Inizio col calcolare
    \begin{equation}
    	\begin{split}
   			Var(X_i) & = \mathbb{E}(X_i)(1 - \mathbb{E}(X_i))\\
            & = \frac{\text{funzionante}}{\text{guasto} + \text{funzionante}} \cdot \frac{\text{guasto}}{\text{guasto} + \text{funzionante}}\\
            & = \frac{\text{funzionante} \cdot \text{guasto}}{(\text{funzionante} + \text{guasto})^2}\\
            & = \frac{NM}{(N+M)^2}
        \end{split}
    \end{equation}
    Noto che le X non sono indipendenti perché l'estrazione senza reinserimento influisce sugli esiti successivi. Dunque proseguo:
    \begin{equation}
    	\begin{split}
   			Var(X) & = Var \left( \sum_{i=1}^n X_i \right)\\
            & \rightarrow \text{ non sono indipendenti}\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} Cov(X_i, X_j)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} (\mathbb{E}(X_i \cdot X_j) - \mathbb{E}(X_i)\mathbb{E}(X_j))\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( P(X_i \cdot X_j = 1) - \left( \frac{N}{N+M} \right)^2 \right)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( P(X_i \cap X_j = 1) - \left( \frac{N}{N+M} \right)^2 \right)\\
            & \rightarrow P(E \mid F) = \frac{P(E \cap F)}{P(F)}\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( P(X_i \mid X_j = 1)P(X_i=1) - \left( \frac{N}{N+M} \right)^2 \right)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( \frac{N-1}{N-1+M} \cdot \frac{N}{N+M} - \left( \frac{N}{N+M} \right)^2 \right)\\
            & \rightarrow \text{ inverto il significato di N e M per essere coerente}\\
            & \ \ \ \ \ \text{con i miei appunti, altrimenti rischio di sbagliare}\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( \frac{M-1}{N+M-1} \cdot \frac{M}{N+M} - \left( \frac{M}{N+M} \right)^2 \right)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( \frac{M}{N+M} \cdot \left( \frac{M-1}{N+M-1} - \frac{M}{N+M} \right) \right)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( \frac{M}{N+M} \cdot \frac{(N+M)(M-1) - (N+M-1)M}{(N+M-1)(N+M)} \right)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( \frac{M}{N+M} \cdot \frac{NM - N + M^2 - M - NM - M^2 + M}{(N+M-1)(N+M)} \right)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( \frac{M}{N+M} \cdot \frac{-N}{(N+M-1)(N+M)} \right)\\
            & = \sum_{i=1}^n Var(X_i) + \sum_{i \neq j} \left( \frac{-NM}{(N+M-1)(N+M)^2} \right)\\
            & = \sum_{i=1}^n \frac{NM}{(N+M)^2} + \sum_{i \neq j} \left( \frac{-NM}{(N+M-1)(N+M)^2} \right)\\
            & = n \cdot \frac{NM}{(N+M)^2} - n(n-1) \cdot \frac{NM}{(N+M-1)(N+M)^2}
        \end{split}
    \end{equation}
    Continuando...
    \begin{equation}
    	\begin{split}
            & = \frac{nNM}{(N+M)^2} - \frac{n(n-1)NM}{(N+M-1)(N+M)^2}\\
            & = \frac{nNM}{(N+M)^2} \cdot \left( 1 - \frac{(n-1)}{N+M-1} \right)\\
            & = \frac{nNM}{(N+M)^2} \cdot \left( 1 - \frac{n-1}{N+M-1} \right)\\
            & = n \frac{N}{N+M} \frac{M}{N+M} \cdot \left( 1 - \frac{n-1}{N+M-1} \right)\\
            & = n \left( 1 - \frac{M}{N+M} \right) \frac{M}{N+M} \cdot \left( 1 - \frac{n-1}{N+M-1} \right)\\
            & = n \frac{M}{N+M} \left( 1 - \frac{M}{N+M} \right) \left( 1 - \frac{n-1}{N+M-1} \right)\\
            & = np(1-p) \left( 1 - \frac{n-1}{N+M-1} \right)\\
            & \rightarrow \text{ per } N+M \rightarrow \infty\\
            & = np(1-p)
		\end{split}
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{hyper-m}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{hyper-r}
\end{center}
\subsection{Esempio 1}
Ipotizzo $N=5$ oggetti funzionanti e $M=1$ oggetti difettosi. Posso effettuare $n=2$ estrazioni senza reimmissione: qual è la probabilità che gli $X$ oggetti funzionanti che pesco siano $i$, e dunque $P(X=i)$?\\
Innanzitutto, dati i valori precedenti, può essere $P(X=0)$? Solo se $P(X=0)=0$, perché so che $M=1$ e dunque se sono sfortunato lo pesco, ma poi non essendoci reimmissione non ci sono altri oggetti non funzionanti da pescare, quindi per forza di cose non posso avere due estrazioni, senza reimmissione, di oggetti non funzionanti quando di oggetti non funzionanti ce n'è solo uno. Nel dettaglio
\begin{equation}
    P(X=i) = \frac{\binom{N}{i}\binom{M}{n-i}}{\binom{N+M}{n}} \mathbb{I}_{\{0, \dots ,n\}}(i)
\end{equation}
E da qui sappiamo che $\binom{M}{n-i} = \binom{1}{2}$, ossia stiamo estraendo 2 elementi da un universo che ne contiene 1, è strano, tant'è che $\frac{1!}{2!(-1)!} = 0$ e quindi
\begin{equation}
    \frac{\binom{N}{i} \cdot 0}{\binom{N+M}{n}} \mathbb{I}_{\{0, \dots ,n\}}(i) = 0
\end{equation}
Quindi il dominio in realtà è
\begin{equation}
    [max \{ 0, n-M \}, min \{ n, N \}]
\end{equation}
Ossia l'intervallo tra il numero minimo di oggetti funzionanti che io devo necessariamente trovare in quelli che ho estratto (scelto come il massimo valore tra $n-M$ e 0 in caso $n-M$ fosse negativo) e il numero di estrazioni funzionanti (il minimo tra le n estrazioni e le N possibilità funzionanti).
\subsection{Esempio 2}
Ipotizzo $M=15$ oggetti funzionanti e $N=5$ oggetti guasti. Immagino ora di estrarre casualmente $n=6$ oggetti e so che il sistema funziona quando al più 2 componenti sono guasti, quindi quando su $n=6$ estrazioni almeno 4 funzionano, allora:
\begin{equation}
    \begin{split}
       P(X \geq 4) & = P(X=4 \cup X=5 \cup X=6)\\
       & = P(X=4) + P(X=5) + P(X=6)\\
       & = \frac{\binom{15}{4}\binom{5}{2} + \binom{15}{5}\binom{5}{1} + \binom{15}{6}\binom{5}{0}}{\binom{20}{6}}\\
       & \approx 0.8687
    \end{split}
\end{equation}

\chapter{Modelli Continui}

Siccome ora si lavora su intervalli continui, non ha più senso parlare della funzione di massa, in quanto ogni valore preciso avrebbe probabilità pari a 0. Fissato un punto $a$ e un $\varepsilon$ piccolo a piacere, definisco dunque la funzioni di densità nel seguente modo
\begin{equation}
    \begin{split}
        f_X(a) & =\int_{a-\frac{\varepsilon}{2}}^{a+\frac{\varepsilon}{2}}f_x(x)\ dx\\
        & \approx\varepsilon f(a)
    \end{split}
\end{equation}
E posso dire che
\begin{equation}
    \int_{-\infty}^{+\infty}f_x(x)\ dx = 1
\end{equation}
Inoltre definisco la funzione di ripartizione come
\begin{equation}
    F_X(x) = \int_{-\infty}^{x}f_x(u)\ du
\end{equation}
Da cui si evince che la funzione di ripartizione non è altro che la primitiva della funzione di densità.

\section{Modello Uniforme Continuo}
\begin{equation}
    X \sim U([a,b])
\end{equation}
\begin{itemize}
	\item \textbf{Supporto:} $D_X=[a,b]\subseteq \mathbb{R}$
    \item \textbf{Funzione di densità:}
    \begin{equation}
        f_X(x) = \frac{1}{b-a} \mathbb{I}_{[a,b]}(x)
    \end{equation}
    \item \textbf{Funzione di ripartizione:}
    \begin{equation}
    	\begin{split}
   			F_X(x) & = P(X \leq x)\\
            & = \int_{-\infty}^x f_X(u)\ du\\
            & = \int_a^x \frac{1}{b-a}\ du\\
            & = \frac{1}{b-a} u \int_a^x\\
            & = \frac{x-a}{b-a} \mathbb{I}_{[a,b]}(x) + \mathbb{I}_{(b, \infty)}(x)\\
		\end{split}
    \end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
    	\begin{split}
   			\mathbb{E}(X) & = \int_{-\infty}^\infty x f_X(x)\ dx\\
            & = \int_a^b \frac{x}{b-a}\ dx\\
            & = \frac{1}{b-a} \int_a^b x\ dx\\
            & = \frac{1}{b-a} \frac{x^2}{2} \int_a^b\\
            & = \frac{1}{b-a} \frac{b^2 - a^2}{2}\\
            & = \frac{1}{b-a} \frac{(b+a)(b-a)}{2}\\
            & = \frac{b+a}{2}\\
		\end{split}
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
    	\begin{split}
   			Var(X) & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
   			& = \mathbb{E}(X^2) - \left( \frac{b+a}{2} \right)^2\\
            & = \mathbb{E}(X^2) - \left( \frac{b+a}{2} \right)^2\\
            & = \left( \int_a^b x^2 f_X(x)\ dx \right) - \left( \frac{b+a}{2} \right)^2\\
            & = \left( \frac{1}{b-a} \int_a^b x^2\ dx \right) - \left( \frac{b+a}{2} \right)^2\\
            & = \left( \frac{1}{b-a} \frac{x^3}{3} \int_a^b \right) - \left( \frac{b+a}{2} \right)^2\\
            & = \left( \frac{b^3-a^3}{3(b-a)} \right) - \left( \frac{b+a}{2} \right)^2\\
            & = \left( \frac{(b-a)(a^2+ab+b^2)}{3(b-a)} \right) - \left( \frac{b+a}{2} \right)^2\\
            & = \left( \frac{a^2+ab+b^2}{3} \right) - \left( \frac{b+a}{2} \right)^2\\
            & = \frac{a^2+ab+b^2}{3} - \frac{(a+b)^2}{4}\\
            & = \frac{a^2+ab+b^2}{3} - \frac{a^2+2ab+b^2}{4}\\
            & = \frac{4a^2+4ab+4b^2-3a^2-6ab-3b^2}{12}\\
            & = \frac{a^2-2ab+b^2}{12}\\
            & = \frac{(b-a)^2}{12}
		\end{split}
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{unifc-d}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{unifc-r}
\end{center}
\subsection{Considerazione 1}
\begin{equation}
    \begin{split}
        P(X \in \mathbb{I}) & = \int f_X(x)\ dx\\
        & = \int_\mathbb{I} \frac{1}{b-a}\ dx\\
        & = \frac{1}{b-a} \int_\mathbb{I}\ dx\\
        & = \frac{1}{b-a} x \int_\mathbb{I}\\
        & = \frac{\mid \mathbb{I} \mid}{b-a}
    \end{split}
\end{equation}
\subsection{Considerazione 2}
\begin{equation}
    \begin{split}
        \int_{-\infty}^\infty f_X(x)\ dx & = \int_a^b \frac{1}{b-a}\ dx\\
        & = \frac{1}{b-a} x \int_a^b\\
        & = \frac{b-a}{b-a}\\
        & = 1\\
    \end{split}
\end{equation}
\subsection{Esempio}
Sappiamo che $X \sim U([0,30])$ e $X$ è il minuto, dalle 07:00 in poi, nel quale arrivo alla fermata dell'autobus: qual è la probabilità che io attenda meno di 5 minuti per l'autobus, sapendo che l'autobus passa una volta ogni 15 minuti (alle 07:00, 07:15 e 07:30)?
\begin{equation}
    \begin{split}
        P(\text{attendo meno di 5 minuti}) & = P(X=0 \cup 10 < X \leq 15 \cup 25 < X \leq 30)\\
        & = P(X=0) + P(10 < X \leq 15) + P(25 < X \leq 30)\\
        & \rightarrow P(X=0) = 0\\
        & = P(10 < X \leq 15) + P(25 < X \leq 30)\\
        & \rightarrow P(\alpha < X \leq \beta) = F_X(\beta) - F_X(\alpha)\\
        & = F_X(15) - F_X(10) + F_X(30) - F_X(25)\\
        & = \frac{15-0}{30-0} - \frac{10-0}{30-0} + \frac{30-0}{30-0} - \frac{25-0}{30-0}\\
        & = \frac{15-10}{30} + \frac{30-25}{30}\\
        & = \frac{5}{30} + \frac{5}{30}\\
        & = \frac{10}{30}\\
        & = \frac{1}{3}
    \end{split}
\end{equation}

\section{Modello Esponenziale}
La distribuzione esponenziale descrive la \textit{"durata di vita"} di un fenomeno senza memoria. Per esempio può essere usata per calcolare l'occorrenza di eventi come terremoti o predizioni varie. Essa è rappresentata da:
\begin{equation}
    X \sim E(\lambda)\ \ \ \lambda\in\mathbb{R}^+\setminus\{0\}
\end{equation}
\begin{itemize}
	\item \textbf{Supporto:} $D_x=\mathbb{R}^+$
    \item \textbf{Funzione di densità:}
    \begin{equation}
        f_X(x) = \lambda e^{-\lambda x} \mathbb{I}_{[0,+\infty)}(x)
    \end{equation}
    \item \textbf{Funzione di ripartizione:}
    \begin{equation}
    	\begin{split}
   			F_X(x) & = \int_0^x f_X(y)\ dy\\
            & = \int_0^x \lambda e^{-\lambda y}\ dy\\
            & \rightarrow z = \lambda y,\ \ \ dz = \lambda\ dy\\
            & = \int_0^{\lambda x} e^{-z}\ dz\\
            & = - e^{-z} \int_0^{\lambda x}\\
            & = - e^{-\lambda x} + e^0\\
            & = \left( 1 - e^{-\lambda x} \right) \mathbb{I}_{R^+}(x)\\
		\end{split}
    \end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
    	\begin{split}
   			\mathbb{E}(X) & = \int_0^\infty x f_X(x)\ dx\\
            & = \int_0^\infty x \lambda e^{-\lambda x}\ dx\\
            & \rightarrow f(x)=x,\ \ \ g'(x)=\lambda e^{-\lambda x},\ \ \ g(x)=-e^{-\lambda x}\\
            & = -xe^{-\lambda x} \int_0^\infty + \int_0^\infty e^{-\lambda x}\ dx\\
            & = 0 + \int_0^\infty e^{-\lambda x}\ dx\\
            & = \int_0^\infty e^{-\lambda x}\ dx\\
            & \rightarrow \text{ introduco } \lambda\\
            & = \frac{1}{\lambda} \int_0^\infty \lambda e^{-\lambda x}\ dx\\
            & \rightarrow \text{ guardo la Considerazione 1 qui sotto}\\
            & = \frac{1}{\lambda} \cdot 1\\
            & = \frac{1}{\lambda}
		\end{split}
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
    	\begin{split}
   			Var(X) & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
            & = \mathbb{E}(X^2) - \left( \frac{1}{\lambda} \right)^2\\
            & = \mathbb{E}(X^2) - \frac{1}{\lambda^2}\\
            & = \left( \int_0^\infty x^2 \lambda e^{-\lambda x}\ dx \right) - \frac{1}{\lambda^2}\\
            & \rightarrow f(x)=x^2,\ \ \ g'(x)=\lambda e^{-\lambda x},\ \ \ g(x)=-e^{-\lambda x}\\
            & = \left( -x^2e^{-\lambda x} \int_0^\infty + \int_0^\infty 2x e^{-\lambda x}\ dx \right) - \frac{1}{\lambda^2}\\
            & = \left( 0 + 2 \int_0^\infty x e^{-\lambda x}\ dx \right) - \frac{1}{\lambda^2}\\
            & = \left( 2 \int_0^\infty x e^{-\lambda x}\ dx \right) - \frac{1}{\lambda^2}\\
            & \rightarrow \text{ introduco } \lambda\\
            & = \left( \frac{2}{\lambda} \int_0^\infty \lambda x e^{-\lambda x}\ dx \right) - \frac{1}{\lambda^2}\\
            & \rightarrow f(x)=x,\ \ \ g'(x)=\lambda e^{-\lambda x},\ \ \ g(x)=-e^{-\lambda x}\\
            & = \left( \frac{2}{\lambda} \cdot \left( -xe^{-\lambda x} \int_0^\infty + \int_0^\infty e^{-\lambda x}\ dx \right) \right) - \frac{1}{\lambda^2}\\
            & = \left( \frac{2}{\lambda} \cdot \left( 0 + \int_0^\infty e^{-\lambda x}\ dx \right) \right) - \frac{1}{\lambda^2}\\
            & = \left( \frac{2}{\lambda} \cdot \left( \int_0^\infty e^{-\lambda x}\ dx \right) \right) - \frac{1}{\lambda^2}\\
            & \rightarrow \text{ introduco } \lambda\\
            & = \left( \frac{2}{\lambda} \cdot \left( \frac{1}{\lambda} \int_0^\infty \lambda e^{-\lambda x}\ dx \right) \right) - \frac{1}{\lambda^2}\\
            & \rightarrow \text{ guardo la Considerazione 1 qui sotto}\\
            & = \left( \frac{2}{\lambda} \cdot \left( \frac{1}{\lambda} \cdot 1 \right) \right) - \frac{1}{\lambda^2}\\
            & = \left( \frac{2}{\lambda} \cdot \frac{1}{\lambda} \right) - \frac{1}{\lambda^2}\\
            & = \frac{2}{\lambda^2} - \frac{1}{\lambda^2}\\
            & = \frac{1}{\lambda^2}\\
            \\
            & \implies \sigma_X = \mathbb{E}(X)
		\end{split}
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{expon-d}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{expon-r}
\end{center}
\subsection{Considerazione 1}
\begin{equation}
    \begin{split}
        \int_0^\infty f_X(x)\ dx & = \int_0^\infty \lambda e^{-\lambda x}\ dx\\
        & \rightarrow y = \lambda x,\ \ \ dy = \lambda\ dx\\
        & = \int_0^\infty e^{-y}\ dy\\
        & = - e^{-y} \int_0^\infty\\
        & = 0 + e^{-0}\\
        & = 1
    \end{split}
\end{equation}
\subsection{Considerazione 2}
Come il modello geometrico, così il modello esponenziale gode dell'assenza di memoria. Ipotizziamo infatti di avere giustamente
\begin{equation}
    F_X(x) = 1 - e^{-\lambda x}
\end{equation}
E dunque
\begin{equation}
    \begin{split}
        P(X > x) & = 1 - F_X(x)\\
        & = e^{-\lambda x}
    \end{split}
\end{equation}
Quindi
\begin{equation}
    \begin{split}
        P(X > s+t) & = e^{-\lambda (s+t)}\\
        & = e^{-\lambda s} \cdot e^{-\lambda t}
    \end{split}
\end{equation}
Infatti
\begin{equation}
    \begin{split}
        P(X > s+t) & = P(X > s) \cdot P(X > t)\\
        & \rightarrow P(X > s) = \frac{P(X > s+t)}{P(X > t)}\\
        & \implies P(X > s+t \mid X > t) = P(X > s)
    \end{split}
\end{equation}
\subsection{Considerazione 3}
Ora immagino $x_1, \dots, x_n$ indipendenti e $Y_i = \underset{i}{max}\ X_i$, allora:
\begin{equation}
    \begin{split}
        F_Y(x) & = P(Y \leq x)\\
        & = P(\underset{i}{max}\ X_i \leq x)\\
        & \rightarrow \underset{i}{max}\ X_i \leq x \iff \forall i\ X_i \leq x\\
        & = P(\forall i\ X_i \leq x)\\
        & \rightarrow \text{ per indipendenza } P(X_1>x, X_2>x, \dots)\\
        & = \prod_{i=1}^n P(X_i \leq x)\\
        & = \prod_{i=1}^n F_{X_i}(x)\\
    \end{split}
\end{equation}
Quindi se $X_1, \dots, X_n$ sono i.i.d. (\textit{indipendenti e identicamente distribuite}) secondo $F$ allora
\begin{equation}
    \begin{split}
        F_Y(x) & = \prod_{i=1}^n F(x)\\
        & = F(x)^n
    \end{split}
\end{equation}
Per cui, date $X_1, \dots, X_n$ indipendenti (e non i.i.d), allora
\begin{equation}
    \begin{split}
        Z & = \underset{i=1, \dots, n}{min}\ X_i\\
        & = P(Z > x)\\
        & \rightarrow \underset{i}{min}\ X_i > x \iff \forall i\ X_i > x\\
        & = P(\forall i\ X_i > x)\\
        & = \prod_{i=1}^n P(X_i > x)\\
        & = \prod_{i=1}^n \left( 1 - P(X_i \leq x) \right)\\
        & = \prod_{i=1}^n (1 - F_{X_i}(x))\\
        & \rightarrow \text{ possiamo vedere } P(Z > x) \text{ come } 1 - P(Z \leq x)\\
        & = 1 - \prod_{i=1}^n (1 - F_{X_i}(x))\\
        & \rightarrow X_i \text{ i.i.d secondo } F_Z(x) = 1 - (1 - F_Z(x))^n\\
        & \rightarrow X_i \sim E(\lambda_i) \text{ e indipendente}\\
        & \rightarrow F_{X_i}(x) = 1 - e^{-\lambda_i x}\\
        & = 1 - \prod_{i=1}^n (e^{-\lambda_i x})\\
        & = 1 - e^{\sum_{i=1}^n -\lambda_i x}\\
        & = 1 - e^{-x \sum_{i=1}^n \lambda_i}\\
        & = 1 - e^{-x \lambda}\\
        & = 1 - e^{-\lambda x}\\
        \\
        & \implies F_Z = 1 - e^{-\lambda x}\\
        & \implies Z \sim E(\lambda)
    \end{split}
\end{equation}
Ho dimostrato che se considero $X_1, \dots, X_n$ e $\forall i\ X_i \sim E(\lambda_i)$ allora $min\ X_i \sim E(\sum_{i=1}^n \lambda_i)$
\subsection{Considerazione 4}
Dato $X \sim E(\lambda)$ con $c \in R^+$ e $Y = c \cdot X$, allora:
\begin{equation}
    \begin{split}
        F_Y(x) & = P(Y \leq x)\\
        & = P(cX \leq x)\\
        & = P \left(X \leq \frac{x}{c} \right)\\
        & = F_X \left( \frac{x}{c} \right)\\
        & = 1 - e^{-\lambda \frac{x}{c}}\\
        & = 1 - e^{- \frac{\lambda}{c} x}
    \end{split}
\end{equation}
\subsection{Esempio 1}
La mia $X \sim E(1)$ rappresenta le ore necessarie per riparare un macchinario, qual è la probabilità che io ci impieghi più di 2 ore per ripararlo?:
\begin{equation}
    \begin{split}
        P(X > 2) & = 1 - F_X(2)\\
        & = 1 - (1 - e^{-2})\\
        & = e^{-2}\\
        & \approx 0.1353
    \end{split}
\end{equation}
Qual è invece la probabilità che io ci impieghi 3 ore dato che prima ce ne ho messe 2 di ore?
\begin{equation}
    \begin{split}
        P(X > 3 \mid X > 2) & = P(X > 1+2 \mid X > 2)\\
        & = P(X > 1)\\
        & = e^{-1}\\
        & \approx 0.3678
    \end{split}
\end{equation}
\subsection{Esempio 2}
La mia $X \sim E \left( \frac{1}{20} \right)$ rappresenta le migliaia di km che l'auto percorre prima di rompersi. Ho usato l'auto per 10k km, la vendo e il compratore si domanda la probabilità che duri altri 20km, quindi:
\begin{equation}
    \begin{split}
        P(X > 30 \mid X > 10) & = P(X > 20+10 \mid X > 10)\\
        & = P(X > 20)\\
        & = e^{-\frac{20}{20}}\\
        & = e^{-1}\\
        & \approx 0.3678
    \end{split}
\end{equation}
Come cambierebbe la risposta se $X \sim U((0,40))$?
\begin{equation}
    \begin{split}
        P(X > 30 \mid X > 10) & = \frac{P(X>30 \cap X>10)}{P(X>10)}\\
        & \rightarrow \text{ se si verifica } X > 30 \text{ allora si è verificato anche } X > 10\\
        & = \frac{P(X>30)}{P(X>10)}\\
        & = \frac{P(30<X<40)}{P(10<X<40)}\\
        & = \frac{F_X(40) - F_X(30)}{F_X(40) - F_X(10)}\\
        & = \frac{\frac{40-0}{40-0} - \frac{30-0}{40-0}}{\frac{40-0}{40-0} - \frac{10-0}{40-0}}\\
        & = \frac{\frac{10}{40}}{\frac{30}{40}}\\
        & = \frac{1}{3}\\
    \end{split}
\end{equation}

\section{Modello Gaussiano}
\begin{equation}
    X\sim G(\mu,\sigma)\ \ \ \mu\in\mathbb{R},\ \sigma\in\mathbb{R}^+\setminus\{0\}
\end{equation}
Questo modello è noto anche come \textit{Modello Normale}, tuttavia a volte specifica un caso particolare del Modello Gaussiano.
\begin{itemize}
	\item \textbf{Supporto:} $D_x=\mathbb{R}$
    \item \textbf{Funzione di densità:}
    \begin{equation}
        f_X(x) = \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}}
    \end{equation}
    \item \textbf{Funzione di ripartizione:}
    \begin{equation}
        F_X(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}}\ du
    \end{equation}
    \item \textbf{Valore atteso:}
    \begin{equation}
        E(X) = \mu
    \end{equation}
    \item \textbf{Varianza:}
    \begin{equation}
        Var(X) = \sigma^2
    \end{equation}
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{norm-d}
\end{center}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{norm-r}
\end{center}
\subsection{Considerazione 1}
Consideriamo
\begin{equation}
    \lim_{x\rightarrow\infty} f_X(x) = \lim_{x\rightarrow\infty} \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}
Per prima cosa calcoliamo la derivata prima $f'_X(x)$:
\begin{equation}
    \begin{split}
        f'_X(x) & = -\frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \frac{1}{2\sigma^2}2(x-\mu)\\
        & = -\frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \frac{1}{\sigma^2}(x-\mu)\\
        & = -\frac{1}{\sqrt{2\pi}\sigma^3} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot (x-\mu)\\
        & = \frac{1}{\sqrt{2\pi}\sigma^3} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot (\mu-x)\\
        & \rightarrow f'_X(x) \geq 0 \text{ quando } \mu-x \geq 0 \rightarrow x \leq \mu\\
        \\
        & \implies \text{ cresce verso } \mu \text{ per poi decrescere}\\
    \end{split}
\end{equation}
Ora calcoliamo la derivata seconda $f''_X(x)$:
\begin{equation}
    \begin{split}
        f''_X(x) & = \frac{1}{\sqrt{2\pi}\sigma^3} \cdot \left( e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \left( -\frac{2(x-\mu)}{2\sigma^2} \right) \cdot (\mu-x) - e^{-\frac{(x-\mu)^2}{2\sigma^2}} \right)\\
        & \frac{1}{\sqrt{2\pi}\sigma^3} \cdot \left( e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \left( -\frac{(x-\mu)}{\sigma^2} \right) \cdot (\mu-x) - e^{-\frac{(x-\mu)^2}{2\sigma^2}} \right)\\
        & \frac{1}{\sqrt{2\pi}\sigma^3} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \cdot \left( \frac{(x-\mu)^2}{\sigma^2} - 1 \right)\\
        & \rightarrow f''_X(x) \geq b \text{ quando } \frac{(x-\mu)^2}{\sigma^2} - 1 \geq 0 \rightarrow \left( \frac{x-\mu}{\sigma} \right) \geq 1\\
    \end{split}
\end{equation}
Questi calcoli dimostrano che non solo $G(\mu,\sigma)$ è convessa fino a $\mu-\sigma$ e da $\mu+\sigma$, con $\mu$ al centro, ma anche che quindi essa assumerà la classica forma a campana. Inoltre se abbiamo $\mu_2 < \mu_1$, allora $\mu_2$ farà traslare la funzione a sinistra rispetto a $\mu_1$, siccome stiamo spostando a sinistra il valore atteso; mentre se $\sigma_2 < \sigma_1$ allora avremo un picco, sempre in $\mu$ ma più evidenziato per $\sigma_2$, in quando stiamo dicendo che i dati sono meno deviati rispetto a $\sigma_1$ e quindi più concentrati.
\subsection{Considerazione 2}
\begin{equation}
    \begin{split}
        f_X(\mu) & = \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(\mu-\mu)^2}{2\sigma^2}}\\
        & = \frac{1}{\sqrt{2\pi}\sigma}
    \end{split}
\end{equation}
Quindi la densità in $\mu$ sarà più grande tanto più $\sigma$ è piccola.
\subsection{Considerazione 3}
Ipotizzo di essere abbastanza confidente del fatto che
\begin{equation}
    \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\left( \frac{x-\mu}{\sigma} \right)^2}\ dx = 1
\end{equation}
Ma come lo provo?\\
Innanzitutto  considero $z = \frac{x-\mu}{\sigma}$ e quindi $dz = \frac{dx}{\sigma}$, dunque
\begin{equation}
    \begin{split}
        I = & \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} e^{-z^2}\ dz\\
        &\rightarrow I \cdot I\\
        & \frac{1}{2\pi} \int_{-\infty}^\infty e^{-x^2}\ dx \int_{-\infty}^\infty e^{-y^2}\ dy\\
        & \frac{1}{2\pi} \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-x^2} e^{-y^2}\ dx\ dy\\
        & \frac{1}{2\pi} \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(x^2+y^2)}\ dx\ dy\\
        & \rightarrow p\sin\theta = y,\ p\cos\theta = x \text{ e matrice jacobiana}\\
        & = 1
    \end{split}
\end{equation}
\subsection{Considerazione 4}
Partendo da $F_X(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}\sigma} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}}\ du$ io so che $\mu = \mathbb{E}(X)$ e che $\sigma^2 = Var(X)$, dunque se io prendo $y = ax + b$, se $X \sim G(\mu,\sigma)$ allora $Y \sim G(a\mu+b, a\sigma)$.\\
Inoltre so che se ho $X_1 \sim G(\mu_1, \sigma_1)$ e $X \sim G(\mu_2, \sigma_2)$ più indipendenza, allora avrò
\begin{equation}
    X_1 + X_2 \sim G \left( \mu_1+\mu_2, \sqrt{\sigma_1^2+\sigma_2^2} \right)
\end{equation}
N.B. Sto sommando variabili aleatorie, non funzioni. Nel caso della somma di funzioni non posso fare come ho scritto e il risultato anzi sarà una funzione multimodale.
\subsection{Considerazione 5}
Se usassi $Y = \frac{X-\mu}{\sigma}$ otterrei
\begin{equation}
    \begin{split}
        \mathbb{E}(Y) & = \mathbb{E}\left( \frac{X-\mu}{\sigma} \right)\\
        & = \frac{1}{\sigma} \mathbb{E}(X-\mu)\\
        & = \frac{1}{\sigma} \left( \mathbb{E}(X)  -\mu \right)\\
        & = \frac{1}{\sigma} \left( \mathbb{E}(X) - \mathbb{E}(X) \right)\\
        & = 0
    \end{split}
\end{equation}
Inoltre:
\begin{equation}
    \begin{split}
        Var(Y) & = Var \left( \frac{X-\mu}{\sigma} \right)\\
        & = \frac{1}{\sigma^2} Var(X-\mu)\\
        & = \frac{1}{\sigma^2} Var(X)\\
        & = 1
    \end{split}
\end{equation}
Dunque otteniamo
\begin{equation}
    X \sim G(\mu,\sigma) \rightarrow Y \sim G(0,1)
\end{equation}
Dunque $Y = \frac{X-\mu}{\sigma}$ non è altro che la gaussiana normale standard. Essa mi è utile perché tramite tabulati posso convertire i miei dati per usarli nella gaussiana standard.
\subsection{Legge empirica}
Se un campione numerico è approssimativamente normale, ha media campionaria $\overline{x}$ e deviazione standard $s$, allora:
\begin{enumerate}
	\item Circa il $68\%$ dei suoi dati cade nell'intervallo $\overline{x} \pm s$
	\item Circa il $95\%$ dei suoi dati cade nell'intervallo $\overline{x} \pm 2s$
	\item Circa il $99.7\%$ dei suoi dati cade nell'intervallo $\overline{x} \pm 3s$
\end{enumerate}
\subsection{Considerazione 6}
Partendo dalla Considerazione 4, se invece di applicare una trasformazione lineare io avessi usato variabili aleatorie gaussiane, il risultato sarebbe stata una variabile aleatoria gaussiana, pertanto, avendo $X_1, \dots, X_n$ indipendenti, $\forall i\ X_i \sim G(\mu_i, \sigma_i)$ allora $Y = \sum_{i=1}^n X_i$. In particolare $\mathbb{E}(Y) = \sum_{i=1}^n \mu_i$, $Var(Y) = \sum_{i=1}^n \sigma_i^2$ e nel complesso
\begin{equation}
    Y \sim G \left( \sum_{i=1}^n \mu_i, \sqrt{\sum_{i=1}^n \sigma_i^2} \right)
\end{equation}
Quindi sommando variabili aleatorie che hanno un modello comune, ottengo una variabile aleatoria che fa ancora parte di quel modello comune, questo concetto è noto come \textbf{riproducibilità}. Quindi se $X$ è una variabile casuale con media $\mu$ e deviazione standard $\sigma$, allora la standardizzazione di $X$ tramite la trasformazione $Z = \frac{X-\mu}{\sigma} \sim G(0,1)$ segue una distribuzione normale standard $N(0,1)$. In altre parole, la variabile standardizzata $Z$ ha media $0$ e deviazione standard $1$, indipendentemente dalla distribuzione originale di $X$. Questo processo rende le misurazioni comparabili, poiché esprime ciascun valore in termini di deviazioni standard dal valore atteso, facilitando così la valutazione delle relazioni e delle differenze tra dati che potrebbero avere scale diverse.\\
E' importante notare che la riproducibilità vale, oltre che per le variabili aleatorie gaussiane, anche per le variabili aleatorie poissoniane e per le binomiali, in condizioni specifiche, tant'è che
\begin{equation}
	X_1 \sim B(n,p)\ \ \ \ X_2 \sim B(m, p)\ \ \ \ X_1 + X_2 = B(n + m, p)
\end{equation}
\begin{equation}
	X_1 \sim P(\lambda_1)\ \ \ \ X_2 \sim P(\lambda_2)\ \ \ \ X_1 + X_2 = P(\lambda_1 + \lambda_2)
\end{equation}
A proposito della legge di riproducibilità, è giusto tenere in considerazione che la differenza di variabili aleatorie distribuite secondo un modello normale è anch'essa una variabile aleatoria normale. Infine, la distribuzione gaussiana e la distribuzione esponenziale, godono entrambe del concetto di scalabilità.
\subsection{Considerazione 7}
Cosa succede se voglio calcolare $F_X(x)$? Viene un integrale non risolvibile in forma analitica, quindi uso la Normale Standard:
\begin{equation}
    \begin{split}
        F_X(x) & = P(X \leq x)\\
        & = P \left( \frac{X-\mu}{\sigma} \leq X \leq \frac{X-\mu}{\sigma} \right)\\
        & = P \left( Z \leq \frac{X-\mu}{\sigma} \right)\\
        & = F_Z \left( \frac{X-\mu}{\sigma} \right)\\
        & = \Phi
    \end{split}
\end{equation}
E nel dettaglio valgono le seguenti proprietà:
\begin{itemize}
	\item $\Phi(0) = \frac{1}{2}$
    \item $\Phi(-x) = P(Z > x) = 1 - P(Z \leq x) = 1 - \Phi(x)$
    \item $P( \mid X - \mu \mid \leq n\sigma ) \implies P(-n \leq X \leq n) = P( \mid X \mid \leq n)$\\
    In generale:
    \begin{equation}
    \begin{split}
        P(-n \leq Z \leq n) & = P( \mid Z \mid \leq n)\\
        & = \Phi(n) - \Phi(-n)\\
        & = \Phi(n) - (1 - \Phi(n))\\
        & = 2\Phi(n) - 1
    \end{split}
\end{equation}
\end{itemize}
\subsection{Quantili}
La mediana di $X$ è $m \in \mathbb{R}$ tale che $P(X \leq m) = P(X \geq m) = \frac{1}{2}$ (ossia 0, il valore atteso).\\
La moda di $X$ è la specificazione che massimizza la densità o la massa di probabilità (ossia 0, il valore atteso).\\
Il quantile di livello $q \in [0,1]$ di $X$ è la specificazione $x_q \in \mathbb{R}$ tale che $P(X \leq x) = q$. Dunque pongo $X \sim G(\mu,\sigma)$, da qui, se volessi calcolare $q = P(X \leq 2)$ procedo come
\begin{equation}
    \begin{split}
        q & = P(X \leq x_q)\\
        & = F_X(x_q)\\
        & = F_X^{-1}(q)
    \end{split}
\end{equation}
Da qui
\begin{itemize}
    \item $F_X \left( F_X^{-1}(q) \right)$
    \item $F_X^{-1}(q) = x_q$ (Funzione \texttt{ppf} di Python)
    \item $F_X(x_q) = q$
\end{itemize}
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{quant-1}
\end{center}
Risulta importante notare che il quantile Q1 e il quantile Q3 si trovano rispettivamente a $\mu - z \cdot \sigma$ e $\mu + z \cdot \sigma$, con $z = 0.675$. Questo perché osservando le tabelle della distribuzione cumulativa normale standardizzata, possiamo notare che $\Phi(-0.675) = 0.25 = 25\%$ e $\Phi(0.675) = 0.75 = 75\%$.
\begin{center}
    \includesvg[inkscape=force, width=0.75\textwidth]{quant-2}
\end{center}
Per disegnare un Q-Q plot, ossia i grafici sovrastanti, basta seguire i seguenti passi:
\begin{enumerate}
	\item Raccolgo i dati e mi assicuro di avere un numero abbastanza elevato di valori così da ottenere un'analisi significativa
	\item Ordino i dati in ordine crescente
	\item Calcolo i quantili teorici, solitamente di una distribuzione normale standard, e i quantili campionari
	\item Creo una coppia di coordinate $(x_i, y_i)$ dove le $x_i$ saranno i quantili teorici mentre le $y_i$ quelli campionari
	\item Traccio la linea bisettrice di riferimento per osservare se i punti giacciono su essa
\end{enumerate}
\subsubsection{Esempio}
Si determini il più piccolo valore di $X$, chiamiamolo $x_{0.8}$, tale che $P(X \leq x_{0.8}) \geq 0.8$. Questo non è altro che $F_X(x) \geq 0.8$ ossia il quantile $0.8$ di $X$, che può essere facilmente calcolato con \texttt{X.ppf(0.8)}
\subsection{Teorema Centrale del Limite}
Il Teorema Centrale del Limite o Teorema del Limite Centrale asserisce che, dati $X_1, \dots, X_2$ i.i.d. allora $\forall i\ \mathbb{E}(X_i) = \mu,\ Var(X_i) = \sigma^2$ si ha che
\begin{equation}
	\sum^n_{i=1} X_i\ \dot\sim\ N \left(n\mu,\sqrt{n}\sigma \right)
\end{equation}
E quindi, posto
\begin{equation}
	Y_n = \frac{\frac{1}{n}\sum^n_{j=1} X_j - \mu}{\frac{\sigma}{\sqrt{n}}}\ \dot\sim\ N(0,1)
\end{equation}
Allora
\begin{equation}
	\frac{\sum^n_{i=1} X_i - n\mu}{\sqrt{n}\sigma}\ \dot\sim\ N(0,1)
\end{equation}
E dunque
\begin{equation}
	\lim_{n \rightarrow \infty} P \left( \frac{\sum^n_{i=1} X_i - n\mu}{\sqrt{n}\sigma} \leq x \right) = \Phi(x)
\end{equation}
Dove tanto più è grande $n$, tanto migliore sarà l'approssimazione.\\
Empiricamente si evidenzia come per $n \geq 30$ si iniziano ad avere risultati accettabili. In ogni caso se voglio verificare qualitativamente la bontà dell'approssimazione posso usare un QQ-plot.\\
Il TCL è valido per ognuna delle distribuzioni viste in questo corso e ricordiamoci infine che quindi, nel caso della media campionaria, avremo
\begin{equation}
	\frac{1}{n} \sum^n_{i=1} X_i\ \dot\sim\ N \left(\mu, \frac{\sigma}{\sqrt{n}} \right)
\end{equation}
\subsection{Teorema di De Moivre-Laplace}
Una distribuzione normale può essere usata per approssimare una distribuzione binomiale, a patto di avere un parametro $n$ sufficientemente grande. Dato $X \sim B(n,p)$ con $n$ grande, allora
\begin{equation}
	X = \sum_{i=1}^n X_i\ \dot\sim\ N \left( np, \sqrt{np(1-p)} \right) \rightarrow \frac{X - np}{\sqrt{np(1-p)}}\ \dot\sim\ N(0,1)
\end{equation}
In generale
\begin{equation}
	\frac{X - \mu}{\sqrt{Var(X)}}\ \dot\sim\ N(0,1)
\end{equation}
N.B. Su Python andranno usati i comandi \texttt{Z = st.norm()} per definire una distribuzione normale e \texttt{Z.rvs()} per ricevere una specificazione plausibile per una variabile aleatoria della distribuzione normale create precedentemente. Entrambe le funzioni appartengono al modulo \texttt{scipy.stats}.
\subsection{Esempio 1}
Per $n>1$ si controlli che $P(-\varepsilon < T_n - p < \varepsilon) \approx 2\Phi \left( \frac{\varepsilon\sqrt{n}}{\sigma} \right) - 1$:
\begin{equation}
    \begin{split}
        P(-\varepsilon < T_n - p < \varepsilon) & \approx P \left( -\frac{\varepsilon\sqrt{n}}{\sigma} < Z < \frac{\varepsilon\sqrt{n}}{\sigma} \right)\\
        & \approx 2\Phi \left( \frac{\varepsilon\sqrt{n}}{\sigma} \right) - 1
    \end{split}
\end{equation}
\subsection{Esempio 2}
Verifico che $P(\mu-k \leq X \leq \mu+k) \leq 2\Phi \left( \frac{k}{\sigma} \right) - 1$:
\begin{equation}
    \begin{split}
        P(\mu-k \leq X \leq \mu+k) & = P(\mu \leq X-k \leq \mu)\\
        & = P(\mu \leq X^* \leq \mu)\\
        & = F_X \left( \frac{k}{\sigma} \right) - F_X \left( -\frac{k}{\sigma} \right)\\
        & \approx \Phi \left( \frac{k}{\sigma} \right) - \Phi \left( -\frac{k}{\sigma} \right)\\
        & = \Phi \left( \frac{k}{\sigma} \right) - \left( 1 - \Phi \left( \frac{k}{\sigma} \right) \right)\\
        & = 2\Phi \left( \frac{k}{\sigma} \right) - 1
    \end{split}
\end{equation}
\subsection{Esempio 3}
Ipotizzo di avere 25000 polizze auto e $X$ è il risarcimento annuo di un cliente. Mi aspetto di risarcire intorno ai 320 euro ogni cliente, quindi $\mathbb{E}(X) = 320$ euro, con una deviazione di $\sigma_x = 540$ euro: qual è la probabilità di dover risarcire più di $8.3 \cdot 10^6$ euro?
\begin{equation}
    \begin{split}
        P(\text{più di } 8.3 \cdot 10^6 \text{ di risarcimento/anno})& = P(Y > 8.3 \cdot 10^6)\\
        & \rightarrow \text{ per il TCL } \sum^n_{i=1} X_i\ \dot\sim\ N \left(n\mu,\sqrt{n}\sigma \right)\\
        & \rightarrow \sum_{i=1}^{25000} X_i = Y\ \dot\sim\ N \left(25000 \cdot 320, \sqrt{25000} \cdot 540 \right)\\
        & = Y\ \dot\sim\ N(8 \cdot 10^6, 8.54 \cdot 10^6)\\
        & \rightarrow \text{ standardizzo}\\
        & = P \left( \frac{Y - 8 \cdot 10^6}{8.54 \cdot 10^6} > \frac{8.3 \cdot 10^6 - 8 \cdot 10^6}{8.54 \cdot 10^6} \right)\\
        & = P \left( \left( \dot\sim N(0,1) \right) >\ \left( \approx 3.51 \right) \right)\\
        & \approx P(Z > 3.51)\\
        & = 1 - \Phi(3.51)\\
        & \approx 2.2 \cdot 10^{-4}
    \end{split}
\end{equation}
Tuttavia sto approssimando due volte, qual è la soglia entro cui questa approssimazione continua a valere? Lo domando alla \textit{Funzione Cumulativa Empirica (ECDF)}
\begin{equation}
	\hat{F}(x) = \frac{1}{n} \sum_{i=1}^n \mathbb{I}_{(-\infty, x]}(X_i)
\end{equation}
E quindi
\begin{equation}
	\hat{F}(8.3 \cdot 10^6) = \frac{1}{25000} \sum_{i=1}^n \mathbb{I}_{(-\infty, 8.3 \cdot 10^6]}(X_i)
\end{equation}
Ipotizzo (con dati immaginari) che $\hat{F}(8.3 \cdot 10^6) \approx 0.85$, questo significa che l'$85\%$ delle osservazioni è inferiore o uguale a $8.3 \cdot 10^6$ e che quindi l'approssimazione normale sembra funzionare abbastanza bene fino a quel punto di divergenza. Quindi se per esempio avessi avuto bisogno di una precisione del $95\%$ quel valore non sarebbe andato bene.
\subsection{Esempio 4}
Il 30\% degli iscritti è frequentante, quindi $P(\text{uno studente sia frequentante}) = 0.3$. Modelliamo $X$ come il numero di studenti frequentanti su 450, tale che $X \sim B(450, 0.3)$. La nostra aula ha 150 posti: qual è la probabilità di superare la capienza massima dell'aula?\\
Sappiamo che
\begin{equation}
	X\ \dot\sim\ N \left( 450 \cdot 0.3, \sqrt{450 \cdot 0.3 \cdot 0.7} \right) \rightarrow X\ \dot\sim\ N(135, 9.72)
\end{equation}
Quindi servendoci della distribuzione normale standard $Z \sim N(0,1)$
\begin{equation}
	\begin{split}
		P(X > 150) & = P \left( \frac{X-135}{9.72} > \frac{150-135}{9.72} \right)\\
		& \approx P(Z > 1.59)\\
		& \approx 0.06
	\end{split}
\end{equation}
Dunque ho un 6\% di giornate di lezione in cui gli studenti iscritti non trovano posto in aula.
\subsection{Esempio 5}
La media delle precipitazioni del 2022 è 12.08cm mentre la deviazione standard è 9.1cm. Le precipitazioni del 2022 sommate a quelle del 2023, supereranno i 25cm?\\
Sappiamo che $X_{2022},\ X_{2023} \sim N(\mu, \sigma)$, dunque
\begin{equation}
	Y = X_{2022} + X_{2023} \sim N \left( 2\mu, \sqrt{2}\sigma \right)
\end{equation}
Allora
\begin{equation}
    \begin{split}
       P(X_{2022} + X_{2023} > 25) & = P(Y > 25)\\
       & = P \left( \frac{Y - 2 \cdot 12.08}{\sqrt{2} \cdot 3.1} > \frac{25 - 2 \cdot 12.08}{\sqrt{2} \cdot 3.1} \right)\\
       & = P(Z > 0.1916)\\
       & \approx 0.4240
    \end{split}
\end{equation}
Cosa succede se volessi calcolare la probabilità che nel 2022 ci sia stata più pioggia (di almeno 3cm)?
$$P(X_{2022} > X_{2023} + 3) = P(X_{2022} - X_{2023} > 3)$$
Quindi se io definisco $T = X_{2022} - X_{2023}$, il cui risultato sarà una distribuzione normale standard, grazie alla proprietà della riproducibilità, allora
\begin{equation}
	\begin{split}
		\mathbb{E}(T) & = \mathbb{E}(X_{2022}) - \mathbb{E}(X_{2023})\\
		& = 0
    \end{split}
\end{equation}
E infine
\begin{equation}
	\begin{split}
		Var(T) & = Var(X_{2022} - X_{2023})\\
		& = Var(X_{2022}) + Var(X_{2023})\\
		& = 2\sigma^2
    \end{split}
\end{equation}
\subsection{Esempio 6}
So che $P \left( \mid \overline{X}_n - \mu \mid \leq \varepsilon \right)$, determino $n$ dato $\alpha$ tale che $P \left( \mid \overline{X}_n - \mu \mid \leq \varepsilon \right) \geq \alpha$.
Risolvo il seguente
\begin{equation}
    \begin{split}
       P \left( \mid \overline{X}_n - \mu \mid \leq \varepsilon \right) & = P \left( -\varepsilon \leq \overline{X}_n - \mu \leq \varepsilon \right)\\
       & \rightarrow TCL\\
       & = P \left( -\frac{\varepsilon}{\frac{\sigma}{\sqrt{n}}} \leq \frac{\overline{X_n} - \mu}{\frac{\sigma}{\sqrt{n}}} \leq \frac{\varepsilon}{\frac{\sigma}{\sqrt{n}}} \right)\\
       & \approx P \left( -\frac{\varepsilon}{\frac{\sigma}{\sqrt{n}}} \leq Z \leq \frac{\varepsilon}{\frac{\sigma}{\sqrt{n}}} \right)\\
       & = \Phi \left( \frac{\varepsilon}{\frac{\sigma}{\sqrt{n}}} \right) - \Phi \left( -\frac{\varepsilon}{\frac{\sigma}{\sqrt{n}}} \right)\\
       & = 2\Phi \left( \frac{\varepsilon}{\frac{\sigma}{\sqrt{n}}} \right) - 1\\
       & = 2\Phi \left( \frac{\varepsilon\sqrt{n}}{\sigma} \right) - 1\\
       & \rightarrow 2\Phi \left( \frac{\varepsilon\sqrt{n}}{\sigma} \right) - 1 \geq \alpha\\
       & \rightarrow \Phi \left( \frac{\varepsilon\sqrt{n}}{\sigma} \right) \geq \frac{\alpha+1}{2}\\
       & \rightarrow \frac{\varepsilon\sqrt{n}}{\sigma} \geq \Phi^{-1} \left( \frac{\alpha+1}{2} \right)\\
       & \rightarrow \sqrt{n} \geq \Phi^{-1} \left( \frac{\alpha+1}{2} \right) \cdot \frac{\sigma}{\varepsilon}\\
       & \rightarrow n \geq \left( \Phi^{-1} \left( \frac{\alpha+1}{2} \right) \cdot \frac{\sigma}{\varepsilon} \right)^2
    \end{split}
\end{equation}
\section{Esercizi}
\textbf{Uno stabilimento ha 6 macchinari che usano in media energia elettrica
per 20 minuti ogni ora.}
\begin{itemize}
	\item \textbf{Se i macchinari vengono usati indipendentemente, mostrare che la probabilità che 4 o più macchinari usino energia elettrica
contemporaneamente è 0.1.}\\
	Consideriamo la variabile casuale $X =$ "numero di macchine che consumano energia", possiamo quindi assumere che $X \approx B(n,p)$, dove $n = 6$, numero totale di macchinari disponibili e $p = \frac{20}{60} = \frac{1}{3} = 0.33$, "successo" ossia minuti in cui i macchinari usano energia ogni ora. Dunque $X \approx B(6, 0.33)$. A questo punto basterà considerare il seguente
	\begin{equation}
    	\begin{split}
       		P(X \geq 4) & = P(X=4) + P(X=5) + P(X=6)\\
       		& = 0.0823 + 0.0165 + 0.0014\\
       		& = 0.1002
    	\end{split}
	\end{equation}
	Ovviamente potevo anche considerare
	$$P(X \geq 4) = 1 - P(X=3) - P(X=2) - P(X=1) - P(X=0) = \cdots$$
	\item \textbf{Se lo stabilimento avesse 60 macchinari, quale sarebbe la probabilità di avere al massimo 30 macchinari in funzione contemporaneamente?}\\
	Come prima $X \approx B(60, 0.33)$, ma siccome abbiamo un valore di $n$ elevato, possiamo ricorrere all'approssimazione normale della distribuzione binomiale considerando
	$$N \left( np, \sqrt{np(1-p)} \right)$$
	Il nostro $\mu$ non sarà altro che $np = 60 \cdot \frac{1}{3} = 20$ mentre la varianza $np(1-p) = 20 \cdot \frac{2}{3} = 13.33$ e quindi la deviazione standard sarà uguale a $\sqrt{13.33} = 3.65$. A questo punto la probabilità di avere al massimo 30 macchinari in funzione risulta essere
	\begin{equation}
    	\begin{split}
       		P(X \leq 30) & \approx \Phi \left( \frac{30 + 0.5 - 20}{3.65} \right)\\
       		& = \Phi(2.877)\\
       		& = 0.998
    	\end{split}
	\end{equation}
	\item \textbf{Sempre considerando 60 macchinari trovare un numero approssimato $r$, tale che la probabilità che più di $r$ macchinari usino energia elettrica allo stesso tempo sia 0.1.}\\
	Vogliamo cercare $r$ tale che
	$$P(X > r) = 0.1$$
	Dunque procediamo semplicemente nel seguente modo
	\begin{equation}
    	\begin{split}
       		P(X > r) & = 1 - P(X \leq r)\\
       		& = 1 - \Phi \left( \frac{r + 0.5 - 20}{3.65} \right)\\
       		& \rightarrow 1 - \Phi \left( \frac{r + 0.5 - 20}{3.65} \right) = 0.1\\
       		& \rightarrow - \Phi \left( \frac{r - 19.5}{3.65} \right) = 0.1 - 1\\
       		& \rightarrow \Phi \left( \frac{r - 19.5}{3.65} \right) = - 0.1 + 1\\
       		& \rightarrow \Phi \left( \frac{r - 19.5}{3.65} \right) = 0.9\\
       		& \rightarrow \frac{r - 19.5}{3.65} = \Phi^-1 (0.9)\\
       		& \rightarrow r - 19.5 = \Phi^-1 (0.9) \cdot 3.65\\
       		& \rightarrow r = 19.5 + \Phi^-1 (0.9) \cdot 3.65\\
       		& \rightarrow r = 19.5 + 1.29 \cdot 3.65\\
       		& \rightarrow r = 19.5 + 4.70\\
       		& \rightarrow r = 24.2\\
       		& \rightarrow r \approx 24
    	\end{split}
	\end{equation}
\end{itemize}

\part{Statistica Inferenziale}

\chapter{Statistiche e Stimatori}

\section{Statistica}
Definiamo una \textbf{popolazione} come una variabile aleatoria $X \sim F(\theta)$ di supporto $D_x$ e un \textbf{campione} $X_1, \dots, X_n$ i.i.d. come una conformazione aleatoria. Nota bene che il campione, non ancora osservato, differisce da $x_1, \dots, x_n$ che invece rappresentano i valori osservati.\\
Una \textbf{statistica} o \textbf{stimatore}, non è altro che una funzione $t:D_x^n \rightarrow \mathbb{R}$ dove presi $n$ valori del campione produce una stima $\hat{\tau} = t(X_1, \dots, X_n)$. Ovvero un algoritmo che applico, il cui input è il mio campione e l'output è la stima di qualcosa che non conosco. Così la mia statistica mi permette di conoscere $F$:
\begin{itemize}
    \item \textbf{Statistica non parametrica} $\rightarrow F$ è completamente sconosciuta e non si fanno ipotesi sulla forma specifica della distribuzione sottostante
    \item \textbf{Statistica parametrica} $\rightarrow F$ è parzialmente sconosciuta e si assume che i valori siano generati da una particolare famiglia di distribuzioni
\end{itemize}
Quindi, data una variabile aleatoria $T = t(X_1, \dots, X_n)$ e dato che voglio stimare $\tau(\theta)$, cerco una stima $\hat{\tau} = t(X_1, \dots, X_n)$ tale che $\hat{\tau} \approx \tau(\theta)$.\\
Dunque, una statistica è una variabile aleatoria che è semplicemente una funzione dei dati di un campione, e una qualunque statistica il cui scopo sia quello di dare una stima di un parametro $\theta$ si dice stimatore di $\theta$.
\subsection{Esempio}
Mi ritrovo con una popolazione $X \sim B(p)$ e voglio stimare $\Theta = p$. Ipotizzo che $t(X_1, \dots, X_n) = \frac{1}{n} \sum_{i=1}^n X_i = \overline{X}$ sia una stima accettabile e lo verifico risolvendo il seguente
\begin{equation}
    \begin{split}
        \mathbb{E}(\overline{X}) & = \mathbb{E} \left( \frac{1}{n} \sum_{i=1}^n X_i \right)\\
        & = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X_i)\\
        & \rightarrow \text{ le } X_i \text{ appartengono allo stesso campione, quindi hanno la stessa distribuzione di } X\\
        & = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X)\\
        & \rightarrow \mathbb{E}(X) = p \text{ perché lavoriamo con una variabile aleatoria bernoulliana}\\
        & = \frac{1}{n} \sum_{i=1}^n p\\
        & = \frac{1}{n} np\\
        & = p
    \end{split}
\end{equation}
Però cosa significa? Significa che per esempio farò un esperimento bernoulliano e otterrò certi risultati i quali, se faccio la media campionaria di quei valori, oscilleranno attorno a $p$.\\
In realtà capita spesso di non voler stimare direttamente $\theta$, bensì un valore dipendente da esso come la varianza e il valore atteso. Quando approssimo voglio valutare la bontà della mia approssimazione secondo due metriche: il \textbf{bias} (o \textbf{distorsione}) e l'\textbf{errore quadratico medio}, noto come \textbf{MSE}.

\section{Distorsione}
Se ho $\theta$ e voglio stimarlo, allora deve essere che $\tau(\theta) = \theta$ e ricordiamo che $T = t(X_1, \dots, X_n)$ è una variabile aleatoria: dunque per farmi prediligere una variabile aleatoria anziché un'altra, devo avere $\mathbb{E}(T) = \tau(\theta)$. In questo caso quindi diciamo che $t$, la mia statistica, è uno stimatore \textbf{non distorto}, \textbf{non deviato} o \textbf{unbiased} di $\tau(\theta)$.
\subsection{Esempio 1}
Definisco con $X \sim E(\lambda)$ il numero di minuti attesi in coda. Sapendo che il parametro della mia distribuzione esponenziale è $\lambda$, suppongo di voler stimare il tempo medio di attesa $\mathbb{E}(X) = \frac{1}{\lambda} = \tau(\lambda)$ e per farlo provo ad usare la media campionaria $\overline{X}$
\begin{equation}
    \begin{split}
        \mathbb{E}(\overline{X}) & = \mathbb{E} \left( \frac{1}{n} \sum_{i=1}^n X_i \right)\\
        & = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X_i)\\
        & = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(X)\\
        & = \mathbb{E}(X)\\
        & = \frac{1}{\lambda}
    \end{split}
\end{equation}
Dunque la media campionaria $\overline{X}$ è non deviata in questo caso, ma non solo: essa è sempre non deviata rispetto a $\mathbb{E}(X)$, il valore atteso della popolazione.
\subsection{Esempio 2}
Cambiando i dati all'esempio precedente, ipotizziamo che $X \sim B(n,p)$, dove conosco $n$ ma ignoro $p$. Dunque $\theta = p$, $\tau(\theta) = \tau(p) = p$ e $\mathbb{E}(\overline{X}) = \mathbb{E}(X) = np$.\\
Se io adesso dovessi ipotizzare che non vale più $np$ ma $mp$ tale che $Y_{n,m} = X_{1,1}, \dots, X_{1,m}, \dots, X_{n,m}$, allora
\begin{equation}
	\begin{split}
		\overline{X} & = \overline{Y}\\
		& = \frac{1}{nm} \sum_{i=1}^{nm} Y_i\\
		& = \frac{1}{nm} \sum_{i=1}^{n} X_i\\
		& = \frac{1}{nm} \sum_{i=1}^{n} X_i\\
		& = \frac{1}{m} \overline{X}
	\end{split}
\end{equation}
Dunque sapendo che ora vale $\mathbb{E}(\overline{X}) = mp$ dobbiamo vedere se $T = \frac{1}{m} \overline{X}$ è deviato rispetto a $p$
\begin{equation}
	\begin{split}
		\mathbb{E}(\overline{X}) & = mp\\
		& \rightarrow \frac{1}{m} \mathbb{E}(\overline{X}) = p\\
		& \rightarrow \mathbb{E} \left(\frac{1}{m} \overline{X} \right) = p
	\end{split}
\end{equation}
Quindi $T = \frac{1}{m} \overline{X}$ non è deviato rispetto a $p$.

\subsection{Esempio 3}
Solito esempio, ma ora voglio stimare $\tau(\theta) = \lambda$. Inizio col dire che $\overline{X}$ è uno stimatore non deviato di $\mathbb{E}(X) = \frac{1}{\lambda}$ (infatti $\mathbb{E}(X) = \mathbb{E}(\overline{X})$).\\
Dunque inizio dicendo che $\mathbb{E}(\overline{X}) = \frac{1}{\lambda}$ e che posso semplificarlo con $\lambda = \frac{1}{\mathbb{E}(\overline{X})}$. Tuttavia dopo questo mi devo fermare perché non posso più compiere trasformazioni lineari.

\subsection{Considerazione 1}
Se noi abbiamo uno stimatore non deviato per una certa quantità, calcolo la varianza di quello stimatore, per esempio quello della media campionaria, e osservo cosa succede
\begin{equation}
    \begin{split}
        Var(\overline{X}) & = Var \left( \frac{1}{n} \sum_{i=1}^n X_i \right)\\
        & = \frac{1}{n^2} Var \left( \sum_{i=1}^n X_i \right)\\
        & \rightarrow \text{ sono v.a. indipendenti quindi } Var \left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i)\\
        & = \frac{1}{n^2} \sum_{i=1}^n Var(X_i)\\
        & = \frac{1}{n^2} \sum_{i=1}^n Var(X)\\
        & = \frac{1}{n^2} \sum_{i=1}^n \sigma^2\\
        & = \frac{\sigma^2}{n}
    \end{split}
\end{equation}

\subsection{Considerazione 2}
Ci sono casi particolari in cui si può risolvere il problema della stima in modo esatto, per esempio con $\theta = \mathbb{N}$ e $X \sim U((\theta - \frac{1}{2}, \theta + \frac{1}{2}))$. A questo punto mi basta conoscere un valore della popolazione per capire il valore di $\theta$; infatti se $x_1 = 3.1$ allora $\theta = 3$ in quanto numero naturale più vicino a 3.1 (ricordo che avevamo definito $\theta = \mathbb{N}$). Dunque, in questo contesto, il mio stimatore, distorto in quanto operazione non lineare, risulta essere $\lfloor X_i \rfloor$.

\section{Mean Square Error}
Il \textit{Mean Square Error (MSE)} o \textit{scarto quadratico medio} è definito partendo da $T = t(X_1, \dots, X_2)$ come
\begin{equation}
	MSE_{\tau(\theta)}(T) = \mathbb{E} \left( (T - \tau(\theta))^2 \right)
\end{equation}
E più precisamente
\begin{equation}
    \begin{split}
        MSE_{\tau(\theta)}(T) & = \mathbb{E} \left( (T - \tau(\theta))^2 \right)\\
        & = \mathbb{E} \left( (T - \mathbb{E}(T) + \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = \mathbb{E} \left( (\underline{T - \mathbb{E}(T)} + \underline{\mathbb{E}(T) - \tau(\theta)})^2 \right)\\
        & = \mathbb{E} \left( (T - \mathbb{E}(T))^2 + 2(T - \mathbb{E}(T))(\mathbb{E}(T) - \tau(\theta)) + (\mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = \mathbb{E} \left( (T - \mathbb{E}(T))^2 \right) + \mathbb{E}(2(T - \mathbb{E}(T))(\mathbb{E}(T) - \tau(\theta))) + \mathbb{E} \left( \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = Var(T) + \mathbb{E}(2(T - \mathbb{E}(T))(\mathbb{E}(T) - \tau(\theta))) + \mathbb{E} \left( \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = Var(T) + \mathbb{E}(T - \mathbb{E}(T))\mathbb{E}(2(\mathbb{E}(T) - \tau(\theta))) + \mathbb{E} \left( \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = Var(T) + (\mathbb{E}(T) - \mathbb{E}(\mathbb{E}(T))) \mathbb{E}(2(\mathbb{E}(T) - \tau(\theta))) + \mathbb{E} \left( \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = Var(T) + (\mathbb{E}(T) - \mathbb{E}(T)) \mathbb{E}(2(\mathbb{E}(T) - \tau(\theta))) + \mathbb{E} \left( \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = Var(T) + 0 \cdot \mathbb{E}(2(\mathbb{E}(T) - \tau(\theta))) + \mathbb{E} \left( \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = Var(T) + \mathbb{E} \left( \mathbb{E}(T) - \tau(\theta))^2 \right)\\
        & = Var(T) + (\mathbb{E}(T) - \tau(\theta))^2
    \end{split}
\end{equation}

\section{Bias}
Definisco il bias come
\begin{equation}
	b_{\tau(\theta)}(T) = \mathbb{E}(T) - \tau(\theta)
\end{equation}
E dunque posso riscrivere l'errore quadratico medio come
\begin{equation}
	MSE_{\tau(\theta)}(T) = Var(T) + b_{\tau(\theta)}(T)^2
\end{equation}
Infine posso dire che se lo stimatore $T$ è non deviato per $\tau(\theta)$, allora
\begin{equation}
	b_{\tau(\theta)}(T) = 0
\end{equation}
e dunque
\begin{equation}
	\mathbb{E}(T) = \tau(\theta)
\end{equation}
e
\begin{equation}
	MSE_{\tau(\theta)}(T) = Var(T)
\end{equation}

\section{Consistenza in media quadratica}
Uno stimatore $T$ si dice \textbf{consistente in media quadratica} per $\tau(\theta)$ quando
\begin{equation}
	\lim_{n\rightarrow\infty} MSE_{\tau(\theta)}(T) = 0\ \ \ \forall \rightarrow T_n\ MSE_{\tau(\theta)}(T) \overset{n\rightarrow\infty}{\rightarrow} 0
\end{equation}
Uno stimatore $T$ si dice invece \textbf{debolmente consistente} rispetto a $\tau(\theta)$ se
\begin{equation}
	\lim_{n\rightarrow\infty} P(\tau(\theta) - \varepsilon \leq T_n \leq \tau(\theta) + \varepsilon) = 1\ \ \ \forall \varepsilon > 0
\end{equation}
\subsection{Considerazione}
Posso facilmente dimostrare che se $T$ è consistente in media quadratica per $\tau(\theta)$, allora $T$ è debolmente consistente per $\tau(\theta)$
\begin{equation}
    \begin{split}
        P(\tau(\theta) - \varepsilon \leq T_n \leq \tau(\theta) + \varepsilon) & = P(-\varepsilon \leq T_n - \tau(\theta) \leq \varepsilon)\\
        & = P \left( \mid T_n - \tau(\theta) \mid \leq \varepsilon \right)\\
        & = P \left( (T_n - \tau(\theta))^2 \leq \varepsilon^2 \right)\\
        & = 1 - P \left( (T_n - \tau(\theta))^2 > \varepsilon^2 \right)\\
        & \rightarrow \text{ applico Markov } P(X > a) < \frac{\mathbb{E}(X)}{a}\\
        & \rightarrow 1 - P \left( (T_n - \tau(\theta))^2 > \varepsilon^2 \right) > 1 - \frac{\mathbb{E} \left( (T_n - \tau(\theta))^2 \right)}{\varepsilon^2}\\
        & = 1 - \frac{MSE_{\tau(\theta)}(T)}{\varepsilon^2} \overset{n\rightarrow\infty}{\rightarrow} 1
    \end{split}
\end{equation}
Dunque ho dimostrato che avendo $a > b$ e $b \rightarrow 1$, allora per forza anche $a \rightarrow 1$. Se non ho capito il senso, ricordo che la probabilità massima è 1, quindi se $a$ supera $b$ e $b$ tende ad 1, $a$ comunque oltre 1 non va e tenderà anch'esso a 1.
\subsection{Esempio 1}
Ipotizzo una popolazione $X \sim B(p)$ e voglio conoscere il parametro $\theta = p$, dunque $\tau(\theta) = \tau(p) = p$. Siamo in possesso di un campione $X_1, X_2, X_3$ e lo vogliamo usare nello stimatore $t(X_1, X_2, X_3) = \frac{X1 + X_2 + X_3}{5}$: il nostro stimatore è corretto per $p$?
\begin{equation}
	\begin{split}
		\mathbb{E}(T) & = \mathbb{E} \left( \frac{X_1 + X_2 + X_3}{5} \right)\\
		& = \frac{1}{5} \mathbb{E}(X_1 + X_2 + X_3)\\
		& = \frac{3}{5}p
	\end{split}
\end{equation}
Ovviamente è distorto (è presente bias) perché dovrei avere una media ma faccio diviso 5. Qual è il suo bias?
\begin{equation}
    \begin{split}
        b_p(T) & = \mathbb{E} - p\\
        & = \frac{3}{5}p - p\\
        & = -\frac{2}{5}p
    \end{split}
\end{equation}
Qual è invece il suo MSE?
\begin{equation}
    \begin{split}
        MSE_p(T) & = Var(T) + b_p(T)^2\\
        & = Var \left( \frac{1}{5} (X_1 + X_2 + X_3) \right) + \frac{4}{25}p^2\\
        & = \frac{3}{25} Var(X) + \frac{4}{25}p^2\\
        & \rightarrow \text{ ci troviamo in un contesto bernoulliano}\\
        & = \frac{3}{25}p(1-p) + \frac{4}{25}p^2\\
    \end{split}
\end{equation}
\subsection{Esempio 2}
Quanto può saltare un atleta sapendo in media quanto salta e che minimo salta 0 e massimo $\theta$? Riassumendo, considero che $X \sim U((0, \theta))$ e $\tau(\theta) = \theta$ dunque provo a calcolare
\begin{equation}
	\begin{split}
		\mathbb{E}(\overline{X}) & = \mathbb{E}(X)\\
		& = \frac{\theta}{2}
	\end{split}
\end{equation}
In questo caso la media campionaria non è un'idea eccelsa dato che mi trova $\frac{\theta}{2}$, dunque $\overline{X}$ è deviato per $\theta$, tuttavia
\begin{equation}
	\begin{split}
		2 \mathbb{E}(\overline{X}) & = 2 \mathbb{E}(X)\\
		& = 2 \frac{\theta}{2}\\
		& = \theta
	\end{split}
\end{equation}
Dunque $T = 2 \overline{X}$ risulta non deviato per $\theta$. E' consistente in media quadratica per $\theta$?
\begin{equation}
    \begin{split}
        MSE_p(T) & = Var(T)\\
        & = Var(2 \overline{X})\\
        & = 4 Var(\overline{X})\\
        & = \frac{4}{n} Var(X)\\
        & \rightarrow \text{ in un contesto uniforme } Var(X) = \frac{(b-a)^2}{12}\\
        & = \frac{4}{n} \frac{\theta^2}{12}\\
        & = \frac{\theta^2}{3n} \overset{n\rightarrow\infty}{\rightarrow} 0
    \end{split}
\end{equation}
Dunque $T$ è consistente in media quadratica per $\theta$. Ma se volessi considerare $T' = \max:{1 \leq i \leq n}\ X_i$?
Inizio con
\begin{equation}
    \begin{split}
        F_{T'}(x) & = P(T' \leq x)\\
        & = P(\max X-i \leq x)\\
        & = P(\forall i\ X_i \leq x)\\
        & = \prod_{i=1}^n P(X_i \leq x)\\
        & = \prod_{i=1}^n F_X(x)\\
        & = F_X(x)^n\\
        & = \left( \sum_{i=1}^{\lfloor x\rfloor} P(X=i) \right)^n\\
        & \rightarrow \text{ in un contesto uniforme } P(X=i) = \frac{1}{n}\\
        & = \left( \sum_{i=1}^{\lfloor x\rfloor} \frac{1}{n} \right)^n\\
        & = \left( \frac{\lfloor x\rfloor}{n} \right)^n\\
        & = \left( \frac{\lfloor x\rfloor}{\theta} \right)^n
    \end{split}
\end{equation}
A questo punto calcolo
\begin{equation}
    \begin{split}
        \mathbb{E}(T') & = \int_0^\theta x f_{T'}(x)\ dx\\
        & = \int_0^\theta x \frac{d}{dx}F_{T'}(x)\ dx\\
        & = \int_0^\theta x \frac{d}{dx}\left( \frac{\lfloor x\rfloor}{\theta} \right)^n\ dx\\
        & = \int_0^\theta x n \left( \frac{\lfloor x\rfloor}{\theta} \right)^{n-1} \frac{1}{\theta}\ dx\\
        & = \int_0^\theta x n\frac{x^{n-1}}{\theta^n}\ dx\\
        & = \frac{n}{\theta^n} \int_0^\theta x \cdot x^{n-1}\ dx\\
        & = \frac{n}{\theta^n} \int_0^\theta x^n\ dx\\
        & = \frac{n}{\theta^n} \frac{x^{n+1}}{n+1} \int_0^\theta\\
        & = \frac{n}{\theta^n} \frac{\theta^{n+1}}{n+1}\\
        & = \frac{n}{n+1} \theta
    \end{split}
\end{equation}
Vediamo quindi che $\mathbb{E}(T') = \frac{n}{n+1} \theta$ che è deviato per $\theta$. Per risolvere si potrebbe considerare $T'' = \frac{n+1}{n} T'$ che risulta
\begin{equation}
	\begin{split}
		\mathbb{E}(T'') & = \frac{n+1}{n} \mathbb{E}(T')\\
		& = \frac{n+1}{n} \frac{n}{n+1} \theta\\
		& = \theta
	\end{split}
\end{equation}
E dunque non deviato per $\theta$. Il suo MSE?
\begin{equation}
    \begin{split}
        MSE_p(T'') & = Var(T'')\\
        & = Var \left( \frac{n+1}{n} \max_i X_i \right)\\
        & = \left( \frac{n+1}{n} \right)^2\cdot Var \left( \max_i X_i \right)\\
        & = \left( \frac{n+1}{n} \right)^2 \cdot Var(T')\\
        & = \left( \frac{n+1}{n} \right)^2 \cdot \left( \mathbb{E} \left( {T'}^2 \right) - \mathbb{E}(T')^2 \right)\\
        & = \left( \frac{n+1}{n} \right)^2 \cdot \left( \left( \int_0^\theta x^2 f_{T'}(x)\ dx \right) - \mathbb{E}(T')^2 \right)
    \end{split}
\end{equation}
Oppure potevo anche fare
\begin{equation}
    \begin{split}
        MSE_p(T'') & = Var(T'')\\
        & = \mathbb{E} \left( {T''}^2 \right) - \mathbb{E}(T'')^2\\
        & = \mathbb{E} \left( {T''}^2 \right) - \theta^2\\
        & = \mathbb{E} \left( \left( \frac{n+1}{n} T' \right)^2 \right) - \theta^2\\
        & = \left( \frac{n+1}{n} \right)^2 \mathbb{E}(T^2) - \theta^2\\
        & = \left( \frac{n+1}{n} \right)^2 \left( \int_0^\theta x^2 f_{T'}(x)\ dx \right) - \theta^2\\
        & = \left( \frac{n+1}{n} \right)^2 \left( \int_0^\theta x^2 n\frac{x^{n-1}}{\theta^n}\ dx \right) - \theta^2\\
        & = \left( \frac{n+1}{n} \right)^2 \left( \frac{n}{\theta^2} \int_0^\theta x^{n+1}\ dx \right) - \theta^2\\
        & = \left( \frac{n+1}{n} \right)^2 \left( \frac{n}{\theta^2} \frac{x^{n+2}}{n+2} \int_0^\theta \right) - \theta^2\\
        & = \left( \frac{n+1}{n} \right)^2 \left( \frac{n}{\theta^n} \frac{\theta^{n+2}}{n+2} \right) - \theta^2\\
        & = \left( \frac{n+1}{n} \right)^2 \left( \frac{n}{n+2}\theta^2 \right) - \theta^2\\
        & = \frac{(n+1)^2}{n^2} \cdot \frac{n}{n+2}\theta^2 - \theta^2\\
        & = \frac{(n+1)^2}{n(n+2)}\theta^2 - \theta^2\\
        & = \theta^2 \left( \frac{(n+1)^2}{n(n+2)} - 1 \right)\\
        & = \theta^2 \left( \frac{n^2 + 2n + 1 - n^2 - 2n}{n(n+2)} \right)\\
        & = \theta^2 \left( \frac{1}{n(n+2)} - 1 \right)\\
        & = \frac{\theta^2}{n(n+2)}
    \end{split}
\end{equation}
Dunque per riassumere:
\begin{itemize}
    \item $MSE_\theta(T'') = MSE_\theta \left( \frac{n+1}{n} \max\ X_i \right) = \frac{\theta^2}{n(n+2)}$
    \item $MSE_\theta(T) = MSE_\theta(\overline{X}) = \frac{\theta^2}{3n}$
    \item $\mathbb{E}(T) = \mathbb{E}(T'') = \theta$
\end{itemize}
Per quali valori di $n$, $MSE_\theta(T'') \leq MSE_\theta(T)$?
\begin{equation}
    \begin{split}
        & \frac{\theta^2}{n(n+2)} \leq \frac{\theta^2}{2n}\\
        & \rightarrow \frac{1}{n(n+2)} \leq \frac{1}{2n}\\
        & \rightarrow n(n+2) \geq 3n\\
        & \rightarrow n \geq 1
    \end{split}
\end{equation}
Ma $n$ è sempre $ \geq 1$, dunque ho sempre almeno un campione e questo implica che $MSE_\theta(T'')$ è sempre più basso di $MSE_\theta(T)$.
\subsection{Esempio 3}
Ipotizzo $X \sim N(\mu, \sigma)$ e immagino di avere un campione $X_1, X_2$. Mi piacerebbe conoscere $\sigma$, dunque $\theta = \sigma,\ \tau(\theta) = \tau(\sigma) = \sigma^2$ (stimare in maniera non distorta la deviazione standard non mi è facile dunque provo a stimare la varianza). Sono in possesso dello stimatore $T = X_1^2 - X_1X_2$, è buono?
\begin{equation}
    \begin{split}
        \mathbb{E}(T) & = \mathbb{E}(X_1^2 - X_1X_2)\\
        & = \mathbb{E}(X_1^2) - \mathbb{E}(X_1X_2)\\
        & = \mathbb{E}(X^2) - \mathbb{E}(X)\mathbb{E}(X)\\
        & = \mathbb{E}(X^2) - \mathbb{E}(X)^2\\
        & = \sigma^2
    \end{split}
\end{equation}
\subsection{Esempio 4}
Sono in possesso di un campione $X_1, \dots, X_n$ estratto da una popolazione $X \sim F$. So che $\mathbb{E}(X) = \mu$ e che $Var(F) = \sigma^2$, dunque $T_n = \frac{1}{3} \sum_{i=1}^n X_i$ è non deviato per $\mu$?
\begin{equation}
    \begin{split}
        \mathbb{E}(T) & = \mathbb{E} \left( \frac{1}{3} \sum_{i=1}^n X_i \right)\\
        & = \frac{1}{3} \sum_{i=1}^n \mathbb{E}(X_i)\\
        & = \frac{n}{3} \mu\\
        & = \mu \iff n = 3
    \end{split}
\end{equation}
Dunque osservo
\begin{equation}
    \begin{split}
        MSE_\mu(T_n) & = Var(T_n) + b_\mu(T_n)^2\\
        & = Var \left( \frac{1}{3} \sum_{i=1}^n X_i \right) + \left( \mu - \mathbb{E}(T_n) \right)\\
        & = \left( \frac{1}{9} \sum_{i=1}^n Var(X_i) \right)+ \left( \mu - \frac{n}{3}\mu \right)\\
        & = \frac{n}{9}\sigma^2 + \left( 1 - \frac{n}{3} \right)^2\mu^2
    \end{split}
\end{equation}
\subsection{Esempio 5}
Considero un campione $X_1, \dots, X_n$ di $X \sim N(\mu, \sigma)$. Inoltre $\theta = \mu$ e $\tau(\theta) = \mathbb{E}(X) = \mu$.\\
Se prendo $T_1 = \overline{X}$ questa stima è non deviata. Se invece prendo $T_2 = X_3$?
\begin{equation}
	\begin{split}
		\mathbb{E}(T_2) & = \mathbb{E}(X_3)\\
		& = \mathbb{E}(X)\\
		& = \mu
	\end{split}
\end{equation}
Anch'esso non deviato, ma noto che i due MSE sono diversi. Questo l'MSE per $T_1$
\begin{equation}
	\begin{split}
		MSE_\mu(T_1) & = Var(T_1)\\
		& = \frac{\sigma^2}{n}
	\end{split}
\end{equation}
Questo invece l'MSE per $T_2$
\begin{equation}
	\begin{split}
		MSE_\mu(T_2) & = Var(T_2)\\
		& = \sigma^2
	\end{split}
\end{equation}
Proviamo ora invece con $T_3 = \sum_{i=1}^n \lambda_i X_i$ sapendo che $\forall i\ \lambda_i > 0$ e che $\sum_i \lambda_i = 1$. Esso è distorto?
\begin{equation}
	\begin{split}
		\mathbb{E}(T_3) & = \sum_{i=1}^n \lambda_i \mathbb{E}(X_i)\\
		& = \sum_{i=1}^n \lambda_i \mu\\
		& = 1 \cdot \mu\\
		& = \mu
	\end{split}
\end{equation}
Non distorto, e il suo MSE?
\begin{equation}
    \begin{split}
        MSE_\mu(T_3) & = Var(T_3)\\
        & = Var \left( \sum_{i=1}^n \lambda_i X_i \right)\\
        & = \sum_{i=1}^n Var(\lambda_i X_i)\\
        & = \sum_{i=1}^n \lambda_i^2 Var(X_i)\\
        & = \sigma^2 \sum_{i=1}^n \lambda_i^2\\
    \end{split}
\end{equation}
\subsection{Esempio 6}
Prendo $X_1$, $X_2$ oltre che a $T_1 = \frac{1}{2}(X_1 + X_2)$, ossia la media, e $T_2 = \lambda_1 X_1 + (1 - \lambda_1)X_2$. Dato $\lambda_1^2 + (1-\lambda)^2$, qual è il $\lambda_1$ che mi rende l'espressione il più piccolo possibile?
\begin{equation}
    \begin{split}
        \min_{\text{val}}\ \text{expression} & = \min_{\lambda_1} \lambda_1^2 + (1-\lambda)^2\\
        & \rightarrow \text{ derivo}\\
        & = 2\lambda_1 - 2(1-\lambda_1)\\
        & = 2\lambda_1 - 2 + 2\lambda_1\\
        & = 4\lambda_1 - 2\\
        & \rightarrow \lambda_1=\frac{1}{2},\ \ \ \lambda_2 = 1-\lambda_1 = \frac{1}{2}
    \end{split}
\end{equation}

\subsection{Esempio 7}
Il TCL ci dice che prese $X_1, \dots, X_n$ i.i.d., allora $\sum_{i=1}^n X_i\ \dot\sim\ N \left( n\mu, \sqrt{n}\sigma \right)$. Dunque
\begin{equation}
	\begin{split}
		\overline{X} & = \frac{1}{n} \sum_{i=1}^n X_i\ \dot\sim\ N(\mu_{\overline{X}}, \sigma_{\overline{X}})\\
		& = N \left( \mu, \frac{\sigma}{\sqrt{n}} \right)
	\end{split}
\end{equation}
E guarda caso $\mathbb{E}(\overline{X}) = \mu$ e $Var(\overline{X}) = \frac{\sigma^2}{n}$.

\section{Legge dei Grandi numeri}

\subsection{Forma forte}
All'aumentare di $n$, la media campionaria è equivalente al valore atteso della popolazione
\begin{equation}
	P \left(\lim_{n\to\infty} \overline{X}_n = \mathbb{E}(X) \right) = 1
\end{equation}
\subsection{Forma debole}
La probabilità che la media si discosti dal valore atteso, per al più di $\varepsilon$, tende a 0 all'aumentare di $n$
\begin{equation}
	\lim_{n\to\infty} P \left(\mid \overline{X}_n-\mathbb{E}(X) \mid > \varepsilon \right) = 0,\ \ \ \forall\varepsilon > 0
\end{equation}
\subsection{Esempio 1}
Ipotizzo un campione $X_1, \dots, X_n$ estratto da una popolazione $X$ e ipotizzo di voler stimare $\tau(\theta) = Var(X)$.\\
Lo stimatore $s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2$ è non distorto per $Var(X)$, lo dimostro ricordandomi che
\begin{equation}
    \begin{split}
        \sum_{i=1}^n (x_i - \overline{x})^2 & = \sum_{i=1}^n (x_i^2 - 2\overline{x}x_i + \overline{x}^2)\\
        & = \sum_{i=1}^n x_i^2 - 2\overline{x} \sum_{i=1}^n x_i + n\overline{x}^2\\
        & = \sum_{i=1}^n x_i^2 - 2n\overline{x}^2 + n\overline{x}^2\\
        & = \sum_{i=1}^n x_i^2 -  n\overline{x}^2
    \end{split}
\end{equation}
Noto che
\begin{equation}
	\begin{split}
		(n-1)s^2 & = \sum_{i=1}^n (x_i - \overline{x})^2\\
		& = \sum_{i=1}^n x_i^2 - n\overline{x}^2
	\end{split}
\end{equation}
Quindi tramite metodo plug-in
\begin{equation}
    \begin{split}
        \mathbb{E} \left( (n-1)s^2 \right) & = \mathbb{E} \left( \sum_{i=1} x_i^2 -  n\overline{x}^2 \right)\\
        & = \sum_{i=1}^n \mathbb{E} \left( X_i^2 \right) - n\mathbb{E} \left( \overline{X}^2 \right)\\
        & = n \left( \mathbb{E} \left( X^2 \right) - \mathbb{E} \left( \overline{X}^2 \right) \right)\\
        & = n \left( Var(X) + \mathbb{E}(X)^2 - Var(\overline{X}) - \mathbb{E}(\overline{X})^2 \right)\\
        & = n \left( \sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2 \right)\\
        & = n\sigma^2 - \sigma^2\\
        & = (n-1)\sigma^2
    \end{split}
\end{equation}
Dunque
\begin{equation}
	\mathbb{E} \left( (n-1)s^2 \right) \implies (n-1) \mathbb{E} \left( s^2 \right) = (n-1) \sigma^2 \implies \mathbb{E} \left( s^2 \right) = \sigma^2
\end{equation}
Risulta importante sottolineare che il metodo plug-in non mi dice se il risultato è deviato o non deviato, ma solo che è ragionevole.
\subsection{Esempio 2}
Ipotizzo di avere un campione $X_1, \dots, X_n$ tale che $X \sim N(d,2)$. So che $\theta = d$ e la mia statistica è $\overline{X}_n$. Inoltre sono a conoscenza del fatto che $\mathbb{E} \left( \overline{X}_n \right) = d$, $Var\left( \overline{X}_n \right) = \frac{4}{n}$ e $Z = N(0,1)$. Inoltre $P \left( \mid \overline{X}_n - d \mid \leq 0.5 \right) \geq 0.95$, ma per quale $n$? Oltre quale soglia?
\begin{equation}
    \begin{split}
        P \left( \mid \overline{X}_n - d \mid \leq 0.5 \right) & = P \left( \frac{\mid \overline{X}_n - d \mid}{\frac{2}{\sqrt{n}}} \leq \frac{0.5}{{\frac{2}{\sqrt{n}}}} \right)\\
        & = P \left( \middle| \frac{\overline{X}_n - d}{\frac{2}{\sqrt{n}}} \middle| \leq \frac{\sqrt{n}}{4} \right)\\
        & \rightarrow TCL\\
        & \approx P \left( \mid Z \mid \leq \frac{\sqrt{n}}{4} \right)\\
        & = \Phi \left( \frac{\sqrt{n}}{4} \right) - \Phi \left( - \frac{\sqrt{n}}{4} \right)\\
        & = \Phi \left( \frac{\sqrt{n}}{4} \right) - 1 + \Phi \left( \frac{\sqrt{n}}{4} \right)\\
        & = 2 \Phi \left( \frac{\sqrt{n}}{4} \right) - 1\\
        & \rightarrow 2 \Phi \left( \frac{\sqrt{n}}{4} \right) - 1 \geq 0.95\\
        & \rightarrow \Phi \left( \frac{\sqrt{n}}{4} \right) \geq \frac{1.95}{2}\\
        & \rightarrow \Phi \left( \frac{\sqrt{n}}{4} \right) \geq 0.975\\
        & \rightarrow \Phi^{-1} \left( \Phi \left( \frac{\sqrt{n}}{4} \right) \right) \geq \Phi^{-1}(0.975)\\
        & \rightarrow \frac{\sqrt{n}}{4} \geq 1.96\\
        & \rightarrow n \geq (4 \cdot 1.96)^2\\
        & \rightarrow n \geq \approx 61.45\\
        & \rightarrow n \geq 62\ :\ P \left( \mid \overline{X_n} - d \mid \leq 0.5 \right) \geq 0.95
    \end{split}
\end{equation}

\section{Stima dell'errore}
Più in generale, la stima dell'errore avviene tramite l'impiego del TCL nel seguente modo
\begin{equation}
    \begin{split}
        P \left( \mid \overline{X}_n - \mu \mid \leq r \right) \geq 1 - \delta & \rightarrow P \left( \frac{\mid \overline{X}_n - \mu \mid}{\frac{\sigma}{\sqrt{n}}} \leq \frac{r}{\frac{\sigma}{\sqrt{n}}} \right)\\
        & = P \left( \middle| \frac{\overline{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \middle| \leq \frac{r}{\sigma}\sqrt{n} \right)\\
        & \approx P \left( \mid Z \mid \leq \frac{r}{\sigma}\sqrt{n} \right)\\
        & = P \left( -\frac{r}{\sigma} \sqrt{n} \leq Z \leq \frac{r}{\sigma}\sqrt{n} \right)\\
        & = \Phi \left( \frac{r}{\sigma}\sqrt{n} \right) - \Phi \left( -\frac{r}{\sigma}\sqrt{n} \right)\\
        & = \Phi \left( \frac{r}{\sigma}\sqrt{n} \right) - 1 + \Phi \left( \frac{r}{\sigma}\sqrt{n} \right)\\
        & = 2 \Phi \left( \frac{r}{\sigma}\sqrt{n} \right) - 1\\
        & \rightarrow 2 \Phi \left( \frac{r}{\sigma}\sqrt{n} \right) - 1 \geq 1 - \delta\\
        & \rightarrow \Phi \left( \frac{r}{\sigma}\sqrt{n} \right) \geq 1 - \frac{\delta}{2}
    \end{split}
\end{equation}
Da qui posso trovare quello che succede con una probabilità di almeno $(1-\delta)$
\begin{equation}
	\delta \geq 2 \left( 1 - \Phi \left( \frac{r}{\sigma}\sqrt{n} \right) \right)
\end{equation}
Oppure posso trovare
\begin{equation}
	n \geq \left( \frac{\sigma}{r} \Phi^{-1}(1-\delta) \right)^2
\end{equation}
In alternativa
\begin{equation}
	r \geq \frac{\sigma}{\sqrt{n}} \Phi^{-1} \left(1 - \frac{\delta}{2} \right)
\end{equation}
In realtà tutti questi conti possono essere svolti diversamente applicando la disuguaglianza di Chebyshev
\begin{equation}
	P \left( \mid X - \mu \mid \geq r \right) \leq \frac{\sigma^2}{r^2}
\end{equation}
e proseguendo così con
\begin{equation}
    \begin{split}
        P \left( \mid X - \mu \mid \geq r \right) \leq \frac{\sigma^2}{r^2} & \rightarrow P \left( \mid X - \mu \mid \geq \varepsilon \right) \leq \frac{\sigma^2}{\varepsilon^2}\\
        & \rightarrow P \left( \mid X - \mu \mid < \varepsilon \right) \implies 1 - P \left( \mid X - \mu \mid \leq \varepsilon \right) \geq 1 - \frac{\sigma^2}{\varepsilon^2}\\
        & \rightarrow Var \left( \overline{X} \right) = \frac{Var(X)}{n}\\
        & \rightarrow P \left( \mid \overline{X} - \mu \mid \leq \varepsilon \right) \geq 1 - \frac{\sigma^2}{n\varepsilon^2} \geq 1 - \delta\\
        & \rightarrow 1 - \frac{\sigma^2}{n\varepsilon^2} \geq 1 - \delta
    \end{split}
\end{equation}
Dunque avrò:
\begin{itemize}
    \item $\delta \geq \frac{\sigma^2}{n\varepsilon^2}$
    \item $n \geq \frac{\sigma^2}{\delta\varepsilon^2}$
    \item $\varepsilon \geq \sqrt{\frac{\sigma^2}{n\delta}}$
\end{itemize}
Questo è possibile siccome sono quantità piccole e posso non preoccuparmi della differenza tra $<$ e $\leq$, dunque
\begin{equation}
	P \left( \mid \overline{X} - \mu \mid \leq \varepsilon \right) \geq 1 - \frac{\sigma^2}{n\varepsilon^2} \longleftrightarrow P \left( \mid \overline{X} - \mu \mid < \varepsilon \right) \geq 1 - \delta
\end{equation}
Infine, per diletto, contronto TLC con Chebyshev e mi domando $\frac{\sigma^2}{\varepsilon^2} \Phi^{-1} \left( 1 - \frac{\delta}{2} \right)^2$ quando è $\leq \frac{\sigma^2}{\delta\varepsilon^2}$?
\begin{equation}
    \begin{split}
        \frac{\sigma^2}{\varepsilon^2} \Phi^{-1} \left( 1 - \frac{\delta}{2} \right)^2 \leq \frac{\sigma^2}{\delta\varepsilon^2} & \rightarrow \Phi^{-1} \left( 1 - \frac{\delta}{2} \right)^2 \leq \frac{1}{\delta}
    \end{split}
\end{equation}
Preferire il metodo del TCL è spesso una scelta più vantaggiosa quando si ha ragione di credere che la media campionaria sia approssimativamente distribuita in modo normale. Questo perché il TCL offre non solo un limite superiore sulla probabilità di discostamento della variabile aleatoria dalla media, ma fornisce anche una approssimazione più precisa della distribuzione della media campionaria. D'altra parte, il metodo con la disuguaglianza di Chebyshev, offre solo limiti superiori basati sulla deviazione standard, senza specificare la forma precisa della distribuzione. Di conseguenza, quando la normalità della media campionaria è plausibile, il TCL è preferibile per ottenere una descrizione più dettagliata e un'approssimazione più accurata della distribuzione della media campionaria.

\section{Riepilogo stimatori}
\subsection{Distorsione}
Un'importante caratteristica di uno stimatore è la sua non distorsione, che si verifica quando il suo valore atteso è uguale al parametro che si intende stimare. Per garantire l'assenza di distorsione, è fondamentale limitarsi a trasformazioni lineari. Ciò significa che sono consentite solo operazioni di addizione, sottrazione, moltiplicazione per uno scalare, derivazione e integrazione.
\subsection{Stimatore media campionaria}
Se mi ritrovo con $n$ variabili aleatorie $X_1\dots X_n$ i.i.d., allora posso usare la media campionaria come stimatore del valore atteso della popolazione, e valgono le seguenti proprietà:
\begin{itemize}
    \item $\mathbb{E} \left( \overline{X} \right) = \frac{1}{n}\mathbb{E}\left( X_1+\dots+X_n \right) = \frac{1}{n} \left( \mu+\dots+\mu \right) = \mu$
    \item $Var(\overline{X}) = \frac{1}{n^2}Var(X_1 + \dots + X_n) = \frac{1}{n^2}(\sigma^2+\dots+\sigma^2) = \frac{\sigma^2}{n}$
\end{itemize}
\subsection{Stimatore varianza e deviazione standard}
Lo stimatore della varianza è ottenibile tramite il metodo plug-in visto precedentemente, mentre per la deviazione standard
\begin{equation}
	s = \sqrt{s^2} \centernot \implies \mathbb{E}(s) = \sigma
\end{equation}
Questo perché ho eseguito un'operazione non lineare (ho estratto una radice quadrata), quindi non posso dire nulla. Per ottenere uno stimatore non deviato della deviazione standard, esso dipende dal modello seguito dalla popolazione.

\chapter{Processo di Poisson}
\section{Il processo di Poisson}
Un processo di Poisson è un modello stocastico usato per descrivere l'occorrenza di eventi rari in un tempo continuo, dove gli eventi sono casuali e indipendenti, ma avvengono ad una frequenza costante. Esso è spesso usato per modellare fenomeni come per esempio il numero di chiamate che un call center riceverà in un certo lasso di tempo, oppure per stimare il traffico ad un server o la frequenza di attacchi informatici.\\
Per modellarlo inizio sapendo che, dato tempo $t=0$ e una variabile aleatoria $N(t)=\text{ numero di eventi in } [0,t)$, al variare di $t$ ottengo una famiglia di variabili aleatorie, ossia i miei \textbf{processi stocastici}. Sapendo che $\lambda$ rappresenta il numero medio di eventi che capitano per unità di tempo, definisco le seguenti regole:
\begin{enumerate}
    \item $N(0) = 0 \longrightarrow$ il processo inizia a tempo 0
    \item Indipendenza per intervalli disgiunti $\longrightarrow$ gli eventi in $t$ sono indipendenti dagli eventi in $t+s$
    \item \[ \lim_{h \rightarrow 0} \frac{P(N(h)=1)}{h} = \lambda \longrightarrow \] la probabilità di un evento in un intervallo di tempo è proporzionale alla lunghezza dell'intervallo, infatti, dato $h$ piccolo avrò $P(N(h)=1) \approx \lambda h$
    \item \[ \lim_{h \rightarrow 0} \frac{P(N(h) \geq 2)}{h} = 0 \longrightarrow \] la probabilità che accadano 2 o più eventi in un piccolo intervallo di tempo è trascurabile, infatti, dato $h$ piccolo avrò $P(N(h) \geq 2) \approx 0$
\end{enumerate}
Definisco infine $N(t) \sim P(\lambda t)$, dove $N(t) = k \in \mathbb{N}$, $n \in \mathbb{N}$ e $n > k$. Inoltre assicuro che ci sia un incremento stazionario degli eventi da 0 a $t$. Dunque posso dichiarare i seguenti eventi mutualmente esclusivi:
\begin{itemize}
    \item \textbf{Evento A}: l'evento che si verifica quando ogni avvenimento cade in un intervallo diverso. Dunque $N(t)=k$ e massimo un elemento ogni sotto-intervallo.
    \item \textbf{Evento B}: l'evento che si verifica tutte le altre volte (tutti gli altri modi per cui $N(t)=k$). Dunque $N(t)=k$ e almeno un intervallo ha 2 o più eventi.
\end{itemize}
Dunque $\{N(t)=k\} = A \cup B,\ \ \ A \cap B = \emptyset$ e quindi
\begin{equation}
	P(N(t)=k) = P(A) + P(B)
\end{equation}
Dove $P(A)$ sono $k$ intervalli a partire dai miei $n$ ($\binom{n}{k}$) e immagino di aver preso i primi $k$ sotto-intervalli: sempre per il fatto che il numero di occorrenze all'interno di due intervalli disgiunti è indipendente, io posso calcolare la probabilità che abbia una e una solo occorrenza nell'ennesimo intervallo, $k$ volte. Questo si traduce in $P(\text{occorrenza nel primo intervallo}) = \lambda \cdot \text{ grandezza intervallo } = \lambda \frac{t}{n}$, che per $k$ intervalli diventa $\left( \lambda \frac{t}{n} \right)^k$. Per quanto riguarda $P(B)$ invece, più $n$ è grande e più i singoli intervalli sono piccoli; per cui da un certo $n$ in avanti la probabilità di avere il numero di avvenimenti entro un particolare intervallo è zero. Ma siccome sto moltiplicando (per 2) è intersezione indipendente (quindi l'evento è risultato dalla produttoria degli eventi), allora $P(B)$ è zero. Allora la probabilità di avere $k$ avvenimenti del mio evento, che non avvengono ognuno in un intervallo diverso, è zero. Infine devo dire che so che posso avere 0 occorrenze, 1 occorrenza o 2 o più occorrenze, tuttavia l'ultimo caso ho visto che ha probabilità zero ($P(B)=0$). Dunque gli unici casi possibili sono 1 occorrenza o nessuna. \textit{"Nessuna occorrenza"} si traduce semplicemente in $1 - (\text{1 occorrenza})$, ossia $\left( 1 - \lambda \frac{t}{n} \right)^{n-k}$.\\
Quanto detto può sembrare confusionario ma è possibile riassumerlo in punti per chiarire:
\begin{itemize}
	\item \textbf{Calcolo di $P(A)$}
	\begin{itemize}
		\item Si considerano $k$ intervalli da un totale di $n$. Questo è rappresentato dalla combinazione binomiale $\binom{n}{k}$, che rappresenta il numero di modi in cui è possibile selezionare $k$ intervalli da un totale di $n$.
		\item Per ogni intervallo, si calcola la probabilità che si verifichi esattamente un evento. Dato che gli eventi sono indipendenti tra intervalli disgiunti, si può calcolare la probabilità di una singola occorrenza nel primo intervallo come $\lambda$ (il tasso medio di arrivo degli eventi) moltiplicato per la dimensione dell'intervallo, che è $\frac{t}{n}$.
		\item Questo si traduce in $P(\text{occorrenza nel primo intervallo}) = \lambda \cdot \text{ grandezza intervallo } = \lambda \frac{t}{n}$, che per $k$ intervalli diventa $\left( \lambda \frac{t}{n} \right)^k$
	\end{itemize}
	\item \textbf{Calcolo di $P(B)$}
	\begin{itemize}
		\item Quando si tratta di $P(B)$, si osserva che all'aumentare di $n$, gli intervalli diventano più piccoli. Infatti considerando un numero crescente di intervalli ($n$ crescente), si distribuisce la probabilità di eventi in modo più uniforme lungo l'asse temporale, riducendo la probabilità di avere due o più eventi simultaneamente in uno stesso sotto-intervallo.
		\item Quando gli intervalli diventano sufficientemente piccoli, la probabilità di avere due o più eventi all'interno di un singolo intervallo diventa trascurabile, e quindi $P(B)$ tende a $0$.
		\item Poiché la probabilità di $P(B)$ è trascurabile all'aumentare del numero di intervalli $n$, allora $P(B)=0$ e l'intersezione $A \cap B$ è essenzialmente nulla.
		\item Dunque gli unici casi possibili sono 1 occorrenza o nessuna, che si traduce in $1 - (\text{1 occorrenza})$, ossia $\left( 1 - \lambda \frac{t}{n} \right)^{n-k}$.
	\end{itemize}
\end{itemize}
Unendo il tutto ottengo una combinazione tra le occorrenze singole e l'assenza di occorrenze, ossia
\begin{equation}
	P(N(t)=k) = \binom{n}{k} \cdot \left( \lambda \frac{t}{n} \right)^k \cdot \left( 1 - \lambda \frac{t}{n} \right)^{n-k}
\end{equation}
Quindi $P(N(t)=k) = \binom{n}{k} \cdot \left( \lambda \frac{t}{n} \right)^k \cdot \left( 1 - \lambda \frac{t}{n} \right)^{n-k}$ dato $\frac{\lambda t}{n} = p \implies np = \lambda t$ ricorda $B(n,p)$ e per essere esaustivi, siccome $N(t) \sim P(\lambda t)$
\begin{equation}
	P(N(t)=k) = e^{-\lambda t} \frac{(\lambda t)^k}{k!} I_{\mathbb{N} \cup \{0\}}(k)
\end{equation}
Dove le specificazioni sono numeri interi non negativi.\\
Ora ipotizzo di sapere che l'evento $X_1$ occorre in $[0,t_1)$, l'evento $X_2$ occorre in $[t_1, t_2)$ e così via: io voglio sapere quanto tempo deve passare tra il tempo in cui occorre un evento e il tempo in cui occorre il successivo
\begin{equation}
    \begin{split}
        P(X_1 > t) & = \text{ P(non si è verificato } X_1 \text{ entro il tempo } t \text{)}\\
        & = P(N(t)=0)\\
        & = e^{-\lambda t} \frac{(\lambda t)^0}{0!}\\
        & = e^{-\lambda t}
    \end{split}
\end{equation}
E dunque
\begin{equation}
    \begin{split}
        F_{X_1}(t) & = P(X_1 \leq t)\\
        & = 1 - P(X_1 > t)\\
        & = 1 - e^{-\lambda t}\\
        \\
        & \implies X_1 \sim E(\lambda)
    \end{split}
\end{equation}
Dove nella prima equazione voglio sapere qual è la probabilità che il primo evento accada oltre un certo tempo $t$, mentre nella seconda il suo duale. Si noti che la seconda equazione è la funzione di ripartizione di una distribuzione esponenziale, infatti l'intertempo degli eventi segue una distribuzione esponenziale.\\
Invece per $X_2$ cosa posso dire?
\begin{equation}
	P(X_2 > t \mid X_1 = s) = P(\text{nessun evento in } [s, s+t) \mid X_1 = s)
\end{equation}
Ma a cosa serve condizionare rispetto ad un evento indipendente? A nulla, infatti
\begin{equation}
	P(\text{nessun evento in } [s, s+t) \mid X_1 = s) = P(N(t)=0) = e^{-\lambda t}
\end{equation}
Questo implica che $X_2 \sim E(\lambda)$ e quindi $\forall i\ X_i \sim E(\lambda)$, dunque $\forall i\ X_i$ i.i.d. e quindi $X_2 \perp X_1$.\\
Dunque possiamo ricapitolare dicendo che
\begin{equation}
	\begin{split}
		P(X_2 > t \mid X_1 = s) & = P(X_2 > t)\\
		& = 1 - F_{X_2}(t)\\
		& = 1 - \left( 1 - e^{- \lambda t} \right)\\
		& = e^{- \lambda t}
	\end{split}
\end{equation}
\subsection{Esempio}
Supponiamo di avere una sorgente radioattiva che emette particelle alfa. Il numero medio di particelle alfa emesse da questa sorgente in 1 minuto è di $\lambda = 0.5$. Vogliamo calcolare la probabilità di non osservare alcuna particella alfa nei prossimi 5 minuti. Poiché il numero di particelle alfa emesse in un dato intervallo di tempo segue una distribuzione di Poisson, possiamo utilizzare un modello esponenziale per descrivere il tempo tra due emissioni successive.\\
Dato che $\lambda$ rappresenta il tasso di emissione medio su 1 minuto, possiamo calcolare il parametro della distribuzione esponenziale come $\frac{1}{\lambda} = \frac{1}{0.5} = 2$. Infine, dato $X \sim E(\lambda)$, che su Python sarebbe \texttt{st.expon(scale=1/lambda)}, possiamo considerare $F_X(x)$ che, sempre su Python, equivale a \texttt{expon.cdf(5)}.

\part{Appendice}

\chapter{Cheatsheet Matematica}
Qui di seguito alcune regole matematiche utili.
\section{Dai dati alla distribuzione}
\begin{center}
	\renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|}
        \hline
        	\textbf{Dati} & \textbf{Modello}\\
        \hline
        	Esito binario, 1 esperimento & Bernoulli\\
        \hline
        	Esito binario, n esperimenti, conteggio dei successi & Binomiale\\
        \hline
        	Spazio equiprobabile discreto & Uniforme Discreta\\
        \hline
        	Esito binario, n esperimenti, conteggio degli insuccessi & Geometrico\\
        \hline
        	Binomiale, $\mathbb{E}(X) = Var(X)$ & Poisson\\
        \hline
        	Estrazione di oggetti binari senza reimmissione & Ipergeometrico\\
        \hline
        	Spazio equiprobabile e continuo & Uniforme Continuo\\
        \hline
        	Durata di vita di un fenomeno, $\sigma_X = \mathbb{E}(X)$ & Esponenziale\\
        \hline
        	Variabile casuale continua, media = mediana & Gaussiano (aka Normale)\\
        \hline
        	Gaussiano, $\mu = 0$, $\sigma = 1$ & Normale Standard\\
        \hline
        	Conteggio di eventi con frequenza media variabile nel tempo & Processo di Poisson\\
        \hline
    \end{tabular}
\end{center}
\section{Derivate}
\begin{center}
	\renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|}
        \hline
        	\textbf{Derivata} & \textbf{Risultato}\\
        \hline
        	$[k]'$ & $0$\\
        \hline
        	$[kf(x)]'$ & $kf'(x)$\\
        \hline
        	$[x^n]'$ & $nx^{n-1}$\\
        \hline
        	$[e^x]'$ & $e^x$\\
        \hline
        	$[f(x) \pm g(x)]'$ & $f'(x) \pm g'(x)$\\
        \hline
        	$[f(x) \cdot g(x)]'$ & $f'(x) \cdot g(x) + f(x) \cdot g'(x)$\\
        \hline
        	$\left[ \frac{f(x)}{g(x)} \right]'$ & $\frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{g(x)^2}$\\
        \hline
    \end{tabular}
\end{center}
\section{Integrali}
\begin{center}
	\renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|}
        \hline
        	\textbf{Integrale} & \textbf{Risultato}\\
        \hline
        	$\int k \,dx$ & $kx + c$\\
        \hline
        	$\int kf(x) \,dx$ & $k \int f(x) \,dx$\\
        \hline
        	$\int x^n \,dx$ & $\frac{x^{n+1}}{n+1} + c$\\
        \hline
        	$\int e^x \,dx$ & $e^x + c$\\
        \hline
        	$\int [f(x) \pm g(x)] \,dx$ & $\int f(x) \,dx \pm \int g(x) \,dx$\\
        \hline
        	\multicolumn{2}{|c|}{\textbf{Integrazione per sostituzione}}\\
        \hline
        	\quad $\int f(g(x)) \cdot g'(x) \,dx$ & $\int f(u) \,du$, dove $u = g(x)$\\
        \hline
        	\multicolumn{2}{|c|}{\textbf{Integrazione per parti}}\\
        \hline
        	\quad $\int f'(x) \cdot g(x) \,dx$ & $f(x) \cdot g(x) - \int f(x) \cdot g'(x) \,dx$\\
        \hline
    \end{tabular}
\end{center}

\chapter{Cheatsheet Python}
Serie di appunti Python presi a lezione o durante la correzione di esami passati.

\begin{mintedbox}{python}
%matplotlib inline
import itertools
import graphviz
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as st
import statsmodels.api as sm

from sklearn import tree
from sklearn.preprocessing import LabelEncoder

plt.style.use('fivethirtyeight')
plt.rc('figure', figsize=(10.0, 7.0))
\end{mintedbox}

\section{Alberi di decisione}
\begin{mintedbox}{python}
# Eseguo tutto in ordine

# Preparo i dati
good_guys = heroes.loc[['Superman', 'Flash', 'Batman', 'Wolverine']]
bad_guys = heroes.loc[['Kingpin', 'Galactus', 'Joker', 'Penguin', 'Magneto']]
all_guys = pd.concat([good_guys, bad_guys])
features = ['Hair color', 'Intelligence']
X = all_guys[features]
Y = pd.concat([
    pd.DataFrame(['good guy'] * len(good_guys), index=good_guys.index),
    pd.DataFrame(['bad guy'] * len(bad_guys), index=bad_guys.index)
])
# Y[X['Intelligence'] <= 40]

# Label Encoder
hair_col_encoder = LabelEncoder()
hair_col_encoder.fit(all_guys['Hair color'])
all_guys['Hair color'] = hair_col_encoder.transform(all_guys['Hair color'])
all_guys['Intelligence'] = intelligence_encoder.transform(all_guys['Intelligence'])
X = all_guys[features]

# Classificatore binario
clf = tree.DecisionTreeClassifier()
#clf = tree.DecisionTreeClassifier(criterion='entropy')     # di norma usa 'gini'
clf = clf.fit(X, Y)
predictions = clf.predict([X.loc[name] for name in X.index])

# Per il classificatore binario posso anche fare
model = GaussianNB()
model.fit(X,Y)
model.predict(X)

# Mostrare l'albero graficamente
graphviz.Source(
    tree.export_graphviz(
        clf,
        out_file=None,
        class_names=['bad guy', 'good guy'],
        feature_names=features
    )
)
\end{mintedbox}

\section{Calcolo combinatorio}
\begin{mintedbox}{python}
# Disposizioni con ripetizione (3 oggetti, 2 posti)
list(itertools.product(range(3), repeat=2))

# Disposizioni senza ripetizione (3 oggetti, 2 posti)
list(itertools.permutations(range(3), repeat=2))

# Permutazioni (3 oggetti)
list(itertools.permutations(range(3), repeat=3))

# Combinazioni (4 oggetti, 2 posti)
list(itertools.combinations(range(4), repeat=2))
\end{mintedbox}

\section{Errore nella stima}
\begin{mintedbox}{python}
# Quanto siamo disposti a sbagliare nella stima con una probabilità del 95 percento? Qual è il margine di errore che dobbiamo tollerare?
Z = st.norm()
eps = 0.95
n = len(vals)
sigma = vals.std()
(
    (Z.ppf((1 + eps) / 2) * (n ** 0.5)) / sigma,
    (Z.ppf((1 + eps) / 2) * sigma) / (n ** 0.5)
)
\end{mintedbox}

\section{Frequenze}
\begin{mintedbox}{python}
# Frequenze congiunte relative
pd.crosstab(
    index=df['col'],
    columns=df['col2'],
    margins=True,           # congiunte (default=False)
    normalize=True,         # relative (default=False)
    colnames=[''],
    # dropna=False          se serve
)

# Frequenze assolute
pd.crosstab(
    index=df['col1'],
    columns=['Freq. Assolute'],
    colnames=['']
)

# Frequenze cumulate
(
    pd.crosstab(
        index=df['col1'],
        columns=['Freq. Assolute'],
        normalize=True,
        colnames=['']
    )
).cumsum()

# Frequenze per raggruppamento
pd.crosstab(
    index=pd.cut(
        df['col1'],
        bins=[30,50,80,100,200,500,1000],
        right=False),
    columns=[df['col2']]
)
\end{mintedbox}

\section{Grafici}
\begin{mintedbox}{python}
# Barplot con subplot
# Permette di rappresentare meglio i dati nominali o categorici, calcolare la frequenza o fare df.groupby(['col1', 'col2']).sum()
plt.subplot(1,2,1)
df['val1'].value_counts().plot.bar()
plt.subplot(1,2,2)
df['val2'].plot.bar()
plt.show()

# Barplot da frequenze
freqs.plot.bar(color=['pink', 'blue'], stacked=False)
plt.show()

# Boxplot
# Mostra com'è distribuita la frequenza: la linea centrale è la mediana, i lati orizzontali sono primo e terzo quartile mentre i pallini, se presenti, sono gli outlier.
plt.boxplot(
    df['val1'],
    df['val2'],
    labels=['val1','val2'],
    whisk=1.5
)
plt.show()

# Due grafici nello stesso grafico
data = [df['val1'], df['val2']]
fig,ax = plt.subplots()
ax.boxplot(data)
ax.set_xticklabels(['Label1','Label2'])
plt.show()

# ECDF
# Evidenzia valori mancanti, distribuzione unimodale e/o simmetrica, distribuzione asimettrica a sinistra (mediana < media) o distribuzione asimmetrica a destra (mediana > media)
ecdf = sm.distributions.ECDF(df['col'])
x = np.arange(1980, 1991)
y = ecdf(x)
plt.step(x,y)
plt.show()

# Grafico di dispersione
# Determina la relazione tra più attributi del dataframe
plt.scatter(df['val1'], df['val2'])
plt.show()

# Istogramma
# Suddivide la popolazione in classi e ne rappresenta le frequenze: mette in risalto simmetria, dispersione, intervalli di concentrazione, vuoti nei dati e distacco nei dati
df['val'].hist()
#df['val'].hist(bins=10)
#df['val'].hist(bins=[0, 30, 50, 70, 100])
#df['val'].hist(bins=np.arange(0,1001,100))
plt.show()

# QQplot
# Stima se la popolazione segue una normale (o qualsiasi distribuzione scelta)
sm.qqplot(df['val'].dropna(), fit=True, line='45')
#sm.qqplot(df['val'].dropna(), dist=st.expon(), line='45')
#sm.qqplot(df['val'].dropna(), dist=st.distributions.poisson(lambda), line='45')
plt.show()
\end{mintedbox}

\section{Indici}
\begin{mintedbox}{python}
# Descrizione generale
df.describe()

# Indici di centralità
(vals.mean(), vals.median())

# Dato più presente
vals.mode()

# Indici di dispersione
vals.var()
vals.quantile(0.75) - vals.quantile(0.25)

# Indice di correlazione
vals1.corr(vals2)

# Indice di Gini (il dato 'col' è rappresentato in maniera uniforme?)
def gini(series):
    return 1 - sum(series.value_counts(normalize=True).map(lambda f: f**2))

def norm_gini(series):
    s = len(series.unique())
    return s * gini(series) / (s-1)

norm_gini(df['col'])

# Entropia
def entropy(series, normalized=False):
    freq = series.value_counts(normalize=True)
    e = sum((freq.map(lambda f: -f * np.log2(f))))
    if normalized:
        e /= np.log2(len(freq))
    return e

entropy(df['col'], normalized=True)
\end{mintedbox}

\section{Miscellanea}
\begin{mintedbox}{python}
# Aggiungere una colonna
# Aggiungo una colonna 'new_val' che vale 1 se 'val > 50', 0 altrimenti
df['new_val'] = [1 if (val > 50) else 0 for i in val]

# Applicare una trasformazione su una serie di dati
# Ogni valore * 2 ed elevato al quadrato
serie.apply(lambda x: (x*2)**2)

# Classificatori
# Dato s come terzo quantile per esempio
VP = len([x for x in df[col1==1]['col2'] if x >= s]
FP = len([x for x in df[col1==0]['col2'] if x >= s]
FN = len([x for x in df[col1==1]['col2'] if x < s]
VN = len([x for x in df[col1==0]['col2'] if x < s]

# Genero un campione casuale di 32 elementi
X = st.norm(mean, std)
simulati = X.rvs(taglia)

# Leggere un dataset
df = pd.read_csv(
    'file.csv',
    delimiter=';',
    decimal=',',
    index_col='col_name',
    parse_dates=True
)

# Minimo e massimo
(df['val'].min(), df['val'].max())

# Raggruppamento
# Ipotizzo un dataframe con studenti, materie e voti: come calcolo la media dei voti delle materie per ciascuno studente?
grouped_df = df.groupby(['Studente', 'Materia'])
medie = grouped_df['Voto'].mean()
# Ipotizzo delle biblioteche, ad ognuna ogni anno si iscrive gente: quanti iscritti totali ogni anno?
iscritti = df[['Anno', 'Iscritti']].groupby(df['Anno']).sum()

# Rimuovere valori mancanti
# Utile per quando vengono richieste frequenze o si esegue uno scatter plot
# Rimuove tutta la riga se in quella riga c'è una colonna mancante
df.dropna()

# Selezione
# Selezionare i valori della colonna 'val2' che sono nel range [40, 60]
df[(df['val2'] >= 40) & (df['val2'] <= 60)]

# Somma dei valori in una colonna
df['val'].sum()

# Sorteggio
# Qual è la probabilità che sorteggiando un valore dal dataset, quel valore sia di un dato tipo?
# (dopo aver eseguito prendo il valore * 100 per ottenere la percentuale)
df['val'].value_counts(normalize=True)

# Stima
# Quando voglio "stimare" uso la media
len(esito) / len(totale)

# Taglia del campione
len(vals)

# Verificare l'assenza di valori mancanti
pd.isnull(df['val']).any()
# Oppure
df['val'].isnull().any()

# Seleziona i valori unici nella colonna
df['val'].unique()
\end{mintedbox}

\section{Ordinamento}
\begin{mintedbox}{python}
# Ordinare una lista in base alla lunghezza degli elementi, in ordine decrescente
sorted(list, key=len, reverse=True)

# Ordinare una lista in base ad un attributo 'attributo'
sorted(tuple_list, key=lambda x: getattr(x, 'attributo'))

# Ordinare una lista in base alla lunghezza usando .sort()
list.sort(key=lambda n: len(n))

# Ordinare un dataframe in base ai valori di due colonne, al contrario e basandosi sulla stringa in minuscolo
df.sort_values(by=['col1', 'col2'], ascending=False, key=lambda s: s.str.lower())

# Ordinare una colonna
col.sort_values()
# Oppure
np.sort(col)
\end{mintedbox}

\end{document}
